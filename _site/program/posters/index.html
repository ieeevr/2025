<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">



<!-- begin _includes/seo.html --><title>Posters | IEEE VR 2025</title>
<meta name="description" content="The 32nd IEEE Conference on Virtual Reality and 3D User Interfaces ">


  <meta name="author" content="IEEE VR 2025">


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="IEEE VR 2025">
<meta property="og:title" content="Posters">
<meta property="og:url" content="http://localhost:4000/program/posters/">


  <meta property="og:description" content="The 32nd IEEE Conference on Virtual Reality and 3D User Interfaces ">



  <meta property="og:image" content="http://localhost:4000/assets/images/IEEEVR_2025_OGImage.png">



  <meta name="twitter:site" content="@IEEEVR">
  <meta name="twitter:title" content="Posters">
  <meta name="twitter:description" content="The 32nd IEEE Conference on Virtual Reality and 3D User Interfaces ">
  <meta name="twitter:url" content="http://localhost:4000/program/posters/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="http://localhost:4000/assets/images/IEEEVR_2025_OGImageSquare.png">
    
  

  







  

  


<link rel="canonical" href="http://localhost:4000/program/posters/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "IEEE VR 2025",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="IEEE VR 2025 Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>



      <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/png" href="favicon.png"/> 

<!-- end custom head snippets -->

      
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">      
      <script src="https://code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  </head>

  <body class="layout--ieeevr-default">
    <div class="initial-content">
     <!-- For all browsers -->
<script src="https://kit.fontawesome.com/4eee35f757.js"></script>
<link rel="stylesheet" href="//code.jquery.com/ui/1.13.2/themes/base/jquery-ui.css">
<script src="https://code.jquery.com/ui/1.13.2/jquery-ui.js"></script>
<link rel="stylesheet" href="/assets/css/main.css?version=20240318">
<link rel="stylesheet" href="/assets/css/tableStyles.css?version=20240318">
<link rel="stylesheet" href="/assets/css/styles.css?version=20240318">

<button onclick="topFunction()" id="myBtnTop" title="Go to top">Top</button>

<script>
    var mybutton = document.getElementById("myBtnTop");

    window.onscroll = function() {
        scrollFunction()
    };

    function scrollFunction() {
        if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100) {
            mybutton.style.display = "block";
        } else {
            mybutton.style.display = "none";
        }
    }

    function topFunction() {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
    }

    function myFunction() {
        var x = document.getElementById("myTopnav");
        if (x.className === "topnav") {
            x.className += " responsive";
        } else {
            x.className = "topnav";
        }
    }
</script>

<div id="main" role="main">
    <article class="splash" style="margin:0px;" itemscope itemtype="https://schema.org/CreativeWork">
         <!-- banner -->
         <a href=/><img class="ieeevrbanner" src=/assets/images/IEEEVR_2025_LogoBanner.jpg alt="The official banner for the IEEE Conference on Virtual Reality + User Interfaces, comprised of a Kiwi wearing a VR headset overlaid on an image of Mount Cook and a braided river."></a>
       
        <!-- content -->
        <section class="page__content" itemprop="text">
            <div class="row">
                <!-- Main Content -->
                <h1>Posters</h1>
<div>
    <table class="styled-table">
        <tr>
            <th colspan="3">Posters (Timezone: Orlando, Florida USA UTC-4)</th>
        </tr>
        <tr>
            <td class="medLarge"><a href="#P1">Monday Posters</a></td>
            <td class="medLarge">Sorcerer's Apprentice Ballroom</td>
            <td class="medLarge">Talk with the authors: 9:45&#8209;10:15, 13:00&#8209;13:30, 15:00&#8209;15:30, 17:00&#8209;17:30</td>
        </tr>
        <tr>
            <td class="medLarge"><a href="#P2">Tuesday Posters</a></td>
            <td class="medLarge">Sorcerer's Apprentice Ballroom</td>
            <td class="medLarge">Talk with the authors: 9:45&#8209;10:15, 13:00&#8209;13:30, 15:00&#8209;15:30, 17:00&#8209;17:30</td>
        </tr>
        <tr>
            <td class="medLarge"><a href="#P3">Wednesday Posters</a></td>
            <td class="medLarge">Sorcerer's Apprentice Ballroom</td>
            <td class="medLarge">Talk with the authors: 9:45&#8209;10:15, 13:00&#8209;13:30, 15:00&#8209;15:30, 17:00&#8209;17:30</td>
        </tr>
    </table>
</div>

<div>    
    <h2 id="P1" class="pink" style="padding-top:25px;">Monday Posters</h2>  
    <p class="small">Talk with the authors: 9:45&#8209;10:15, 13:00&#8209;13:30, 15:00&#8209;15:30, 17:00&#8209;17:30, Room: Sorcerer's Apprentice Ballroom</p>  
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1002" style="margin-bottom: 0.3em;">
                <strong>Effects of constant and sinusoidal display lag on sickness during active exposures to virtual reality (ID:&nbsp;P1002)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Stephen Palmisano,</span>
                        
                    
                        
                            <i>University of Wollongong;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Vanessa Morrison,</span>
                        
                    
                        
                            <i>University of Wollongong;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Robert Allison,</span>
                        
                    
                        
                            <i>York University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rodney Davies,</span>
                        
                    
                        
                            <i>University of Wollongong;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Juno Kim,</span>
                        
                    
                        
                            <i>University of New South Wales</i>
                        
                     
                
            </p>
            
                <div id="P1002" class="wrap-collabsible"> <input id="collapsibleabstractP1002" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1002" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>When we move our heads in virtual reality (VR), display lag creates differences between our virtual and physical head pose (DVP). This study examined whether objective estimates of these DVP could be used to predict the sickness caused by different types of lag. We found that adding constant and time-varying lag to simulations generated similar levels of sickness - with all added lag conditions producing more severe sickness than our baseline control. Consistent with the DVP hypothesis, the spatial magnitude and temporal dynamics of the DVP were both found to predict cybersickness severity during active HMD VR.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1003" style="margin-bottom: 0.3em;">
                <strong>Display lag effects on postural stability and cybersickness during active exposures to HMD virtual reality (ID:&nbsp;P1003)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Stephen Palmisano,</span>
                        
                    
                        
                            <i>University of Wollongong;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Shao Yang Chia,</span>
                        
                    
                        
                            <i>University of Wollongong</i>
                        
                     
                
            </p>
            
                <div id="P1003" class="wrap-collabsible"> <input id="collapsibleabstractP1003" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1003" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study examined whether a person's spontaneous postural sway before, and their head-movements during, exposure to virtual reality (VR) predicts their experiences of cybersickness. We compared the stability of head and body movements made by 50 HMD users to the sickness they experienced during VR simulations with different amounts of display lag. Consistent with Postural Instability Theory, we found that: 1) naturally unstable participants were significantly more likely to become sick during these laggy simulations; and 2) the severity of this sickness depended on the spatial magnitude and the temporal dynamics of their head movements during active HMD VR.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1005" style="margin-bottom: 0.3em;">
                <strong>Learning Personalized Agent for Real-Time Face-to-Face Interaction in VR (ID:&nbsp;P1005)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Xiaonuo Dongye,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dongdong Weng,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Haiyan Jiang,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Pukun Chen,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1005" class="wrap-collabsible"> <input id="collapsibleabstractP1005" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1005" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Interactive agents in virtual reality are anticipated to make decisions and provide feedback based on the user's inputs. Despite recent advancements in large language models (LLMs), employing LLMs in real-time face-to-face interactions decision-making, and delivering personalized feedback remains challenging. To address this, our proposed system involves generating and labeling symbolic data, pre-training a real-time network, collecting personalized data, and fine-tuning the network. Utilizing inputs such as interaction distances, head orientations, and hand poses, the agents can provide personalized feedback. User experiments show significant advantages in both pragmatic and hedonic aspects over LLM-based agents, suggesting potential applications across diverse interactive domains.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1008" style="margin-bottom: 0.3em;">
                <strong>Deep-Texture: A Foldable Haptic Ring for Shape and Texture Rendering in Virtual Reality (ID:&nbsp;P1008)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Youjin Sung,</span>
                        
                    
                        
                            <i>KAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">DongKyu Kwak,</span>
                        
                    
                        
                            <i>Kaist;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Taeyeon Kim,</span>
                        
                    
                        
                            <i>KAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Woontack Woo,</span>
                        
                    
                        
                            <i>KAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sang Ho Yoon,</span>
                        
                    
                        
                            <i>KAIST</i>
                        
                     
                
            </p>
            
                <div id="P1008" class="wrap-collabsible"> <input id="collapsibleabstractP1008" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1008" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this paper, we suggest a foldable device that renders the shape and texture in Virtual Reality called Deep-Texture. We devised Deep-Texture to achieve effective haptic feedback with lightweight hardware by combining the basic sensations of shape and texture. By integrating the frequency change of linear resonant actuator and 1-bar mechanism, we propose a novel haptic interaction device for immersive VR experiences. During the pilot test, the results show that it enhances realism while keeping usability. By open-sourcing Deep-Texture, we aim to empower a wider range of users to engage with and benefit from haptic technology, ultimately lowering the hurdles.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1010" style="margin-bottom: 0.3em;">
                <strong>Towards Continuous Patient Care with Remote Guided VR-Therapy (ID:&nbsp;P1010)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Julian Kreimeier,</span>
                        
                    
                        
                            <i>Friedrich-Alexander University Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hannah Schieber,</span>
                        
                    
                        
                            <i>Friedrich-Alexander University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Noah Lewis,</span>
                        
                    
                        
                            <i>Friedrich-Alexander University Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Max Smietana,</span>
                        
                    
                        
                            <i>Friedrich-Alexander University Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Juliane Reithmeier,</span>
                        
                    
                        
                            <i>Friedrich-Alexander-Universität Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Vlad Cnejevici,</span>
                        
                    
                        
                            <i>Friedrich-Alexander University Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Prathik Prasad,</span>
                        
                    
                        
                            <i>Friedrich-Alexander University Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Abdallah Eid,</span>
                        
                    
                        
                            <i>Friedrich-Alexander University Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maximilian Maier,</span>
                        
                    
                        
                            <i>Kinfinity;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel Roth,</span>
                        
                    
                        
                            <i>Technical University of Munich</i>
                        
                     
                
            </p>
            
                <div id="P1010" class="wrap-collabsible"> <input id="collapsibleabstractP1010" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1010" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Hand motor impairments heavily impact a people's independence and and overall well-being. Physiotherapy plays a crucial role after surgical interventions. Given the shortage of personnel and therapy session availability, enabling support and monitoring during the absence of the physiotherapist is a key future direction of medical care. Virtual reality has been shown to support the rehabilitation. An individualized and motivating rehabilitation process is crucial to support the affected person until full recovery. We present a prototype of a VR rehabilitation system that allows for the medical expert to control exercise planning and receive a detailed report on the patients success.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1012" style="margin-bottom: 0.3em;">
                <strong>Fovea Prediction Model in VR (ID:&nbsp;P1012)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Daniele Giunchi,</span>
                        
                    
                        
                            <i>University College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Riccardo Bovo,</span>
                        
                    
                        
                            <i>Imperial College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Nitesh Bhatia,</span>
                        
                    
                        
                            <i>Imperial College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Thomas Heinis,</span>
                        
                    
                        
                            <i>Imperial College;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Anthony Steed,</span>
                        
                    
                        
                            <i>University College London</i>
                        
                     
                
            </p>
            
                <div id="P1012" class="wrap-collabsible"> <input id="collapsibleabstractP1012" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1012" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose a lightweight deep learning approach for gaze estimation that represents the visual field as three distinct regions: fovea, near, and far peripheral. Each region is modelled using a gaze parameterization gaze regarding angle-magnitude, latitude, or a combination of angle-magnitude-latitude. We evaluated how accurately these representations can predict a user's gaze across the visual field when trained on data from VR headsets. Our experiments confirmed that the latitude model generates gaze predictions with superior accuracy with an average latency compatible with the demanding real-time functionalities of an untethered device. We generated an outperforming ensemble model with a comparable latency.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1014" style="margin-bottom: 0.3em;">
                <strong>Exploring the Gap between Real and Virtual Nature (ID:&nbsp;P1014)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Katherine Hartley,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Victoria Interrante,</span>
                        
                    
                        
                            <i>University of Minnesota</i>
                        
                     
                
            </p>
            
                <div id="P1014" class="wrap-collabsible"> <input id="collapsibleabstractP1014" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1014" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We describe the design and preliminary findings of an experiment that aims to elucidate the cost of omitting accurate haptic and olfactory stimulation from VR nature immersion experiences. Across four separate sessions, participants are immersed in virtual urban and forest environments, while seated both outside in real nature and indoors, with the correspondence of virtual environment to real environment and the order of presentation counterbalanced. We compare multiple restorative outcome measures between the four conditions, including physiological data (EDA and HR), subjective surveys of stress and contentment, and objective performance on tests of visual and auditory attentional resources.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1024" style="margin-bottom: 0.3em;">
                <strong>DepBoxia: Depth Perception Training in Boxing, an Immersive Approach (ID:&nbsp;P1024)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yen-Ru Chen,</span>
                        
                    
                        
                            <i>National Tsing Hua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tsung-Hsun Tsai,</span>
                        
                    
                        
                            <i>National Tsing Hua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tica Lin,</span>
                        
                    
                        
                            <i>Harvard University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Calvin Ku,</span>
                        
                    
                        
                            <i>National Tsing Hua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Min-Chun Hu,</span>
                        
                    
                        
                            <i>National Tsing Hua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hung-Kuo Chu,</span>
                        
                    
                        
                            <i>National Tsing Hua University</i>
                        
                     
                
            </p>
            
                <div id="P1024" class="wrap-collabsible"> <input id="collapsibleabstractP1024" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1024" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Depth perception is crucial in novice boxer training for understanding punch timing and distance. Traditional methods rely on coach support and teamwork, leading to a high entry barrier for novice boxers to train whenever or wherever. To address this, we propose an immersive training system using virtual reality (VR) that integrates visual and audio guidance, specifically designed to enhance depth perception in boxing for novice boxers. Our rigorous experiment shows significant improvements in accuracy and reaction time. We introduce the system to professional boxing coaches, enabling them to integrate a systematic approach to training novice boxers' depth perception.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1028" style="margin-bottom: 0.3em;">
                <strong>4D Facial Capture Pipeline Incorporating Progressive Retopology Approach (ID:&nbsp;P1028)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Zeyu Tian,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dongdong Weng,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hui Fang,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hanzhi Guo,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yihua Bao,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1028" class="wrap-collabsible"> <input id="collapsibleabstractP1028" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1028" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The pipeline for creating high-fidelity facial models often utilizes multi-view stereo techniques for reconstruction. However, the subsequent step of retopology often involves intricate manual work, limiting the extension of facial capture systems towards 4D acquisition. This paper proposes a facial 4D capture pipeline based on high-speed cameras. We employ standard multi-view stereo techniques for 3D reconstruction. Non-linear deformations of facial expressions are decoupled from rigid movements of the skull using QR code markers. Additionally, a progressive automated retopology approach is introduced for batch processing. Results demonstrate that our system can capture continuous facial motion sequences with detailed 3D models.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1029" style="margin-bottom: 0.3em;">
                <strong>Influence of Prior Acquaintance on the Shared VR Experience (ID:&nbsp;P1029)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Esen Küçüktütüncü,</span>
                        
                    
                        
                            <i>Institute of Neurosciences of the University of Barcelona;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ramon Oliver,</span>
                        
                    
                        
                            <i>Institute of Neurosciences of the University of Barcelona;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mel Slater,</span>
                        
                    
                        
                            <i>Institute of Neurosciences of the University of Barcelona</i>
                        
                     
                
            </p>
            
                <div id="P1029" class="wrap-collabsible"> <input id="collapsibleabstractP1029" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1029" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Exploring social dynamics in virtual reality (VR) at a qualitative level holds great potential for improved applications Here we examined the influence of prior acquaintance on how people interacted with each other in VR. Groups of 3 or 4 participants, represented by realistic look-alike avatars, engaged in discussions on predefined themes. There were two conditions: (1) groups of individuals with prior connections and (2) strangers. Questionnaire responses revealed that pre-existing acquaintances fostered a stronger sense of copresence, and greater sentiment compared to the strangers group. This insight is crucial for optimizing the design and dynamics of VR interactions.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1033" style="margin-bottom: 0.3em;">
                <strong>Towards an Altered Body Image Through the Exposure to a Modulated Self in Virtual Reality (ID:&nbsp;P1033)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Erik Wolf,</span>
                        
                    
                        
                            <i>University of Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Carolin Wienrich,</span>
                        
                    
                        
                            <i>University of Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marc Erich Latoschik,</span>
                        
                    
                        
                            <i>University of Würzburg</i>
                        
                     
                
            </p>
            
                <div id="P1033" class="wrap-collabsible"> <input id="collapsibleabstractP1033" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1033" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Self-exposure using modulated embodied avatars in virtual reality (VR) may support a positive body image. However, further investigation is needed to address methodological challenges and to understand the concrete effects, including their quantification. We present an iteratively refined paradigm for studying the tangible effects of exposure to a modulated self in VR. Participants perform body-centered movement tasks in front of a virtual mirror, encountering their photorealistically personalized embodied avatar with increased, decreased, or unchanged body size. Additionally, we propose different body size estimation tasks conducted in reality and VR before and after exposure to assess participants' putative elicited perceptual adaptations.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1034" style="margin-bottom: 0.3em;">
                <strong>More than Fitness: User Perceptions and Hopes for Getting Fit in Virtual Reality  (ID:&nbsp;P1034)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Aleshia Hayes,</span>
                        
                    
                        
                            <i>University of North Texas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Veronica Johnson,</span>
                        
                    
                        
                            <i>University of North Texas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Deborah Cockerham,</span>
                        
                    
                        
                            <i>University of North Texas</i>
                        
                     
                
            </p>
            
                <div id="P1034" class="wrap-collabsible"> <input id="collapsibleabstractP1034" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1034" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Fitness activities are linked to health, cognitive acuity, and the brain's ability to respond to a stimulus. This article reports on a mixed-methods investigation of user experiences of a commercial off-the-shelf virtual reality fitness experience.  A total of 74 visitors to a southern science museum, spanning from under 18 to 64 years of age participated in a VR fitness experience and completed a questionnaire and a semi-structured interview.  Participants reported experiencing social presence with pre-recorded fitness instructors, positive usability, and a majority predicted that VR fitness could improve cognitive acuity.  This pilot indicates a cross-generational interest in VR fitness tools.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1037" style="margin-bottom: 0.3em;">
                <strong>Enhanced Reconstruction of Interacting Hands for Immersive Embodiment (ID:&nbsp;P1037)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yu Miao,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yu Han,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yi xiao,</span>
                        
                    
                        
                            <i>China Academy of Aerospace Science and Innovation;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yue Liu,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1037" class="wrap-collabsible"> <input id="collapsibleabstractP1037" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1037" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Interacting hands reconstruction serves as a channel for natural user engagement in virtual reality, providing realistic embodiment that markedly elevates the immersive experiences. However, accurate prediction of the spatial relations between two hands remains challenging due to the severe occlusion and homogeneous appearance of hands. This paper presents a spatial relationship refinement method for hand reconstruction, employing a module to yield the 3D relative translation between hands and a novel loss function to limit hand mesh penetration. Our method achieves state-of-the-art performance on the InterHand2.6M dataset, offering considerable potential for interacting hands reconstruction in enhancing embodiment for virtual reality.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1038" style="margin-bottom: 0.3em;">
                <strong>ExpressionAuth: Utilizing Avatar Expression Blendshapes for Behavioral Biometrics in VR (ID:&nbsp;P1038)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Tussoun Jitpanyoyos,</span>
                        
                    
                        
                            <i>Shizuoka University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuya Sato,</span>
                        
                    
                        
                            <i>Shizuoka University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Soshi Maeda,</span>
                        
                    
                        
                            <i>Shizuoka Univerisity;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Masakatsu Nishigaki,</span>
                        
                    
                        
                            <i>Shizuoka University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tetsushi Ohki,</span>
                        
                    
                        
                            <i>Shizuoka University</i>
                        
                     
                
            </p>
            
                <div id="P1038" class="wrap-collabsible"> <input id="collapsibleabstractP1038" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1038" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>As interests in Virtual Reality (VR) continue to rise, head-mounted displays (HMDs) are actively being developed. Current user authentication method in HMDs requires the use of virtual keyboard, which has low usability and is prone to shoulder surfing attacks. This paper introduces ExpressionAuth, a novel authentication method which uses face tracking capabilities in certain HMDs to verify the identity of the user. ExpressionAuth leverages smile as the expression for user verification. ExpressionAuth has the potential to be a secure and usable biometrics, achieving EER as low as 0.00178 and AUC of up to 0.999 in our experiments.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1039" style="margin-bottom: 0.3em;">
                <strong>Evaluating the Feasibility of Using Augmented Reality for Tooth Preparation (ID:&nbsp;P1039)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Takuya Kihara,</span>
                        
                    
                        
                            <i>Johns Hopkins University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Andreas Keller,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Takumi Ogawa,</span>
                        
                    
                        
                            <i>Tsurumi University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mehran Armand,</span>
                        
                    
                        
                            <i>Johns Hopkins University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alejandro Martin-Gomez,</span>
                        
                    
                        
                            <i>Johns Hopkins University</i>
                        
                     
                
            </p>
            
                <div id="P1039" class="wrap-collabsible"> <input id="collapsibleabstractP1039" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1039" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Tooth preparation is a fundamental treatment technique to restore oral function in prosthodontic dentistry. This technique is complicated as it requires the preparation of an abutment while simultaneously predicting the ideal shape. We explore the feasibility of using Augmented Reality (AR) Head-Mounted Displays (HMDs) to assist dentists during tooth preparation using two different visualization techniques. A user study (N=24) revealed that AR is effective for angle adjustment, and reduces the occurrence of over-reduction. These results suggest that AR can be used to assist physicians during these procedures and has the potential to enhance the accuracy and safety of prosthodontic treatment.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1041" style="margin-bottom: 0.3em;">
                <strong>Investigating Incoherent Depth Perception Features in Virtual Reality using Stereoscopic Impostor-Based Rendering (ID:&nbsp;P1041)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Kristoffer Waldow,</span>
                        
                    
                        
                            <i>TH Köln;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Lukas Decker,</span>
                        
                    
                        
                            <i>TH Köln;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Martin Mišiak,</span>
                        
                    
                        
                            <i>University of Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Arnulph Fuhrmann,</span>
                        
                    
                        
                            <i>TH Köln;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel Roth,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marc Erich Latoschik,</span>
                        
                    
                        
                            <i>University of Würzburg</i>
                        
                     
                
            </p>
            
                <div id="P1041" class="wrap-collabsible"> <input id="collapsibleabstractP1041" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1041" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Depth perception is essential for our daily experiences, aiding in orientation and interaction with our surroundings. Virtual Reality allows us to decouple such depth cues mainly represented through binocular disparity and motion parallax. Dealing with fully-mesh-based rendering methods these cues are not problematic as they originate from the object's underlying geometry. However, manipulating motion parallax, as seen in stereoscopic imposter-based rendering, raises questions about visual errors and perceived 3-dimensionality. Therefore, we conducted a user experiment to investigate how varying object sizes affect such visual errors and perceived 3-dimensionality, revealing an interestingly significant negative correlation and new assumptions about visual quality.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1042" style="margin-bottom: 0.3em;">
                <strong>Facial Feature Enhancement for Immersive Real-Time Avatar-Based Sign Language Communication using Personalized CNNs (ID:&nbsp;P1042)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Kristoffer Waldow,</span>
                        
                    
                        
                            <i>TH Köln;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Arnulph Fuhrmann,</span>
                        
                    
                        
                            <i>TH Köln;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel Roth,</span>
                        
                    
                        
                            <i>Technical University of Munich</i>
                        
                     
                
            </p>
            
                <div id="P1042" class="wrap-collabsible"> <input id="collapsibleabstractP1042" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1042" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Facial recognition is crucial in sign language communication. Especially for virtual reality and avatar-based communication, increased facial features have the potential to integrate the deaf and hard-of-hearing community to improve speech comprehension and empathy. But, current methods lack precision in capturing nuanced expressions. To address this, we present a real-time solution that utilizes personalized Convolutional Neural Networks (CNNs) to capture intricate facial details, such as tongue movement and individual puffed cheeks. Our system's classification models offer easy expansion and integration into existing facial recognition systems via UDP network broadcasting.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1046" style="margin-bottom: 0.3em;">
                <strong>Comparatively testing the effect of reality modality on spatial memory (ID:&nbsp;P1046)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Leon Mayrose,</span>
                        
                    
                        
                            <i>Ben Gurion University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Shachar Maidenbaum,</span>
                        
                    
                        
                            <i>Ben Gurion University</i>
                        
                     
                
            </p>
            
                <div id="P1046" class="wrap-collabsible"> <input id="collapsibleabstractP1046" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1046" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual and augmented reality hold great potential for understanding spatial cognition. However, it is unclear what effect reality modality has on our perception and interaction with our spatial surroundings. Here, participants performed a spatial memory task using passthrough augmented reality in the real world and in a virtual environment reconstructed by scanning the real environment. We found no significant differences by reality modality for subjective measures such as reported immersion, difficulty, enjoyment and cyber-sickness, nor did we find objective differences in performance. These results suggest limited effects on spatial tasks, and are promising for transfer between virtual and augmented scenarios.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1047" style="margin-bottom: 0.3em;">
                <strong>PUTREE: A PHOTOREALISTIC LARGE-SCALE VIRTUAL BENCHMARK FOR FOREST TRAINING (ID:&nbsp;P1047)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yawen Lu,</span>
                        
                    
                        
                            <i>Purdue Univ;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yunhan Huang,</span>
                        
                    
                        
                            <i>Purdue University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Su Sun,</span>
                        
                    
                        
                            <i>Purdue University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Songlin Fei,</span>
                        
                    
                        
                            <i>Purdue University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yingjie Victor Chen,</span>
                        
                    
                        
                            <i>Purdue University</i>
                        
                     
                
            </p>
            
                <div id="P1047" class="wrap-collabsible"> <input id="collapsibleabstractP1047" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1047" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Forest systems play an important role in mitigating anthropogenic climate change and regulating the global climate. However, due to difficulties in collecting wild data and lack of forestry expertise, the availability of large-scale forest datasets is very limited. In this work, we establish a new virtual forest dataset named PUTree. Our goal is to create a larger, more photo-realistic and diverse dataset as a powerful training resource in the wild forest. Early experimental results demonstrate its validity as a new forest benchmark for the evaluation of tree detection and segmentation algorithms, and its potential in broad application scenarios.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1048" style="margin-bottom: 0.3em;">
                <strong>Effectiveness of Visual Acuity Test in VR vs Real World (ID:&nbsp;P1048)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Sarker Monojit Asish,</span>
                        
                    
                        
                            <i>Florida Polytechnic University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Roberto Enrique Salazar,</span>
                        
                    
                        
                            <i>University of Louisiana at Lafayette;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Arun K Kulshreshth,</span>
                        
                    
                        
                            <i>University of Louisiana at Lafayette</i>
                        
                     
                
            </p>
            
                <div id="P1048" class="wrap-collabsible"> <input id="collapsibleabstractP1048" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1048" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual Reality (VR) devices have opened a new dimension of merging technology and healthcare in an immersive and exciting way to test eye vision. Visual acuity is a person's capacity to perceive small details. An optometrist or ophthalmologist determines a visual acuity score following a vision examination. In this work, we explored how recent VR devices could be utilized to conduct visual acuity tests. We used two Snellen charts to examine eye vision in VR, similar to testing in a doctor's chamber. We found that VR could be utilized to conduct preliminary eye vision tests.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1049" style="margin-bottom: 0.3em;">
                <strong>Effects of moving task condition on improving operational performance with slight delay (ID:&nbsp;P1049)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yakumo Miwa,</span>
                        
                    
                        
                            <i>Nagoya Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kenji Funahashi,</span>
                        
                    
                        
                            <i>Nagoya Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Koji Tanida,</span>
                        
                    
                        
                            <i>Faculty of Science and Engineering;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Shinji Mizuno,</span>
                        
                    
                        
                            <i>Aichi Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1049" class="wrap-collabsible"> <input id="collapsibleabstractP1049" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1049" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We made a hypothesis that appropriate delay in operational system would improve its operational performance from some reviews and papers. As an experimental result, performance was improved in slight delay. The sensory evaluation also confirmed that the subject felt support even though there was no actual force support. Another experiment confirmed that depth movement restriction, and the move ratio of the virtual tool on a screen to the input device affected to improve performance. We also investigated that difference of task conditions, i.e. moving task distance and target area size, affect to performance improvement.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1050" style="margin-bottom: 0.3em;">
                <strong>Cross-Reality Attention Guidance on the Light Field Display (ID:&nbsp;P1050)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Chuyang Zhang,</span>
                        
                    
                        
                            <i>Keio University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kai Kunze,</span>
                        
                    
                        
                            <i>Keio University Graduate School of Media Design</i>
                        
                     
                
            </p>
            
                <div id="P1050" class="wrap-collabsible"> <input id="collapsibleabstractP1050" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1050" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present a cross-reality collaboration system with visual attention guidance that allows a user in VR to share his view with the users in the real world accurately. The VR user can remotely decide the display content of the light field display oriented to multiple real-world users by manipulating the camera in the virtual world. The VR user's focus depth is estimated and then used to adjust the focal plane of the light field display. Our system can improve collaboration in multi-user scenarios especially in which one host user is delivering information to others such as in classrooms and museums.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1054" style="margin-bottom: 0.3em;">
                <strong>Training a Neural Network on Virtual Reality Devices: Challenges and Limitations (ID:&nbsp;P1054)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Francisco Díaz-Barrancas,</span>
                        
                    
                        
                            <i>Justus Liebig University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dr. Daniel Flores-Martín,</span>
                        
                    
                        
                            <i>University of Extremadura;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dr. Javier Berrocal,</span>
                        
                    
                        
                            <i>University of Extremadura</i>
                        
                     
                
            </p>
            
                <div id="P1054" class="wrap-collabsible"> <input id="collapsibleabstractP1054" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1054" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The processing power of Virtual Reality (VR) devices is constantly growing. However, few applications still take advantage of these capabilities. Machine learning algorithms have shown promise in enabling an immersive and personalized experience for VR device users. Therefore, it is interesting that these algorithms are processed directly on the devices themselves, without needing other external resources. In this work, a Neural Network (NN) is trained for real-time image classification using different VR devices. The results show the feasibility of incorporating VR devices for NN training without compromising the quality of the interaction, simply and saving external resources.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1056" style="margin-bottom: 0.3em;">
                <strong>Exploring Influences of Appearance and Voice Realism of Virtual Humans on Communication Effectiveness in Social Virtual Reality (ID:&nbsp;P1056)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yu Han,</span>
                        
                    
                        
                            <i>Beijing Engineering Research Center of Mixed Reality and Advanced Display;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yu Miao,</span>
                        
                    
                        
                            <i>Beijing Engineering Research Center of Mixed Reality and Advanced Display;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hao Sha,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yue Liu,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1056" class="wrap-collabsible"> <input id="collapsibleabstractP1056" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1056" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual humans play a crucial role in elevating user experiences in Virtual Reality (VR). Despite ongoing technological advancements, achieving highly realistic virtual humans with entirely natural behaviors remains a challenge. This paper explores how the appearance and voice realism of virtual humans influence communication effectiveness within social VR scenarios. Our preliminary results indicate the significant impact of alterations in appearance and voice realism on communication effectiveness. We observe that a cross-modal realism mismatch between appearance and voice can impede effective communication. This research provides valuable insights for designing virtual humans and improving the quality of social communication in VR environments.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1059" style="margin-bottom: 0.3em;">
                <strong>Exploring Virtual Reality for Religious Education in Real-World Settings (ID:&nbsp;P1059)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Sara Wolf,</span>
                        
                    
                        
                            <i>Julius-Maximilians-Universität Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ilona Nord,</span>
                        
                    
                        
                            <i>Julius-Maximilians Universität Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jörn Hurtienne,</span>
                        
                    
                        
                            <i>Julius-Maximilians-Universität</i>
                        
                     
                
            </p>
            
                <div id="P1059" class="wrap-collabsible"> <input id="collapsibleabstractP1059" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1059" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Research and design of virtual reality (VR) applications for educational contexts often focus on science-related subjects and evaluate knowledge acquisition while overlooking other subjects like religious education or what actually happens when VR is used in real-world settings. Our article combines both and presents two VR applications, Blessed Spaces and VR Pastor, designed for individual experiences in (Protestant) religious education. We deployed the applications in a real-world setting, an out-of-school learning centre. Surprisingly, the applications mediated social engagement between students. Our findings challenge traditional notions of social experiences in VR-supported education and call for more research in real-world settings.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1060" style="margin-bottom: 0.3em;">
                <strong>Evaluation of Shared-Gaze Visualizations for Virtual Assembly Tasks (ID:&nbsp;P1060)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Daniel Alexander Delgado,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jaime Ruiz,</span>
                        
                    
                        
                            <i>University of Florida</i>
                        
                     
                
            </p>
            
                <div id="P1060" class="wrap-collabsible"> <input id="collapsibleabstractP1060" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1060" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Shared-gaze visualizations (SGV) allow collocated collaborators to understand each other's attention and intentions while working jointly in an augmented reality setting.   However, prior work has overlooked user control and privacy over how gaze information can be shared between collaborators.   In this abstract, we examine two methods for visualizing shared-gaze between collaborators: gaze-hover and gaze-trigger.  We compare the methods with existing solutions through a paired-user evaluation study in which participants participate in a virtual assembly task. Finally, we contribute an understanding of user perceptions, preferences, and design implications of shared-gaze visualizations in augmented reality.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1061" style="margin-bottom: 0.3em;">
                <strong>Effects on Size Perception by Changing Dynamic Invisible Body Size  (ID:&nbsp;P1061)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ryota Kondo,</span>
                        
                    
                        
                            <i>Keio University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maki Sugimoto,</span>
                        
                    
                        
                            <i>Keio University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hideo Saito,</span>
                        
                    
                        
                            <i>Keio University</i>
                        
                     
                
            </p>
            
                <div id="P1061" class="wrap-collabsible"> <input id="collapsibleabstractP1061" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1061" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Only virtual hands and feet move synchronously with an observer's movement, inducing body ownership of an invisible body between them. However, it is unclear whether body ownership is also induced when the size of the invisible body is changed. In this study, we investigated whether body ownership is induced in large or small invisible bodies and whether size perception changes with the size of the invisible body. The results showed that body ownership was induced even if the size of the invisible body was changed, but the size perception did not change.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1062" style="margin-bottom: 0.3em;">
                <strong>Can't touch this? Why vibrotactile feedback matters in educational VR   (ID:&nbsp;P1062)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Fabian Froehlich,</span>
                        
                    
                        
                            <i>NYU</i>
                        
                     
                
            </p>
            
                <div id="P1062" class="wrap-collabsible"> <input id="collapsibleabstractP1062" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1062" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study investigates the relationship between vibrotactile feedback and sense of presence in VR. The inquiry focuses on corrective and reenforcing feedback in STEM learning outcomes using a VR environment called [blinded].  In a randomized within-subject design experiment (N=68) participants got assigned to a vibrotactile and non-vibrotactile condition. Our hypotheses: Participants in the vibrotactile-condition report higher sense of presence ratings compared to the non-haptic condition. Results indicate that vibrotactile feedback increases the sense of presence and impacts metacognition. Participants who received corrective feedback as a vibrotactile stimuli are more likely to underestimate their actual test performance.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1063" style="margin-bottom: 0.3em;">
                <strong>Exploring the efficient and hedonic shopping: A Comparative Study of in-game VR Stores (ID:&nbsp;P1063)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yang Zhan,</span>
                        
                    
                        
                            <i>Waseda University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yiming Sun,</span>
                        
                    
                        
                            <i>Waseda University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tatsuo Nakajima,</span>
                        
                    
                        
                            <i>Waseda University</i>
                        
                     
                
            </p>
            
                <div id="P1063" class="wrap-collabsible"> <input id="collapsibleabstractP1063" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1063" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Shopping in Virtual Reality (VR) become popular in recent years since it provides immersive experiences. However, there is insufficient understanding of how efficient and hedonic features in VR stores affect the user experiences concurrently. This work aimed to address this gap by integrating a 2D user interface store and a 3D diegetic store into a VR game for comparative analysis. We explore the effects of efficient and hedonic factors on the users' perception and experiences. Results from a within-subject study ($N$=14) revealed that the diegetic store surpasses the 2D store in offering hedonic features, providing suggestions for VR store designs.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1064" style="margin-bottom: 0.3em;">
                <strong>Pinhole Occlusion: Enhancing Soft-edge Occlusion Using a Dynamic Pinhole Array (ID:&nbsp;P1064)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Xiaodan Hu,</span>
                        
                    
                        
                            <i>NAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yan Zhang,</span>
                        
                    
                        
                            <i>Shanghai Jiao Tong University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Monica Perusquia-Hernandez,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technolgy;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yutaro Hirao,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hideaki Uchiyama,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kiyoshi Kiyokawa,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1064" class="wrap-collabsible"> <input id="collapsibleabstractP1064" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1064" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Systems with occlusion capabilities have gained interest in augmented reality, vision augmentation, and image processing. To address the challenge of creating a precise yet lightweight occlusion system, we introduce a novel architecture to tackle occlusion blurriness due to defocusing.  Our approach, utilizing a dynamic pinhole array on a transmissive spatial light modulator positioned between the eye and the occlusion layer, offers adaptive pinhole patterns, gaze-contingent functionality, and the potential to reduce visual artifacts. Our preliminary result demonstrates that, with the focal plane at 1.8 m, an occlusion placed at 4 cm can be observed sharply through a 4.3 mm aperture.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1065" style="margin-bottom: 0.3em;">
                <strong>Comparative Efficacy of 2D and 3D Virtual Reality Games in American Sign Language Learning (ID:&nbsp;P1065)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jindi Wang,</span>
                        
                    
                        
                            <i>Durham Univeristy;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ioannis Ivrissimtzis,</span>
                        
                    
                        
                            <i>Durham University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhaoxing Li,</span>
                        
                    
                        
                            <i>University of Southampton;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Lei Shi,</span>
                        
                    
                        
                            <i>Newcastle University</i>
                        
                     
                
            </p>
            
                <div id="P1065" class="wrap-collabsible"> <input id="collapsibleabstractP1065" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1065" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Extensive research on sign language aimed to enhance communication between hearing individuals and the deaf community. With ongoing advancements in virtual reality and gamification, researchers are exploring their application in sign language learning. This study compares the impact of 2D and 3D games on American Sign Language (ASL) learning, using questionnaires to assess user experience. Results show that 3D games enhance engagement, attractiveness, usability, and efficiency, although user performance remains similar in both environments. The findings suggest the potential of 3D game-based approaches to improving ASL learning experiences while also identifying areas for enhancing dependability and clarity in 3D environments.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1068" style="margin-bottom: 0.3em;">
                <strong>Cybersickness Lies in the Eye of the Observer - Pupil Diameter as a Potential Indicator of Motion Sickness in Virtual Reality? (ID:&nbsp;P1068)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Katharina Margareta Theresa Pöhlmann,</span>
                        
                    
                        
                            <i>KITE-Toronto Rehabilitation Institute;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Aalim Makani,</span>
                        
                    
                        
                            <i>Toronto Metropolitan University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Raheleh Saryazdi,</span>
                        
                    
                        
                            <i>Trent University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Keshavarz Behrang,</span>
                        
                    
                        
                            <i>The KITE Research Institute</i>
                        
                     
                
            </p>
            
                <div id="P1068" class="wrap-collabsible"> <input id="collapsibleabstractP1068" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1068" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Cybersickness is a widespread problem for many users of Virtual Reality systems. Changes in pupil diameter have been suggested as potential physiological correlates of cybersickness, but the relationship remains vague. Here, we further investigated how pupil diameter changes in relation to cybersickness by engaging participants in a passive locomotion through an outer-space environment. Participants who experienced sickness showed greater variance in pupil diameter compared to non-sick participants, whereas average pupil diameter did not differ. Our results suggest that irregular pupillary rhythms may be a potential correlate of cybersickness, which could be used to objectively identify cybersickness.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1069" style="margin-bottom: 0.3em;">
                <strong>Never Tell The Trick: Covert Interactive Mixed Reality System for Immersive Theatre (ID:&nbsp;P1069)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Chanwoo Lee,</span>
                        
                    
                        
                            <i>Imperial College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kyubeom Shim,</span>
                        
                    
                        
                            <i>Sogang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sanggyo Seo,</span>
                        
                    
                        
                            <i>Sogang Univ.;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gwonu Ryu,</span>
                        
                    
                        
                            <i>Dept. of Art &amp; Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yongsoon Choi,</span>
                        
                    
                        
                            <i>Sogang Univ.</i>
                        
                     
                
            </p>
            
                <div id="P1069" class="wrap-collabsible"> <input id="collapsibleabstractP1069" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1069" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study explores the integration of Ultra-Wideband (UWB) technology into Mixed Reality (MR) Systems for immersive theatre. Addressing the limitations of existing technologies like Microsoft Kinect and HTC Vive, the research focuses on overcoming challenges in robustness to occlusion, tracking volume, and cost efficiency in props tracking. Utilizing the UWB, the immersive MR system enhances the scope of performance art by enabling larger tracking areas, more reliable and cheaper multi-prop tracking, and reducing occlusion issues. Preliminary user tests demonstrate meaningful improvements in immersive experience, promising a new possibility in Extended Reality (XR) theatre and performance art.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1070" style="margin-bottom: 0.3em;">
                <strong>Enhancing Virtual Walking in Lying Position: Upright Perception by Changing Self-Avatar's Posture (ID:&nbsp;P1070)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Junya Nakamura,</span>
                        
                    
                        
                            <i>Toyohashi University of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michiteru Kitazaki,</span>
                        
                    
                        
                            <i>Toyohashi University of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1070" class="wrap-collabsible"> <input id="collapsibleabstractP1070" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1070" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We aimed to decrease visual-proprioceptive conflict in experiencing virtual walking with the lying posture. An optic flow during an avatar's standing up was presented before virtual walking that was induced by the radial optic flow and foot vibrations. The walking sensation and telepresence slightly increased by the standing-up optic flow, but it did not reach statistical significance. Participants felt as the posture was more matched to the walking avatar with the standing-up optic flow compared to the no-animation condition. These results highlight the potential for posture-informed VR design to improve user experiences in a situation with visual-proprioceptive conflict.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1072" style="margin-bottom: 0.3em;">
                <strong>StreamSpace: A Framework for Window Streaming in Collaborative MR Environments (ID:&nbsp;P1072)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Daniele Giunchi,</span>
                        
                    
                        
                            <i>University College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Riccardo Bovo,</span>
                        
                    
                        
                            <i>Imperial College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Nels Numan,</span>
                        
                    
                        
                            <i>University College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Anthony Steed,</span>
                        
                    
                        
                            <i>University College London</i>
                        
                     
                
            </p>
            
                <div id="P1072" class="wrap-collabsible"> <input id="collapsibleabstractP1072" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1072" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We introduce StreamSpace as a framework for the exploration of screen-based collaborative MR experiences, focusing on the streaming, integration, and layout of screen content in MR environments. Utilizing Unity and Ubiq, this framework allows users to engage with, reposition, and resize uniquely identified screens within a user-centric virtual space. Building on Ubiq's WebRTC capabilities, our framework enables real-time streaming and transformations through peer-to-peer communication. Key features of StreamSpace include distributed streaming, automated screen layout, and flexible privacy settings for virtual screens. Introducing StreamSpace, we aim to provide a foundational basis for research on screen-based collaborative MR applications.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1074" style="margin-bottom: 0.3em;">
                <strong>Can Brain Stimulation Reduce VR motion sickness in Healthy Young Adults During an Immersive Relaxation Application? A Study of tACS (ID:&nbsp;P1074)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Gang Li,</span>
                        
                    
                        
                            <i>University of Glasgow;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ari Billig,</span>
                        
                    
                        
                            <i>SyncVR Medical;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Chao Ping Chen,</span>
                        
                    
                        
                            <i>Shanghai Jiao Tong University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Katharina Margareta Theresa Pöhlmann,</span>
                        
                    
                        
                            <i>KITE Rehabilitation Institution</i>
                        
                     
                
            </p>
            
                <div id="P1074" class="wrap-collabsible"> <input id="collapsibleabstractP1074" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1074" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study marks the first exploration of whether a non-invasive transcranial alternating current stimulation (tACS) on the left parietal cortex can reduce VR motion sickness (VRMS) induced by a commercial VR relaxation app. Two VRMS conditions were examined for 36 healthy young adults: 1) pure VRMS without a moving platform; 2) VRMS with a side-to-side rotary chair. Participants underwent three counterbalanced tACS protocols at the beta frequency band (sham, treatment, and control). Contrary to our hypothesis, the treatment protocol did not significantly reduce VRMS in either condition. Given the protocol's prior success in our previous tACS study, we discussed potential factors hindering the replication of our earlier achievement.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1078" style="margin-bottom: 0.3em;">
                <strong>Super-Resolution AR?: Enhanced Image Visibility for AR Imagery? (ID:&nbsp;P1078)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Hyemin Shin,</span>
                        
                    
                        
                            <i>Korea University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hanseob Kim,</span>
                        
                    
                        
                            <i>Korea University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">DongYun Joo,</span>
                        
                    
                        
                            <i>Korea University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gerard Jounghyun Kim,</span>
                        
                    
                        
                            <i>Korea University</i>
                        
                     
                
            </p>
            
                <div id="P1078" class="wrap-collabsible"> <input id="collapsibleabstractP1078" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1078" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In AR applications, there may be situations in which the visual target is not clearly visible/legible because it is too far or small.  The unclear part of the imagery can be captured and magnified, but the image quality can still be problematic with the aliasing artifacts by the limited resolution.  This poster proposes to apply deep learning-based upscaling to enhance the low-resolution images. We developed a prototype system that can capture an image and upscale/present it to the user. The pilot study demonstrated that upscaled imagery improved image clarity, the ability to find hidden information more quickly, and user experience.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1079" style="margin-bottom: 0.3em;">
                <strong>u-DFOV: User-Activated Dynamic Field of View Restriction for Managing Cybersickness and Task Performance (ID:&nbsp;P1079)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yechan Yang,</span>
                        
                    
                        
                            <i>Korea University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hanseob Kim,</span>
                        
                    
                        
                            <i>Korea University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gerard Jounghyun Kim,</span>
                        
                    
                        
                            <i>Korea University</i>
                        
                     
                
            </p>
            
                <div id="P1079" class="wrap-collabsible"> <input id="collapsibleabstractP1079" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1079" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Dynamic field of view restriction is one effective way of mitigating cybersickness by modulating the amount of visual information during virtual navigation. However, in the presence of an interactive task for which visibility is important, it can impede the task performance. This poster examined the efficiency of users, manually engaging the dynamic field of view restriction to control and mitigate cybersickness while performing interactive tasks. The comparative experiment has shown that the user-activated method significantly reduced cybersickness as much as the automatic method. However, it also achieved significantly higher task performance and usability despite the manual control.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1080" style="margin-bottom: 0.3em;">
                <strong>PianoFMS: Real-time Evaluation of Cybersickness by Keyboard Fingering (ID:&nbsp;P1080)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yechan Yang,</span>
                        
                    
                        
                            <i>Korea University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hanseob Kim,</span>
                        
                    
                        
                            <i>Korea University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jungha Kim,</span>
                        
                    
                        
                            <i>Korea University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gerard Jounghyun Kim,</span>
                        
                    
                        
                            <i>Korea University</i>
                        
                     
                
            </p>
            
                <div id="P1080" class="wrap-collabsible"> <input id="collapsibleabstractP1080" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1080" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Various measurement tools/methods have been developed to assess cybersickness induced in virtual environments, e.g., using the controller, dial device, and verbal input. In this poster, we propose PianoFMS as a cybersickness measurement tool that allows users to directly input absolute scores using the five piano keys without tampering with visual content. The preliminary study revealed that the levels of cybersickness measured using both the dial and PianoFMS were similar, and they each exhibited a significant correlation with that of the conventional post-experiment questionnaire scores. However, the PianoFMS exhibited a markedly enhanced level of usability in comparison to the dial.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1082" style="margin-bottom: 0.3em;">
                <strong>VR Interface vs Desktop to convey Quality of Outerwear: a comparative study (ID:&nbsp;P1082)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Dario Gentile,</span>
                        
                    
                        
                            <i>Politecnico di Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Francesco Musolino,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michele Fiorentino,</span>
                        
                    
                        
                            <i>Polythecnic Institute of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">fabio vangi,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari</i>
                        
                     
                
            </p>
            
                <div id="P1082" class="wrap-collabsible"> <input id="collapsibleabstractP1082" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1082" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Conveying the quality of garments through media as in the physical store is demanding. This study proposes the design of a VR interface that aims to convey outerwear quality, featuring the 3D model animated through physical simulation. To measure the effectiveness of this interface, it is tested simultaneously with its corresponding desktop counterpart, with the photo gallery of the product, in a within-subject analysis on 50 users. Results show that perceived quality of products changes between the experiences. Moreover, in VR visual content was found to be more significant for the quality assessment than written information.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1083" style="margin-bottom: 0.3em;">
                <strong>Development of Force Display Using Pneumatic Actuators for Efficient Conveyance of Emotion (ID:&nbsp;P1083)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Nagisa Ito,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hiroyuki Umemura,</span>
                        
                    
                        
                            <i>National Institute of Advanced Industrial Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kunihiro Ogata,</span>
                        
                    
                        
                            <i>National Institute of Advanced Industrial Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kenta Kimura,</span>
                        
                    
                        
                            <i>National Institute of Advanced Industrial Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1083" class="wrap-collabsible"> <input id="collapsibleabstractP1083" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1083" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study investigated the emotional impact of varying force in haptic feedback during gripping. Previous studies have not focused on how grip strength influences emotional expression, despite its known use in conveying feelings. We developed a haptic device to present grip-like haptic presentations and conducted an experiment (N=17) to evaluate the emotions elicited by different haptic presentations. The study found that force, speed, and presentation pattern influence both valence (positive or negative emotion) and arousal (intensity of emotion). The results also indicated that haptic feedback communicates not only anger and disgust but also happiness and surprise.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1084" style="margin-bottom: 0.3em;">
                <strong>Mid-air Imaging Based on Truncated Cylindrical Array Plate (ID:&nbsp;P1084)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Junpei Sano,</span>
                        
                    
                        
                            <i>The University of Electro-Communications;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Naoya Koizumi,</span>
                        
                    
                        
                            <i>Department of Informatics</i>
                        
                     
                
            </p>
            
                <div id="P1084" class="wrap-collabsible"> <input id="collapsibleabstractP1084" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1084" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper presents a mid-air imaging optical system consisting of two dimensionally arranged truncated cylindrical optical elements. The proposed system aims to reduce the impact of stray light and improve the limited viewing range of mid-air images in micromirror array plates, an existing mid-air imaging optical system. In this study, we used ray tracing to assess mid-air images formed by our proposed optical system. The results show that our method is practical in terms of the invisibility of stray light and brightness of the image when viewed from an angle.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1085" style="margin-bottom: 0.3em;">
                <strong>Multitasking with Graphical Encoding Visualization of Numerical Values in Virtual Reality (ID:&nbsp;P1085)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Amal Hashky,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Benjamin Rheault,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ahmed Rageeb Ahsan,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Lauren Newman,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Eric Ragan,</span>
                        
                    
                        
                            <i>University of Florida</i>
                        
                     
                
            </p>
            
                <div id="P1085" class="wrap-collabsible"> <input id="collapsibleabstractP1085" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1085" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study evaluates the influence of various visual representations of numerical values on users' ability to multitask in virtual reality. We designed a game-like VR simulation where users had to complete one main task while maintaining the status of other subtasks. Supplemental visualizations showed risk status of the subtask depending on experimental condition, with different visual data encodings: position, brightness, color, and area. We collected preliminary data (n=18) on participant performance during the experiment and subjective ratings afterward. The results showed that the intervention rate significantly differed between the four visual encodings, with the position-based version having the lowest rate.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1086" style="margin-bottom: 0.3em;">
                <strong>Alleviating the Uncanny Valley Problem in Facial Model Mapping Using Direct Texture Transfer (ID:&nbsp;P1086)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Kaylee Andrews,</span>
                        
                    
                        
                            <i>Augusta University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jeffrey L Benson Jr.,</span>
                        
                    
                        
                            <i>Augusta University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jason Orlosky,</span>
                        
                    
                        
                            <i>Augusta University</i>
                        
                     
                
            </p>
            
                <div id="P1086" class="wrap-collabsible"> <input id="collapsibleabstractP1086" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1086" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Though facial models for telepresence have made significant progress in recent years, most model reconstruction techniques still suffer from artifacts or deficiencies that result in the uncanny valley problem when used for real-time communication. In this paper, we propose an optimized approach that makes use of direct texture transfer and reduces the inconsistencies present in many facial modeling algorithms. By mapping the source texture from a 2D image to a rough 3D facial mesh, detailed features are preserved, while still allowing a 3D perspective view of the face. Moreover, we accomplish this in real time with a single, monocular camera.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1092" style="margin-bottom: 0.3em;">
                <strong>Embracing Tradition Through Technology: The Mixed Reality Calligraphy Studying Environment (ID:&nbsp;P1092)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yi Wang,</span>
                        
                    
                        
                            <i>Beijing Jiaotong University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ze Gao,</span>
                        
                    
                        
                            <i>Hong Kong University of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1092" class="wrap-collabsible"> <input id="collapsibleabstractP1092" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1092" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This article introduces an innovative Mixed Reality (MR) system designed explicitly for calligraphy learning and practice. Learners must prepare numerous copies of calligraphy works and character templates in traditional calligraphy study and practice. They often rely on oral guidance from teachers in person for their calligraphy practice. However, with the progress of digital technology, we envision leveraging MR wearable glasses combined with image capture and analysis techniques to assist calligraphy learners in improving their practice in a more flexible time.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1094" style="margin-bottom: 0.3em;">
                <strong>Designing Non-Humanoid Virtual Assistants for Task-Oriented AR Environments (ID:&nbsp;P1094)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Bettina Schlager,</span>
                        
                    
                        
                            <i>Columbia University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Steven Feiner,</span>
                        
                    
                        
                            <i>Columbia University</i>
                        
                     
                
            </p>
            
                <div id="P1094" class="wrap-collabsible"> <input id="collapsibleabstractP1094" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1094" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In task-oriented Augmented Reality (AR), humanoid Embodied Conversational Agents can enhance the feeling of social presence and reduce mental workload. Yet, such agents can also introduce social biases and lead to distractions. This presents a challenge for AR applications that require the user to concentrate mainly on a task environment. To address this, we introduce a non-humanoid virtual assistant designed for minimal visual intrusion in AR. Our approach aims to enhance a user's focus on the tasks they need to perform. We explain our design choices based on previously published guidelines and describe our prototype implemented for an optical--see-through headset.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1097" style="margin-bottom: 0.3em;">
                <strong>A Virtual Reality Musical Instrument Integrated with a Remote Playing Robot System (ID:&nbsp;P1097)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Zhonghao Zhu,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Weizhi Nai,</span>
                        
                    
                        
                            <i>Jilin University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xin Wang,</span>
                        
                    
                        
                            <i>Jilin University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yue Liu,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1097" class="wrap-collabsible"> <input id="collapsibleabstractP1097" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1097" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Music education and performance are often constrained by geographical limitations. To evaluate the effectiveness of remote music performance, we designed a virtual reality musical instrument system with a remote playing robot. The system comprises a virtual Irish tin whistle playing system and a remote playing robot system. In the virtual playing system, sensors capture the performer's gestures, translating them into corresponding tin whistle playing commands and producing the music. The playing robot is constructed with mechanical hands, and a data transmission module is programmed to facilitate communication between the virtual playing system and the robot, enabling remote simulated performances.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1098" style="margin-bottom: 0.3em;">
                <strong>Sensory Feedback in a Serious Gaming Environment and Virtual Reality for Training Upper Limb Amputees (ID:&nbsp;P1098)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Reidner Santos Cavalcante,</span>
                        
                    
                        
                            <i>Federal University of Uberlândia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Edgard Afonso Lamounier Jr.,</span>
                        
                    
                        
                            <i>Federal University of Uberlândia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alcimar Soares,</span>
                        
                    
                        
                            <i>Faculty of Electrical Engineering, Federal University of Uberlândia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Aya Gaballa,</span>
                        
                    
                        
                            <i>Qatar University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">John Cabibihan,</span>
                        
                    
                        
                            <i>Qatar University</i>
                        
                     
                
            </p>
            
                <div id="P1098" class="wrap-collabsible"> <input id="collapsibleabstractP1098" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1098" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this article, the authors present a system based on Immersive Virtual Reality and Serious Games for training the use of prostheses by upper limb amputees with a tactile feedback. By using EMG signal processing users can control the opening and closing of a virtual prosthesis, just like in real life. Tactile feedback causes an improvement in the sensation of touch. Tests were carried out with separate groups: with and without sensory feedback with amputated and non-amputee volunteers. Tests in which users who received haptic feedback demonstrated improvements in performance compared to those who did not use haptic feedback.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1101" style="margin-bottom: 0.3em;">
                <strong>Do You XaRaoke? Immersive Realistic Singing Experience \\ with Embodied Singer (ID:&nbsp;P1101)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Germán Calcedo,</span>
                        
                    
                        
                            <i>University of Groningen;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ester Gonzalez-Sosa,</span>
                        
                    
                        
                            <i>Nokia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Diego González Morín,</span>
                        
                    
                        
                            <i>Nokia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Pablo Perez,</span>
                        
                    
                        
                            <i>Nokia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alvaro Villegas,</span>
                        
                    
                        
                            <i>Nokia</i>
                        
                     
                
            </p>
            
                <div id="P1101" class="wrap-collabsible"> <input id="collapsibleabstractP1101" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1101" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We have developed an immersive karaoke experience that allows users to sing their favorite songs in front of a simulated audience. The karaoke experience is designed within an immersive stadium scene with a simulated audience and stage lights that synchronize with the song's beats and displayed lyrics. Unlike VR-based karaoke commercial solutions, users can even see their real bodies as video-based self-avatars through the use of a deep learning network and a real microphone without using VR controllers. Preliminary results from a subset of 17 participants validate the developed prototype and provide insights for future improvements.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1102" style="margin-bottom: 0.3em;">
                <strong>Towards Optimized Cybersickness Prediction for Computationally Constrained Standalone Virtual Reality Devices (ID:&nbsp;P1102)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Md Jahirul Islam,</span>
                        
                    
                        
                            <i>Kennesaw State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rifatul Islam,</span>
                        
                    
                        
                            <i>Kennesaw State University</i>
                        
                     
                
            </p>
            
                <div id="P1102" class="wrap-collabsible"> <input id="collapsibleabstractP1102" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1102" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Cybersickness, affecting 60-95% of VR users, poses a challenge for immersive experiences.  Research using multimodal data, like pupilometry and heart rate, to predict cybersickness using complex machine-learning models often requires off-the-shelf computing resources (i.e., cloud servers), which is impractical for standalone VR devices (SVRs) as network lag and processing limitations can introduce latency during immersion, exacerbating cybersickness. We propose a novel approach that minimizes the computational cost of cybersickness prediction models by hyper-parameter tuning and reducing training parameters while maintaining prediction accuracy. Our method significantly improves training and inference time, paving the way for optimized prediction frameworks on resource-constrained SVRs.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1103" style="margin-bottom: 0.3em;">
                <strong>Enhancing Body Ownership of Avian Avatars in Virtual Reality through Multimodal Haptic Feedback (ID:&nbsp;P1103)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ziqi Wang,</span>
                        
                    
                        
                            <i>university of the arts london;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ze Gao,</span>
                        
                    
                        
                            <i>Hong Kong University of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1103" class="wrap-collabsible"> <input id="collapsibleabstractP1103" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1103" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper uses multimodal haptic feedback to enhance users' body ownership in virtual reality through wearable devices. In this case, the human is transformed into a bird, which belongs to the beyond-real transformations category in virtual reality interactions. For body transformation, wearable retractable straps can help people mimic the movement mechanism of avian bodies; for space transformation, the inflatable cushions and blowers can simulate the air resistance and lift, oxygen deprivation, and temperature decrease during the take-off process of avian avatars. The system aims to establish a realistic fidelity of the haptic feedback to enhance the user's body ownership.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1105" style="margin-bottom: 0.3em;">
                <strong>Effect of Ambulatory Conditions and Virtual Locomotion Techniques on Distance Estimation and Motion Sickness of a Navigated VR Environment (ID:&nbsp;P1105)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Aidan Morris,</span>
                        
                    
                        
                            <i>College of Holy Cross;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michael Vail,</span>
                        
                    
                        
                            <i>College of the Holy Cross;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gabriel Hanna,</span>
                        
                    
                        
                            <i>College of Holy Cross;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Anurag Rimzhim,</span>
                        
                    
                        
                            <i>College of Holy Cross</i>
                        
                     
                
            </p>
            
                <div id="P1105" class="wrap-collabsible"> <input id="collapsibleabstractP1105" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1105" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present preliminary results from a 2 X 2 between-subjects experiment. Our two IVs were ambulatory-restrictive (i.e., without locomotion) postural conditions (sitting vs. standing), and virtual navigation technique of steering versus teleporting. Participants navigated a complex virtual environment comprising outdoor and indoor environments for 10 minutes. We found that teleporting may result in less online distance estimation error than steering. Motion sickness was lower while teleporting than steering and when sitting than standing. Teleporting also resulted in better system usability than steering. We discuss the results' implications for VR usability.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1106" style="margin-bottom: 0.3em;">
                <strong>Tremor Stabilization for Sculpting Assistance in Virtual Reality (ID:&nbsp;P1106)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Layla Erb,</span>
                        
                    
                        
                            <i>Augusta University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jason Orlosky,</span>
                        
                    
                        
                            <i>Augusta University</i>
                        
                     
                
            </p>
            
                <div id="P1106" class="wrap-collabsible"> <input id="collapsibleabstractP1106" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1106" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper presents an exploration of assistive technology for virtual reality (VR) art, such as sculpting and ceramics. For many artists, tremors from Parkinsonian diseases can interfere with molding, carving, cutting, and modeling of different mediums for creating new sculptures.  To help address this, we have developed a system that algorithmically stabilizes tremors to enhance the artistic experience for creators with physical impairments or movement disorder. In addition, we present a real-time sculpting application that allows us to measure differences in sculpting actions and a target object or shape.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1108" style="margin-bottom: 0.3em;">
                <strong>Designing Indicators to Show a Robot's Physical Vision Capability (ID:&nbsp;P1108)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Hong Wang,</span>
                        
                    
                        
                            <i>University of South Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tam Do,</span>
                        
                    
                        
                            <i>College of Engineering;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhao Han,</span>
                        
                    
                        
                            <i>University of South Florida</i>
                        
                     
                
            </p>
            
                <div id="P1108" class="wrap-collabsible"> <input id="collapsibleabstractP1108" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1108" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In human-robot interaction (HRI), studies show humans can mistakenly assume that robots and humans have the same field of view, possessing an inaccurate mental model of a robot. This misperception is problematic during collaborative HRI tasks where robots might be asked to complete impossible tasks about out-of-view objects. In this initial work, we aim to align humans' mental models of robots by exploring the design of field-of-view indicators in augmented reality (AR). Specifically, we rendered nine such indicators from the head to the task space, and plan to register them onto the real robot and conduct human-subjects studies.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1112" style="margin-bottom: 0.3em;">
                <strong>Guiding Gaze: Comparing Cues for Visual Search (ID:&nbsp;P1112)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Brendan Kelley,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Christopher Wickens,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Benjamin A. Clegg,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Amelia C. Warden,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Francisco Raul Ortega,</span>
                        
                    
                        
                            <i>Colorado State University</i>
                        
                     
                
            </p>
            
                <div id="P1112" class="wrap-collabsible"> <input id="collapsibleabstractP1112" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1112" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Visual search tasks are commonplace in daily life. In cases where the time and accuracy of the search is critical (such as first responder, crisis, or military scenarios) augmented reality (AR) visual cueing is potentially beneficial. Three cue conditions (3D Arrow, 2D Wedge, and Gaze Lines) were tested in a visual search task against a baseline no cue condition. Results show that any cue is better than none, however the Gaze Line design produced the lowest search time and greatest accuracy.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1113" style="margin-bottom: 0.3em;">
                <strong>Eye direction control and reduction of discomfort by vection in HMD viewing of panoramic images (ID:&nbsp;P1113)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Seitaro Inagaki,</span>
                        
                    
                        
                            <i>Nagoya Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kenji Funahashi,</span>
                        
                    
                        
                            <i>Nagoya Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1113" class="wrap-collabsible"> <input id="collapsibleabstractP1113" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1113" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We have previously proposed an “eye direction exaggeration method.” That facilitates rearward visibility by exaggerating the angle of the eye direction when viewing panoramic images with an HMD in a seated position. In this study, we improved this exaggeration method. However, the exaggeration sometimes increased discomfort such as VR sickness. We also tried to reduce discomfort by presenting horizontally moving particles and inducing vection stably.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1114" style="margin-bottom: 0.3em;">
                <strong>Collaborative Motion Modes in Serious Game Using Virtual Co-embodiment: A Pilot Study on Usability and Agency (ID:&nbsp;P1114)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Xiongju Sun,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xiaoyi Xue,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yangyang HE,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jingjing Zhang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University</i>
                        
                     
                
            </p>
            
                <div id="P1114" class="wrap-collabsible"> <input id="collapsibleabstractP1114" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1114" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>With the increasing attention to collaboration in Serious Games, particularly in immersive virtual environments, a novel approach of virtual co-embodiment (e.g., one avatar controlled by multiple users) in recent studies has the potential to contribute to the research on collaborative motion in multiplayer games. This pilot study investigates the usability and the agency of collaborative motion modes in a virtual cycling game under the virtual co-embodiment system. Results showed that these new collaborative modes reported a higher perceived usability. Besides, based on quantitative and qualitative findings, co-embodiment modes in this virtual serious game might evoke an enhanced users' agency.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1116" style="margin-bottom: 0.3em;">
                <strong>AIsop: Exploring Immersive VR Storytelling Leveraging Generative AI (ID:&nbsp;P1116)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Elia Gatti,</span>
                        
                    
                        
                            <i>University College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniele Giunchi,</span>
                        
                    
                        
                            <i>University College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Nels Numan,</span>
                        
                    
                        
                            <i>University College London;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Anthony Steed,</span>
                        
                    
                        
                            <i>University College London</i>
                        
                     
                
            </p>
            
                <div id="P1116" class="wrap-collabsible"> <input id="collapsibleabstractP1116" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1116" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We introduce AIsop, a system that autonomously generates VR storytelling experiences using generative artificial intelligence (AI). AIsop crafts unique stories by leveraging state-of-the-art Large Language Models (LLMs) and employs Text-To-Speech (TTS) technology for narration. Further enriching the experience, a visual representation of the narrative is produced through a pipeline that pairs LLM-generated prompts with diffusion models, rendering visuals for clusters of sentences in the story. Our evaluation encompasses two distinct use cases: the narration of pre-existing content and the generation of entirely new narratives. AIsop highlights the myriad research prospects spanning its technical architecture and user engagement.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1117" style="margin-bottom: 0.3em;">
                <strong>Development and Evaluation of an AR News Interface for Efﬁcient Information Access (ID:&nbsp;P1117)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Masaru Tanaka,</span>
                        
                    
                        
                            <i>NHK Science &amp; Technology Research Laboratories;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hiroyuki Kawakita,</span>
                        
                    
                        
                            <i>Japan Broadcasting Corporation;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Takuya Handa,</span>
                        
                    
                        
                            <i>Science &amp; Technology Research Laboratories, NHK</i>
                        
                     
                
            </p>
            
                <div id="P1117" class="wrap-collabsible"> <input id="collapsibleabstractP1117" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1117" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this study, we developed a user interface (UI) for augmented reality (AR) glasses designed to allow users to browse news articles easily and efficiently. The UI uses natural language processing and dimensionality reduction techniques to place articles optimally within a virtual space. We compared this UI with two other AR-based interfaces in a user study with 13 participants, and the results show that the proposed interface reduced the time required to browse articles as well as the cognitive load of the activity.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1119" style="margin-bottom: 0.3em;">
                <strong>Progress Observation in Augmented Reality Assembly Tutorials Using Dynamic Hand Gesture Recognition (ID:&nbsp;P1119)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Tania Kaimel,</span>
                        
                    
                        
                            <i>Graz University of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ana Stanescu,</span>
                        
                    
                        
                            <i>Graz University of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Peter Mohr,</span>
                        
                    
                        
                            <i>Graz University of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dieter Schmalstieg,</span>
                        
                    
                        
                            <i>Graz University of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Denis Kalkofen,</span>
                        
                    
                        
                            <i>Graz University of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1119" class="wrap-collabsible"> <input id="collapsibleabstractP1119" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1119" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We propose a proof-of-concept augmented reality assembly tutorial application that uses a video-see-through headset to guide the user through assembly instruction steps. It is solely controlled by observing the user's physical interactions with the workpiece. The tutorial progresses automatically, making use of hand gesture classification to estimate the progression to the next instruction. For dynamic hand gesture classification, we integrate a neural network module to classify the user's hand movement in real time. We evaluate the learned model used in our application to provide insights into the performance of implicit gestural interactions.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1121" style="margin-bottom: 0.3em;">
                <strong>How Long Do I Want to Fade Away? The Duration of Fade-To-Black Transitions in Target-Based Discontinuous Travel (Teleportation) (ID:&nbsp;P1121)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Matthias Wölwer,</span>
                        
                    
                        
                            <i>University of Trier;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Benjamin Weyers,</span>
                        
                    
                        
                            <i>Trier University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel Zielasko,</span>
                        
                    
                        
                            <i>University of Trier</i>
                        
                     
                
            </p>
            
                <div id="P1121" class="wrap-collabsible"> <input id="collapsibleabstractP1121" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1121" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>A fade-to-black animation enhances the transition during teleportation, yet its duration has not been systematically explored even though it is one of the central parameters. To fill this gap, we conducted a small study to determine a preferred duration. We find a short duration of 0.3s to be the average preference, contrasting durations used previously in the literature. This research contributes to the systematic parameterization of discontinuous travel.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1122" style="margin-bottom: 0.3em;">
                <strong>Real-time shader-based shadow and occlusion rendering in AR (ID:&nbsp;P1122)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Agapi Chrysanthakopoulou,</span>
                        
                    
                        
                            <i>University of Patras;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kostantinos Moustakas,</span>
                        
                    
                        
                            <i>University of Patras</i>
                        
                     
                
            </p>
            
                <div id="P1122" class="wrap-collabsible"> <input id="collapsibleabstractP1122" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1122" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present novel methods designed to elevate the realism of augmented reality (AR) applications focusing specifically on optical see-through devices. Our work integrates shadow rendering methods for multiple light sources and dynamic occlusion culling techniques. By creating custom surface shaders we can manage multiple light sources in real-time, augmenting depth perception and spatial coherence. Furthermore, the dynamic occlusion culling system handles occluded objects, ensuring a more convincing and seamless user experience. Several cases and methods are presented with their results for various lighting and spatial conditions, promising a more enhanced and immersive user experience in various AR domains.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1125" style="margin-bottom: 0.3em;">
                <strong>Identifying Markers of Immersion Using Auditory Event-Related EEG Potentials in Virtual Reality with a Novel Protocol for Manipulating Task Difficulty  (ID:&nbsp;P1125)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Michael Ramirez,</span>
                        
                    
                        
                            <i>Universidad Escuela Colombiana de Ingeniería Julio Garavito;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hamed Tadayyoni,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Heather McCracken,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alvaro Quevedo,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Bernadette A. Murphy,</span>
                        
                    
                        
                            <i>Ontario Tech University</i>
                        
                     
                
            </p>
            
                <div id="P1125" class="wrap-collabsible"> <input id="collapsibleabstractP1125" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1125" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Immersion is defined as the degree in which the senses are engaged with the virtual environment. Recent studies have focused on investigating the role of difficulty (challenge immersion) through correlating auditory event-related potentials (ERPs) to task difficulty. This study introduces a novel experimental protocol for studying immersion in which other confounding variables than difficulty are equalized by choosing a VR jigsaw puzzle as the task in which the difficulty is only adjusted by the number of pieces. By introducing two new metrics in conformity with the metrics from literature, this work shows promise for auditory ERPs as markers of immersion.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1316" style="margin-bottom: 0.3em;">
                <strong>Challenges in the Production of a Mixed Reality Theater Dance Performance (ID:&nbsp;P1316)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Daniel Neves Coelho,</span>
                        
                    
                        
                            <i>Curvature Games;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Eike Langbehn,</span>
                        
                    
                        
                            <i>University of Applied Sciences Hamburg</i>
                        
                     
                
            </p>
            
                <div id="P1316" class="wrap-collabsible"> <input id="collapsibleabstractP1316" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1316" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual and augmented reality systems are becoming more and more popular in artistic performances. Most of these experiences are based on 360-videos or single-user applications. We produced a mixed reality theater dance performance. An audience of 30 people wore XR-headsets during a theater show. The headsets were networked and shared the same tracking space. Three performers (singer, musician, dancer) performed live and their performance was motion captured and transferred in real-time into the virtual environment. In this poster, we report our technical setup and discuss the challenges that this entails.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1310" style="margin-bottom: 0.3em;">
                <strong>Evaluation of Augmented Reality for Collaborative Environments (ID:&nbsp;P1310)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">John Dallas Cast,</span>
                        
                    
                        
                            <i>Johns Hopkins University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alejandro Martin-Gomez,</span>
                        
                    
                        
                            <i>Johns Hopkins University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mathias Unberath,</span>
                        
                    
                        
                            <i>Johns Hopkins University</i>
                        
                     
                
            </p>
            
                <div id="P1310" class="wrap-collabsible"> <input id="collapsibleabstractP1310" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1310" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present ClimbAR, an open-source, collaborative, real-time, augmented reality application running natively on the Hololens 2 that allows climbers to virtually and collaboratively set climbing holds in their physical environments to better understand and plan their routes. We present the qualitative results of demonstrating ClimbAR at two climbing gyms as well as the quantitative results of analyzing the spatial alignment accuracy of its core synchronization framework, SynchronizAR, through a proto-user study. We find an average rotational alignment error of 12.83 degrees and an average translational alignment error of 3.85 centimeters when using SynchronizAR for collaborative layout tasks involving two users.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1142" style="margin-bottom: 0.3em;">
                <strong>Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image Attacking (ID:&nbsp;P1142)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Qianyu Guo,</span>
                        
                    
                        
                            <i>Purdue University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jiaming Fu,</span>
                        
                    
                        
                            <i>Purdue University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yawen Lu,</span>
                        
                    
                        
                            <i>Purdue Univ;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dongming Gan,</span>
                        
                    
                        
                            <i>Polytechnic Institute</i>
                        
                     
                
            </p>
            
                <div id="P1142" class="wrap-collabsible"> <input id="collapsibleabstractP1142" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1142" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In Virtual Reality (VR), adversarial attack remains a significant security threat. Most deep learning-based methods for physical and digital adversarial attacks focus on enhancing attack performance by crafting adversarial examples that contain large printable distortions that are easy for human observers to identify. However, attackers rarely impose limitations on the naturalness and comfort of the appearance of the generated attack image, resulting in a noticeable and unnatural attack. To address this challenge, we propose a framework to incorporate style transfer to craft adversarial inputs of natural styles that exhibit minimal detectability and maximum natural appearance, while maintaining superior attack capabilities.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1344" style="margin-bottom: 0.3em;">
                <strong>Plausible and Diverse Human Hand Grasping Motion Generation (ID:&nbsp;P1344)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Xiaoyuan Wang,</span>
                        
                    
                        
                            <i>IRISA;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yang Li,</span>
                        
                    
                        
                            <i>East China Normal University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Changbo Wang,</span>
                        
                    
                        
                            <i>Depart of Software Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marc Christie,</span>
                        
                    
                        
                            <i>IRISA</i>
                        
                     
                
            </p>
            
                <div id="P1344" class="wrap-collabsible"> <input id="collapsibleabstractP1344" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1344" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Techniques to grasp targeted objects in realistic and diverse ways find many applications in computer graphics, robotics and VR. This study generates diverse grasping motions while keeping plausible final grasps for human hands. We first build on a Transformer-based VAE to encode diverse reaching motions into a latent representation noted as GMF and then train an MLP-based cVAE to learn the grasping affordance of targeted objects. Finally, through learning a denoising process, we condition GMF with affordance to generate grasping motions for the targeted object. We identify improvements in our results, and will further address them in future work.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1212" style="margin-bottom: 0.3em;">
                <strong>DVIO - Distributed Visual Inertial Odometry in a Multi-user Environment (ID:&nbsp;P1212)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Mathieu Lutfallah,</span>
                        
                    
                        
                            <i>ETH Zurich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Juyi Zhang,</span>
                        
                    
                        
                            <i>ETH Zurich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Andreas Kunz,</span>
                        
                    
                        
                            <i>ETH Zurich</i>
                        
                     
                
            </p>
            
                <div id="P1212" class="wrap-collabsible"> <input id="collapsibleabstractP1212" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1212" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Head-mounted displays typically use a visual inertial odometry system, which relies on the headset's camera combined with Inertial Measurement Units. While effective, this setup fails if the camera is obstructed or if the environments lacks features. Traditional recalibration methods like place recognition often fall short in such settings. Addressing this, the paper proposes a novel distributed tracking method that uses the positions of other users. This approach creates a network or "daisy chain" of user locations, enhancing position tracking accuracy. It serves as an alternative and a supplementary solution to the standard system, ensuring precise location tracking for all users.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1216" style="margin-bottom: 0.3em;">
                <strong>AccompliceVR: Lending Assistance to Immersed Users by Adding a Generic Collaborative Layer (ID:&nbsp;P1216)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Anthony Steed,</span>
                        
                    
                        
                            <i>University College London</i>
                        
                     
                
            </p>
            
                <div id="P1216" class="wrap-collabsible"> <input id="collapsibleabstractP1216" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1216" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The current model of development for virtual reality applications is that a single application is responsible for construction of the complete immersive experience. If the application is collaborative that application must implement the functionality for sharing. We present VRAccomplice, and overlay application that add a collaboration layer to applications running on SteamVR. Using the Ubiq software, we can add avatars controlled by remote users as an overlay into any running app used by a local user. Remote users can see video of the local user. We demonstrate this in some common SteamVR games.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1174" style="margin-bottom: 0.3em;">
                <strong>Enacting Molecular Interactions in VR: Preliminary relationships between visual navigation and learning outcomes (ID:&nbsp;P1174)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Julianna C Washington,</span>
                        
                    
                        
                            <i>Southern Methodist University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Prajakt Pande,</span>
                        
                    
                        
                            <i>Southern Methodist University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Praveen Ramasamy,</span>
                        
                    
                        
                            <i>Danish Technological Institute;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Morten E. Moeller,</span>
                        
                    
                        
                            <i>University College Copenhagen;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Biljana Mojsoska,</span>
                        
                    
                        
                            <i>Roskilde University</i>
                        
                     
                
            </p>
            
                <div id="P1174" class="wrap-collabsible"> <input id="collapsibleabstractP1174" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1174" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Twenty-three undergraduates participated in a pre-post quasi-experimental single-group study involving an immersive VR simulation which allowed them to embody (i.e. become) a biomolecule and enact/experience its molecular interactions at a microscopic level using actions and gestures. Based on initial data analyses from this study, the present poster reports preliminary findings on the relationships between the participants' visual navigation, and conceptual as well as affective learning outcomes.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1149" style="margin-bottom: 0.3em;">
                <strong>Autonomous avatar for customer service training VR system (ID:&nbsp;P1149)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Takenori Hara,</span>
                        
                    
                        
                            <i>Dai Nippon Printing Co., Ltd.</i>
                        
                     
                
            </p>
            
                <div id="P1149" class="wrap-collabsible"> <input id="collapsibleabstractP1149" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1149" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>By immersing trainees in a virtual space and conducting customer service training with customer avatars, physical training facilities are no longer required and customer service training costs can be reduced. Furthermore, since there is no need for travel time to the training facility, trainees can easily participate in training even from remote locations. However, the production cost of customer avatars that behave according to training scenarios has become a new issue in social implementation. Therefore, we conducted a preliminary implementation experiment of a customer avatar that works autonomously by incorporating LLM and reported the findings and problems we encountered.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
</div>
<div>
    <h2 id="P2" class="pink" style="padding-top:25px;">Tuesday Posters</h2>
    <p class="small">Talk with the authors: 9:45&#8209;10:15, 13:00&#8209;13:30, 15:00&#8209;15:30, 17:00&#8209;17:30, Room: Sorcerer's Apprentice Ballroom</p>  
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1127" style="margin-bottom: 0.3em;">
                <strong>Using immersive video to recall significant musical experiences in elderly population with intellectual disability (ID:&nbsp;P1127)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Pablo Perez,</span>
                        
                    
                        
                            <i>Nokia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marta Orduna,</span>
                        
                    
                        
                            <i>Nokia Spain;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">María Nava-Ruiz,</span>
                        
                    
                        
                            <i>Fundación Juan XXIII;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Javier Martín-Boix,</span>
                        
                    
                        
                            <i>Fundación Juan XXIII</i>
                        
                     
                
            </p>
            
                <div id="P1127" class="wrap-collabsible"> <input id="collapsibleabstractP1127" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1127" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Comodia Elderly project studies the use of immersive video inelderly population with intellectual disability. Participants in the project took part in live concerts, which were recorded with a 180-degree stereoscopic camera. Recording were visualized in succesive sessions of virtual reality video. Preliminary results show a high level of spacial and social presence, assessed both from adapted questionnaires and for external observation of observable signs.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1128" style="margin-bottom: 0.3em;">
                <strong>Target Selection with Avatars in Mixed Reality (ID:&nbsp;P1128)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Eric DeMarbre,</span>
                        
                    
                        
                            <i>Carleton University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Robert J Teather,</span>
                        
                    
                        
                            <i>Carleton University</i>
                        
                     
                
            </p>
            
                <div id="P1128" class="wrap-collabsible"> <input id="collapsibleabstractP1128" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1128" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This poster presents a Fitts' law experiment evaluating the effects of using an avatar in mixed and virtual reality selection tasks. The avatar had little to no impact on efficiency and surprisingly lowered overall accuracy in both the 2D plane and depth. However, the avatar also reduced variability in depth selection. Avatar design and specific MR hardware parameters may significantly impact efficiency, especially compared to VR devices.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1130" style="margin-bottom: 0.3em;">
                <strong>Georeferenced 360-Degree Photos for Enhancing Navigation and Interaction within Virtual Electric Power Substations (ID:&nbsp;P1130)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Gabriel Fernandes Cyrino,</span>
                        
                    
                        
                            <i>Federal University of Uberlândia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Claudemir José Alves,</span>
                        
                    
                        
                            <i>Federal University of Uberlândia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gerson FlÌÁvio Mendes de Lima,</span>
                        
                    
                        
                            <i>Federal University of Uberlândia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Edgard Afonso Lamounier Jr.,</span>
                        
                    
                        
                            <i>Federal University of Uberlândia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alexandre Cardoso,</span>
                        
                    
                        
                            <i>Federal University of Uberlândia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ana Marotti,</span>
                        
                    
                        
                            <i>Eletrobras;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ricardo Oliveira,</span>
                        
                    
                        
                            <i>Eletrobras</i>
                        
                     
                
            </p>
            
                <div id="P1130" class="wrap-collabsible"> <input id="collapsibleabstractP1130" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1130" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This work introduces a methodology for improving virtual navigation and interaction within power substations by employing georeferenced 360-degree photos. The objective is to swiftly update the current state of the field since, in most cases, incorporating such amendments is not feasible during the reconstruction of virtual environments. This changing speed is very important for critical systems. Preliminary results demonstrate successful updating rates, enabling engineers to make rapid decisions. It is expected that the proposed methodology can improve the efficiency and dependability of step-by-step activities, while also reducing the time and costs associated with system maintenance.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1131" style="margin-bottom: 0.3em;">
                <strong>Exploring the Impact of Virtual Human and Symbol-based Guide Cues in Immersive VR on Real-World Navigation Experience (ID:&nbsp;P1131)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Omar Khan,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Anh Nguyen,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michael Francis,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kangsoo Kim,</span>
                        
                    
                        
                            <i>University of Calgary</i>
                        
                     
                
            </p>
            
                <div id="P1131" class="wrap-collabsible"> <input id="collapsibleabstractP1131" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1131" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this paper, we explore how navigation performance and experience in a real-world indoor environment is impacted after learning the route from various guide cues in a replicated immersive virtual environment. A guide system, featuring two distinct audiovisual guide representations—a human agent guide and a symbol-based guide—was developed and evaluated through a preliminary user study. The results do not show significant differences between the two guide conditions, but offer insight into the user-perceived confidence and enjoyment of the real-world navigation task after experiencing the route in immersive virtual reality. We discuss the results and direction of future research.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1132" style="margin-bottom: 0.3em;">
                <strong>Effects of Nonverbal Communication of Virtual Agents on Social Pressure and Encouragement in VR (ID:&nbsp;P1132)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Pascal Martinez Pankotsch,</span>
                        
                    
                        
                            <i>University of Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sebastian Oberdörfer,</span>
                        
                    
                        
                            <i>University of Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marc Erich Latoschik,</span>
                        
                    
                        
                            <i>University of Würzburg</i>
                        
                     
                
            </p>
            
                <div id="P1132" class="wrap-collabsible"> <input id="collapsibleabstractP1132" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1132" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Our study investigated how virtual agents impact users in challenging VR environments, exploring if nonverbal animations affect social pressure, positive encouragement, and trust in 30 female participants. Despite showing signs of pressure and support during the experimental trials, we could not find significant differences in post-exposure measurements of social pressure and encouragement, interpersonal trust, and well-being. While inconclusive, the findings suggest potential, indicating the need for further research with improved animations and a larger sample size for validation.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1133" style="margin-bottom: 0.3em;">
                <strong>Flowing with Zen: Exploring Empowering the Dissemination of Intangible Cultural Heritage via Immersive Mixed Reality Spaces (ID:&nbsp;P1133)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Wenchen Guo,</span>
                        
                    
                        
                            <i>Peking University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Guoyu Sun,</span>
                        
                    
                        
                            <i>Communication University of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wenbo Zhao,</span>
                        
                    
                        
                            <i>Communication University of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhirui Chen,</span>
                        
                    
                        
                            <i>University of Chinese Academy of Social Sciences;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Menghan Shi,</span>
                        
                    
                        
                            <i>Lancaster University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Weiyue Lin,</span>
                        
                    
                        
                            <i>Peking University</i>
                        
                     
                
            </p>
            
                <div id="P1133" class="wrap-collabsible"> <input id="collapsibleabstractP1133" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1133" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Zen is a treasure of the world's intangible cultural heritage (ICH), but nowadays it is facing difficulties in dissemination. This paper presents an immersive experience space Flowing with Zen by integrating HCI technology and MR. Audiences can explore and interact with the four scenarios, as well as meditate, and experience Zen philosophy. The pilot study shows that the MR space not only evokes users' interest and participation but also deepens their empathy or reflections. This innovative way of combining MR, ICH, and UX enhances the accessibility of Zen, and it may open up a new mode of dissemination for ICH.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1134" style="margin-bottom: 0.3em;">
                <strong>Lessons Learned in Designing Racially Diverse Androgynous Avatars (ID:&nbsp;P1134)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Camille Isabella Protko,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ryan P. McMahan,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tiffany D. Do,</span>
                        
                    
                        
                            <i>University of Central Florida</i>
                        
                     
                
            </p>
            
                <div id="P1134" class="wrap-collabsible"> <input id="collapsibleabstractP1134" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1134" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>As virtual reality technology evolves, avatars play a crucial role in user representation, yet options for gender-diverse individuals remain limited. The goal of this research was to develop 14 racially diverse androgynous avatars using the design guidelines recommended in prior work. However, a perceptual experiment involving 68 participants revealed unexpected results, as most avatars were perceived as predominantly masculine. Additionally, our results yielded discrepancies in perceived gender across different racial identities, as the same design process resulted in widely varying perceptions. Despite these challenges, our research highlights the importance of continued exploration to improve the creation of inclusive avatars.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1135" style="margin-bottom: 0.3em;">
                <strong>Sexual Presence in Virtual Reality: A Psychophysiological Exploration (ID:&nbsp;P1135)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Sara Saint-Pierre Cote,</span>
                        
                    
                        
                            <i>École de technologie supérieure</i>
                        
                     
                
            </p>
            
                <div id="P1135" class="wrap-collabsible"> <input id="collapsibleabstractP1135" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1135" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The increasing use of immersive technologies for sexual purposes raises questions about their capacity to enhance a unique aspect of presence—Sexual Presence (SP). Investigating this phenomenon hinges on our ability to measure it accurately. This paper improves our understanding of SP by identifying potential quantitative electroencephalography variables associated with SP. Twelve heterosexual cisgender males were exposed to virtual scenarios featuring sexual content performed by a Virtual Character (VC). After viewing, participants completed a Sexual Presence questionnaire. A correlation was observed between self-reported SP and the alpha band activity in the frontal and parietal regions.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1138" style="margin-bottom: 0.3em;">
                <strong>NavigAR: Enhancing Localized Space Navigation using Augmented Reality (ID:&nbsp;P1138)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Sahil Deshpande,</span>
                        
                    
                        
                            <i>Indraprastha Institute of Information Technology - Delhi (IIITD);</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yaksh Patel Yaksh,</span>
                        
                    
                        
                            <i>Indraprastha Institute of Information Technology - Delhi (IIITD);</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rahul Ajith,</span>
                        
                    
                        
                            <i>Indraprastha Institute of Information Technology - Delhi (IIITD);</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Anmol Srivastava,</span>
                        
                    
                        
                            <i>Indraprastha Institute of Information Technology Delhi</i>
                        
                     
                
            </p>
            
                <div id="P1138" class="wrap-collabsible"> <input id="collapsibleabstractP1138" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1138" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this study, we use Augmented Reality (AR) to project and let users interact with a 3D-replicated model of a fenced space to understand how AR can improve navigation and enhance space retention by improving wayfinding via recall. We recreated a fenced space in 3D using Blender. Users can see their current position and, using onscreen buttons, see routes to their destinations. We found that users could associate the 3D models of the buildings with their real-world counterparts. We observed that users were better able to navigate the campus after using the application.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1139" style="margin-bottom: 0.3em;">
                <strong>Exploring Radiance Field Content Generation for Virtual Reality (ID:&nbsp;P1139)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Asif Sijan,</span>
                        
                    
                        
                            <i>University of Minnesota Duluth;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Peter Willemsen,</span>
                        
                    
                        
                            <i>University of Minnesota Duluth</i>
                        
                     
                
            </p>
            
                <div id="P1139" class="wrap-collabsible"> <input id="collapsibleabstractP1139" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1139" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Generating content for virtual reality takes effort and expanding how content can be generated has the potential to increase participation by a wider range of users well outside virtual reality researchers and programmers. This work explores some current content acquisition and generation techniques for replicating real world 3D scenes and objects to better understand the practical use and application of these techniques. The techniques explored focus on the recent emergence of radiance field-based methods and their potential to replace previous scene/object-generating techniques.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1141" style="margin-bottom: 0.3em;">
                <strong>Brain Dynamics of Balance Loss in Virtual Reality and Real-world Beam Walking (ID:&nbsp;P1141)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Amanda Studnicki,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ahmed Rageeb Ahsan,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Eric Ragan,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel P. Ferris,</span>
                        
                    
                        
                            <i>University of Florida</i>
                        
                     
                
            </p>
            
                <div id="P1141" class="wrap-collabsible"> <input id="collapsibleabstractP1141" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1141" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality (VR) aims to replicate the sensation of a genuine experience through the integration of realism, presence, and embodiment. In this study, we used mobile electroencephalography to quantify differences in anterior cingulate brain activity, an area involved in error monitoring, with and without VR during a challenging balance task to discern the factors contributing to VR's perceptual shortcomings. We found a major delay in the anterior cingulate response to self-generated loss of balance in VR compared to the real world. We also found a robust response in the anterior cingulate when loss of balance was generated by external disturbance.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1144" style="margin-bottom: 0.3em;">
                <strong>Investigating the Impact of Virtual Avatars and Owner Gender on Virtual Partner Selection in Avatar-based Interactions (ID:&nbsp;P1144)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Anh Nguyen,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Seoyoung Kang,</span>
                        
                    
                        
                            <i>KAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Woontack Woo,</span>
                        
                    
                        
                            <i>KAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kangsoo Kim,</span>
                        
                    
                        
                            <i>University of Calgary</i>
                        
                     
                
            </p>
            
                <div id="P1144" class="wrap-collabsible"> <input id="collapsibleabstractP1144" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1144" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>As real-life social interactions transition to virtual environments using Virtual/Augmented Reality (VR/AR) technologies, understanding how gender representation in virtual avatars affects choices and behaviors becomes increasingly relevant. In this paper, we investigate the combined impact of avatars' gender representation and their owner's gender on virtual partner selection in physical, intellectual, social, and romantic scenarios. We introduce our preliminary research plan, outlining both the study design and system development. The research will contribute to our understanding of social dynamics and gender effects in avatar-mediated interactions.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1145" style="margin-bottom: 0.3em;">
                <strong>An exploration on modeling haptic reaction time of 3D interactive tasks within virtual environments  (ID:&nbsp;P1145)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Stanley Tarng,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yaoping Hu,</span>
                        
                    
                        
                            <i>University of Calgary</i>
                        
                     
                
            </p>
            
                <div id="P1145" class="wrap-collabsible"> <input id="collapsibleabstractP1145" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1145" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual environments (VEs) utilize haptic cues - e.g., vibrotactile and force feedbacks - to facilitate user interactions in 3D tasks. Existing studies reported maximum likelihood estimation (MLE) to integrate cues of different modalities. The MLE integration was excluded for cues of the same modality. Although proportional likelihood estimation (PLE) was able to integrate same-modality cues for the parameter of task accuracy, its applicability remains unclear to other task parameters like reaction time. This feasibility study compared thus MLE and PLE to integrate the haptic cues for reaction time. PLE was found to be applicable to model haptic reaction time.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1148" style="margin-bottom: 0.3em;">
                <strong>HiLoTEL: Virtual Reality Robot Interface-Based Human-in-the-Loop Task Execution and Learning in the Physical World Through Its Digital Twin (ID:&nbsp;P1148)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Amanuel Ergogo,</span>
                        
                    
                        
                            <i>SANO Centre for Computational Personalized Medicine;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Diego Dall'Alba,</span>
                        
                    
                        
                            <i>SANO Centre for Computational Personalized Medicine;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Przemysław Korzeniowski,</span>
                        
                    
                        
                            <i>Sano Centre for Computational Medicine</i>
                        
                     
                
            </p>
            
                <div id="P1148" class="wrap-collabsible"> <input id="collapsibleabstractP1148" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1148" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>HiLoTEL is a flexible virtual-reality framework for executing and learning tasks in virtual and physical environments. It enables human experts to collaborate with learning agents and intervene when necessary through human-in-the-loop imitation learning. HiLoTEL reduces the need to carry out repetitive tasks, providing the user with an intuitive supervision interface. The system is tested on Pick-and-Place task, considering both teleoperated and passthrough interaction modalities. The results show that HiLoTEL improves success rates while maintaining human-level completion time and providing users with 71% hands free supervision time, thus enabling effective human-robot collaboration.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1154" style="margin-bottom: 0.3em;">
                <strong>Design and Analysis of Interaction Method to Adjust Magnification Function Using Microgestures in VR or AR Applications (ID:&nbsp;P1154)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Hao Sun,</span>
                        
                    
                        
                            <i>Beijing Insitute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Shining Ma,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mingwei Hu,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Weitao Song,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yue Liu,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1154" class="wrap-collabsible"> <input id="collapsibleabstractP1154" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1154" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Among various interaction modes in VR/AR, microgestures have distinct advantages over controllers by reducing fatigue and improving the efficiency. In this paper, we proposed a microgesture set that utilizes the number of fingers as input instructions for magnification adjustment in VR environments. To evaluate the effectiveness of the proposed microgestures, a series of tasks have been designed in three scenes. The results revealed a significant improvement in completion time and subjective measures compared to the performance of controller. These findings offer valuable insights for future gesture design, contributing to the development of more efficient and user-friendly interaction techniques in VR.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1156" style="margin-bottom: 0.3em;">
                <strong>Voicing Your Emotion: Integrating Emotion and Identity in Cross-Modal 3D Facial Animations (ID:&nbsp;P1156)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">wenfeng song,</span>
                        
                    
                        
                            <i>Beijing information science and technology university;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhenyu Lv,</span>
                        
                    
                        
                            <i>Beijing Information Science and Technology University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">wang xuan,</span>
                        
                    
                        
                            <i>Beijing Information Science &amp; Technology Universit;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xia Hou,</span>
                        
                    
                        
                            <i>Beijing information science and technology university</i>
                        
                     
                
            </p>
            
                <div id="P1156" class="wrap-collabsible"> <input id="collapsibleabstractP1156" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1156" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Speech-driven 3D facial animation is widely applied in VR fields.Capturing intricate expressiveness remains an intricate challenge. Addressing this deficiency, we unveil a method tailored to produce 3D facial expressions that resonate deeply with emotion and identity, guided by speech and user-provided prompt words. Our key insight is an emotion-identity fusion mechanism, a pre-trained self-reconstruction codebook, meticulously crafted from a wide array of emotional facial movements, serving as an expressive motion benchmark. Using this foundation, prompt words are seamlessly transformed into vivid facial representations. Our approach is a robust tool for crafting 3D talking avatars, rich in emotional depth and distinctive identity.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1158" style="margin-bottom: 0.3em;">
                <strong>Using VR in a Two-Month University Course (ID:&nbsp;P1158)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jose Joskowicz,</span>
                        
                    
                        
                            <i>Facultad de Ingenieria;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Fabricio Gonzalez,</span>
                        
                    
                        
                            <i>Quantik;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Inés Urrestarazu,</span>
                        
                    
                        
                            <i>Facultad de Ciencias Económicas</i>
                        
                     
                
            </p>
            
                <div id="P1158" class="wrap-collabsible"> <input id="collapsibleabstractP1158" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1158" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper describes the experience of using VR in a two-month University course. The experience was performed in the School of Economics, University of the Republic, Uruguay. Fourteen students and the professors attended the “Accounting in integrated management systems” class using Meta Quest 2 VR headsets during seven 1-hour sessions, one session per week. Different aspects were analyzed during the sessions, including audiovisual quality, comfort, sickness, immersion, presence, fatigue, cognitive load, and useful of the technology for the academic purposes.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1161" style="margin-bottom: 0.3em;">
                <strong>Investigating Situated Learning Theory through an Augmented Reality Mobile Assistant for Everyday STEM Learning (ID:&nbsp;P1161)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Abhishek Mayuresh Kulkarni,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Cecelia Albright,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Pratik Kamble,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sharon Lynn Chu,</span>
                        
                    
                        
                            <i>University of Florida</i>
                        
                     
                
            </p>
            
                <div id="P1161" class="wrap-collabsible"> <input id="collapsibleabstractP1161" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1161" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Situated learning theory (SLT) suggests learning should take place within authentic contexts to be effective. Research in virtual and augmented reality (AR) tend to use SLT to ground their work. Yet, the effectiveness of SLT to ground designs is still questionable.  This work investigates situated learning through an AR mobile application called Objectica that seeks to teach STEM (Science, Technology, Engineering, Mathematics) concepts using authentic everyday objects. A between-subjects study compared Objectica with a version not grounded in SLT. Initial results show no significant differences in the effectiveness of the two versions, questioning impact of SLT in educational app design.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1166" style="margin-bottom: 0.3em;">
                <strong>Orienting response is modulated by the human-likeness and realism of the virtual proposer. Exploratory study with physiological measurement. (ID:&nbsp;P1166)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Radoslaw Sterna,</span>
                        
                    
                        
                            <i>Jagiellonian University in Kraków;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Joanna Pilarczyk,</span>
                        
                    
                        
                            <i>Institute of Psychology, Faculty of Philosophy, Jagiellonian University in Kraków;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Agata Szymańska,</span>
                        
                    
                        
                            <i>Institute of Psychology, Faculty of Philosophy, Jagiellonian University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jakub Szczugieł,</span>
                        
                    
                        
                            <i>Jagiellonian University in Kraków;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Magdalena Igras-Cybulska,</span>
                        
                    
                        
                            <i>AGH UST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michał Kuniecki,</span>
                        
                    
                        
                            <i>Institute of Psychology, Faculty of Philosophy, Jagiellonian University in Kraków</i>
                        
                     
                
            </p>
            
                <div id="P1166" class="wrap-collabsible"> <input id="collapsibleabstractP1166" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1166" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This poster presents an exploratory analysis investigating the impact of virtual character's realism and human-likeness on participants' Orienting Response (OR) indexed by heart rate (HR) and skin conductance response (SCR). Fifty-nine participants, wearing HMD, watched the video recordings of the virtual characters and humans differing in terms of behavioral realism (movement and gaze), while their physiological responses were measured. Findings highlight a significant influence of behavioral realism on both indices of the Orienting Response (deeper HR deceleration and stronger SCR), which in case of SCR is further modulated by human-likeness of the virtual proposer</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1167" style="margin-bottom: 0.3em;">
                <strong>The validation of a Polish version of Co-Presence and Social Presence Scale. (ID:&nbsp;P1167)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Radoslaw Sterna,</span>
                        
                    
                        
                            <i>Jagiellonian University in Kraków;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Natalia Lipp,</span>
                        
                    
                        
                            <i>Sano Center for Computational Personalised Medicine;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Agnieszka Strojny,</span>
                        
                    
                        
                            <i>Institute of Applied Psychology Faculty of Management and Social Communication, Jagiellonian University, Kraków, Poland;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michał Kuniecki,</span>
                        
                    
                        
                            <i>Institute of Psychology, Faculty of Philosophy, Jagiellonian University in Kraków;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Paweł Strojny,</span>
                        
                    
                        
                            <i>Institute of Applied Psychology, Faculty of Management and Social Communication, Jagiellonian University in Kraków</i>
                        
                     
                
            </p>
            
                <div id="P1167" class="wrap-collabsible"> <input id="collapsibleabstractP1167" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1167" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study validates the Polish translation of the Co-presence and Social Presence scale, confirming strong reliability and validity through robust internal consistency and favorable confirmatory factor analysis fit indices. Convergent validity results align with expectations, although correlation magnitudes are lower than anticipated. Examining discriminant validity, an unexpected weak positive correlation with eeriness challenges initial expectations of negative relationship. Furthermore, moderate to low correlations between co-presence and presence emphasize their distinct yet related nature</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1169" style="margin-bottom: 0.3em;">
                <strong>XR Slate: XR Swiping and Layout Adjustment for Text Entry (ID:&nbsp;P1169)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Theodore Okamura,</span>
                        
                    
                        
                            <i>University of North Carolina at Greensboro;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Regis Kopper,</span>
                        
                    
                        
                            <i>University of North Carolina at Greensboro</i>
                        
                     
                
            </p>
            
                <div id="P1169" class="wrap-collabsible"> <input id="collapsibleabstractP1169" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1169" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In the expansive realm of virtual reality (VR) and digital interfaces, this project introduces XR Slate, an innovative text entry method to address the challenges associated with the prevailing ray-casting approach. XR Slate mitigates ray-casting difficulty in precisely hitting specific keys due by progressively refining the virtual keyboard using intuitive swiping gestures, eliminating unwanted keys and enhancing the accessibility and user-friendliness of the text entry process.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1170" style="margin-bottom: 0.3em;">
                <strong>Evaluating NeRF Fidelity using Virtual Environments (ID:&nbsp;P1170)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ander J Talley,</span>
                        
                    
                        
                            <i>Mississippi State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Adam Jones,</span>
                        
                    
                        
                            <i>Mississippi State University</i>
                        
                     
                
            </p>
            
                <div id="P1170" class="wrap-collabsible"> <input id="collapsibleabstractP1170" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1170" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Neural Radiance Fields (NeRF) are a promising form of 3D Reconstruction that utilize sparse 2D imagery to recreate synthetic and real scenes. Since NeRF was first developed, numerous methods and techniques have worked upon the original algorithm to improve its accuracy, fidelity, and speed. We aim to evaluate the fidelity of a reconstructed scene, as well as evaluate the quality of the reconstruction.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1171" style="margin-bottom: 0.3em;">
                <strong>Prototyping Autonomous Vehicle Lane Detection for Snow in VR (ID:&nbsp;P1171)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Tatiana Ortegon Sarmiento,</span>
                        
                    
                        
                            <i>Université du Québec à Trois-Rivières;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alvaro Uribe Quevedo,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sousso Kelouwani,</span>
                        
                    
                        
                            <i>Université du Québec à Trois-Rivières;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Patricia Paderewski Rodriguez,</span>
                        
                    
                        
                            <i>Universidad de Granada;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Francisco Gutierrez Vela,</span>
                        
                    
                        
                            <i>Universidad de Granada</i>
                        
                     
                
            </p>
            
                <div id="P1171" class="wrap-collabsible"> <input id="collapsibleabstractP1171" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1171" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Autonomous vehicles (AVs) are gaining momentum, and features such as autopilot are becoming widespread among consumer vehicles. AVs track the road to drive correctly, however, when they fail, the driver must take over. Lane detection is a must-have feature for AVs, which has numerous investigations. Nevertheless, most have gaps in extreme weather, such as snowy winters. The lack of snow datasets adds to this, as most include mild scenarios. This paper presents the prototype of a virtual reality digital twin that will allow training lane detection using synthetic data that would otherwise be difficult to recreate in real life.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1175" style="margin-bottom: 0.3em;">
                <strong>First Steps in Constructing an AI-Powered Digital Twin Teacher: Harnessing Large Language Models in a Metaverse Classroom (ID:&nbsp;P1175)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Marco Fiore,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michele Gattullo,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marina Mongiello,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari</i>
                        
                     
                
            </p>
            
                <div id="P1175" class="wrap-collabsible"> <input id="collapsibleabstractP1175" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1175" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study proposes a ground-breaking idea at the intersection of artificial intelligence and virtual education: the creation of an AI-powered digital twin instructor in a Metaverse-based classroom using Large Language Models. We aim to build a teacher avatar capable of dynamic interactions with students, tailored teaching approaches, and contextual response inside a virtual world. The research aims to address two major issues for both students and teachers: the digital twin can provide feedbacks to resolve doubts about course content and material; also, it can improve student management and allow teachers to answer the trickiest questions raised by students.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1176" style="margin-bottom: 0.3em;">
                <strong>Merging Blockchain and Augmented Reality for an Immersive Traceability Platform (ID:&nbsp;P1176)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Marco Fiore,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michele Gattullo,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marina Mongiello,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Antonio E. Uva,</span>
                        
                    
                        
                            <i>Polytechnic Institute of Bari</i>
                        
                     
                
            </p>
            
                <div id="P1176" class="wrap-collabsible"> <input id="collapsibleabstractP1176" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1176" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The demand for ethically sourced and safe products has surged, prompting industries to adopt intricate traceability systems. Blockchain technology, renowned for its decentralized and immutable ledger, revolutionizes traceability by ensuring data integrity and transparency in supply chains. However, complexities within supply chains often obfuscate meaningful insights for consumers. This paper explores leveraging Augmented Reality to enhance Blockchain-based traceability systems. By integrating AR, consumers can seamlessly access traceability information through QR codes, presented via optimized 3D models. This immersive approach fosters trust by visually demonstrating product quality. The architecture combines QR codes, Vuforia markers, and Blockchain, ensuring data security and immutability.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1178" style="margin-bottom: 0.3em;">
                <strong>Using Machine Learning to Classify EEG Data Collected With or Without Haptic Feedback During a Simulated Drilling Task  (ID:&nbsp;P1178)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Michael Ramirez,</span>
                        
                    
                        
                            <i>Universidad Escuela Colombiana de Ingeniería Julio Garavito;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Heather McCracken,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Brianna L Grant,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alvaro Quevedo,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Paul Yielder,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Bernadette A. Murphy,</span>
                        
                    
                        
                            <i>Ontario Tech University</i>
                        
                     
                
            </p>
            
                <div id="P1178" class="wrap-collabsible"> <input id="collapsibleabstractP1178" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1178" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Simulation environments (SE) are becoming important tools that can be leveraged to implement training protocols and educational resources. Electroencephalography (EEG) is used to compare the effects of different types of feedback in SE, but it can be challenging to know which aspects represent the impact of those feedback on neural processing. For this study, machine learning approaches were applied to differentiate neural circuitry associated with haptic and non-haptic feedback in a simulated drilling task. Electroencephalography was analyzed based on the extraction and selection of different types of features. Trials with haptic feedback were correctly identified from those without haptic feedback.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1181" style="margin-bottom: 0.3em;">
                <strong>Gender Identification of VR Users by Machine Learning Tracking Data (ID:&nbsp;P1181)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Qidi J. Wang,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ryan P. McMahan,</span>
                        
                    
                        
                            <i>University of Central Florida</i>
                        
                     
                
            </p>
            
                <div id="P1181" class="wrap-collabsible"> <input id="collapsibleabstractP1181" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1181" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Gender identification of virtual reality (VR) users by machine learning tracking data could afford personalized experiences, including mitigation of expected human factors or psychology issues. While much research has recently been conducted to identify individual users given their VR tracking data, little research has investigated gender identification. Furthermore, nearly all prior studies have only considered positions and rotations of all the devices. We present a systematic investigation of different combinations and spatial representations of VR tracked devices for predicting a user's gender. Our results indicate head rotations are integral to gender identification while head positions are surprisingly not as important.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1182" style="margin-bottom: 0.3em;">
                <strong>TacPoint: Influence of Partial Visuo-Tactile Feedback on Sense of Embodiment in Virtual Reality (ID:&nbsp;P1182)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jingjing Zhang,</span>
                        
                    
                        
                            <i>University of Liverpool;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mengjie Huang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yonglin Chen,</span>
                        
                    
                        
                            <i>City University of Macau;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xiaoyi Xue,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kailun Liao,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rui Yang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jiajia Shi,</span>
                        
                    
                        
                            <i>Kunshan Rehabilitation Hospital</i>
                        
                     
                
            </p>
            
                <div id="P1182" class="wrap-collabsible"> <input id="collapsibleabstractP1182" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1182" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The employment of Virtual Reality in medical rehabilitation has been broadened to incorporate visual and tactile feedback, while how patients use tangible objects to induce better perceptions in VR remains unexplored. We investigated how partial visuo-tactile feedback influences users' embodiment, and proposed a design idea named TacPoint (a red point mark) that connects to the physical and virtual world. The results reported that higher embodiment illusions could be induced in the TacPoint session than in others without feedback during virtual interactions. TacPoint could help patients induce embodiment easily to increase their positive VR experience for further rehabilitation training.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1183" style="margin-bottom: 0.3em;">
                <strong>Exploring Trustful AI Augmentation with Virtual Evacuation Study (ID:&nbsp;P1183)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ruohan Wang,</span>
                        
                    
                        
                            <i>Marvin Ridge High School;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Aidong Lu,</span>
                        
                    
                        
                            <i>University of North Carolina at Charlotte</i>
                        
                     
                
            </p>
            
                <div id="P1183" class="wrap-collabsible"> <input id="collapsibleabstractP1183" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1183" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This project created a virtual evacuation study to explore people's trust of AI in dangerous situations. Our study adopts a virtual maze-like scenario with automatic AI-augmentation and three levels of fire simulations, where participants need to make the final decision. We have collected performance data and revised the metrics of KUSIV3 for participants to report their trust of AI in the post-questionnaires. Our results show that the levels of trust towards AI are generally positive, and can be affected by dangerous conditions. The participant-reported and measured trust levels also demonstrate a loose but consistent correlation.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1184" style="margin-bottom: 0.3em;">
                <strong>Testing Virtualized Future Technologies Using Stressful Simulations (ID:&nbsp;P1184)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Nicole Kosoris,</span>
                        
                    
                        
                            <i>Georgia Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1184" class="wrap-collabsible"> <input id="collapsibleabstractP1184" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1184" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This work used Virtual Reality (VR) to prototype and test novel devices for First Responders, investigating using intentionally stressful scenarios to better differentiate between designs. Using a mixed methods approach, researchers designed and built calm and stressful scenarios for a traffic stop. Researchers began with qualitative methods, confirmed via survey, to determine the factors First Responders identified as most stressful in a traffic stop. Visibility was identified as the most critical design consideration. Comparisons were then done between low and high-visibility displays; participants responded significantly more negatively to the low-visibility display when in a simulated high-stress scenario.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1185" style="margin-bottom: 0.3em;">
                <strong>VR Reconstruction of Amazonian Geoglyphs Using LiDAR Data (ID:&nbsp;P1185)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Fang Wang,</span>
                        
                    
                        
                            <i>University of Missouri;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Albert Zhou,</span>
                        
                    
                        
                            <i>University of Missouri;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Robert S. Walker,</span>
                        
                    
                        
                            <i>University of Missouri;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mike Sturm,</span>
                        
                    
                        
                            <i>University of Missouri;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Amith Nalmas,</span>
                        
                    
                        
                            <i>University of Missouri;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Scottie D Murrell Mr,</span>
                        
                    
                        
                            <i>University of Missouri</i>
                        
                     
                
            </p>
            
                <div id="P1185" class="wrap-collabsible"> <input id="collapsibleabstractP1185" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1185" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This work presents an interactive Virtual Reality (VR) application to enhance education on Amazonian geoglyphs. Many Amazonian archaeological sites are difficult to reach and visualize as they are hidden by dense rainforest foliage. Using a combination of VR technology and LiDAR (Light Detection and Ranging) data, we generate scale models of geoglyphs without the current foliage. We use this model to create a reconstruction of the historical usage of these earthworks, based on current archaeological and anthropological interpretations. The experience is enhanced by using reconstructions of artifacts commonly found in the vicinity of these geoglyphs.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1186" style="margin-bottom: 0.3em;">
                <strong>Mind-Body TaoRelax: Relieving Stress through Immersive Virtual Reality Relaxation Training in a Taoist Atmosphere (ID:&nbsp;P1186)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Hao Yan,</span>
                        
                    
                        
                            <i>Southeast University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ding Ding,</span>
                        
                    
                        
                            <i>Southeast University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhuying Li,</span>
                        
                    
                        
                            <i>Southeast University</i>
                        
                     
                
            </p>
            
                <div id="P1186" class="wrap-collabsible"> <input id="collapsibleabstractP1186" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1186" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality technology is providing a new method for psychological relaxation training, but most systems focus primarily on meditation as a singular relaxation method. We propose a novel virtual reality relaxation system called TaoRelax. Building upon traditional Taoist culture, the system integrates both meditation and progressive muscle relaxation training. It combines electromyography (EMG) as a means of feedback and assessment during training, providing users with a dual experience of relaxation for both body and mind. Our preliminary results indicate that after training in our system, participants experienced a significant reduction in psychological stress and a notable improvement in meditation abilities.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1187" style="margin-bottom: 0.3em;">
                <strong>Light Field Transmission Fusing Image Super-Resolution and Selective Quality Patterns (ID:&nbsp;P1187)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Wei Zheng,</span>
                        
                    
                        
                            <i>Beijing Technology and Business University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xiaoming Chen,</span>
                        
                    
                        
                            <i>Beijing Technology and Business University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zongyou Yu,</span>
                        
                    
                        
                            <i>Beijing Technology and Business University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zeke Zexi Hu,</span>
                        
                    
                        
                            <i>The University of Sydney;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuk Ying Chung,</span>
                        
                    
                        
                            <i>The University of Sydney</i>
                        
                     
                
            </p>
            
                <div id="P1187" class="wrap-collabsible"> <input id="collapsibleabstractP1187" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1187" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Light field imaging has revolutionized immersive experiences and virtual reality applications. However, the vast amount of data generated by light field imaging presents significant challenges in terms of storage and transmission. In this study, we propose a novel approach for light field storage and transmission. Our approach leverages image super-resolution, which can be utilized to significantly reduce the light field data to be transmitted while maintaining reasonable light field viewing quality. Moreover, we have devised selective transmission patterns that align with human viewing patterns, enhancing the overall efficiency of light field transmission.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1188" style="margin-bottom: 0.3em;">
                <strong>A Taxonomy for Guiding XR Prototyping Decisions by the Non-Tech-Savvy (ID:&nbsp;P1188)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Assem Kroma,</span>
                        
                    
                        
                            <i>Carleton University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Robert J Teather,</span>
                        
                    
                        
                            <i>Carleton University</i>
                        
                     
                
            </p>
            
                <div id="P1188" class="wrap-collabsible"> <input id="collapsibleabstractP1188" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1188" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Within XR design, prototypes play an essential role in materializing, communicating and evaluating concepts. Yet, selecting the appropriate prototype proves intuitive for seasoned designers but daunting for beginners and the non tech-savvy. Addressing the right challenges when making decisions on the prototyping method is vital for a constructive outcome. Taxonomies emerge as powerful tools in this context. We forge a holistic taxonomy based on various prototyping taxonomies in the contexts of XR and other disciplines. We further validate it by surveying existing XR prototyping work. Next, we will further validate it through a series of steps common in such research.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1191" style="margin-bottom: 0.3em;">
                <strong>Emot Act AR: Tailoring Content through User Emotion and Activity Analysis (ID:&nbsp;P1191)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Somaiieh Rokhsaritalemi,</span>
                        
                    
                        
                            <i>Sejong University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Abolghasem Sadeghi-Niaraki,</span>
                        
                    
                        
                            <i>Sejong University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Soo-Mi Choi,</span>
                        
                    
                        
                            <i>Sejong University</i>
                        
                     
                
            </p>
            
                <div id="P1191" class="wrap-collabsible"> <input id="collapsibleabstractP1191" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1191" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In an era characterized by immersive content delivery experiences, the convergence of augmented reality (AR) and user behavior data stands out as a transformative synergy. This paper introduces "Emot Act AR," an innovative system intricately designed to dynamically tailor virtual content by user data. This framework incorporates user emotions and activities as intrinsic components. Through the utilization of sensing devices like camera and advanced AI models, the system dynamically customizes virtual elements, such as virtual flowers, in both static and dynamic settings. Additionally, the system utilizes user activity data to activate messaging avatars that encourage exercise.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1192" style="margin-bottom: 0.3em;">
                <strong>Enhanced Techniques to Implement Jumping-Over-Down and Jumping-At-Air using Pressure-sensing Shoes in Virtual Reality (ID:&nbsp;P1192)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Liuyang Chen,</span>
                        
                    
                        
                            <i>East China Normal University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Liuyang Chen,</span>
                        
                    
                        
                            <i>NetEase, Inc.;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gaoqi He,</span>
                        
                    
                        
                            <i>East China Normal University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Changbo Wang,</span>
                        
                    
                        
                            <i>Depart of Software Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1192" class="wrap-collabsible"> <input id="collapsibleabstractP1192" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1192" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Enhance the experience of jumping in the virtual reality environment is interesting and useful for various applications. This work includes three parts which are Jumping Detection, Jumping-Over-Down and Jumping-at-Air. One particular designed pressure-sensing shoes are used for jumping detection. This design could substantially reduce the cost of detecting jumping behaviors while ensuring high precision. Jumping-Over-Down motion allows users to control a virtual avatar to perform realistic downward jump in VR environment. While Jumping-at-Air motion represents a type of super-real jumping whicht resembles the double-jump feature found in many video games. From the hovering position, user can even execute another jumping.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1193" style="margin-bottom: 0.3em;">
                <strong>Robotic VR Massage System: Physical Care Robot with Out-of-body Experience in Virtual Karesansui (ID:&nbsp;P1193)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Naoya Harada,</span>
                        
                    
                        
                            <i>Aoyama Gakuin University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michiteru Kitazaki,</span>
                        
                    
                        
                            <i>Toyohashi University of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ryosuke Tasaki,</span>
                        
                    
                        
                            <i>Aoyama Gakuin Univercity</i>
                        
                     
                
            </p>
            
                <div id="P1193" class="wrap-collabsible"> <input id="collapsibleabstractP1193" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1193" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We have developed a robotic VR system that combines a physical care robot with a visual massage therapy in virtual environments. The robotic finger generates a periodic pressing pattern designed using learning data from skilled therapists. The virtual massage movement can be seamlessly synchronized with the movements of the physical care robot controlled in the real world using a high-sensitive force sensor on the robot end-effector, This creates an out-of-body experience in the third-person perspective. Furthermore, the robotic VR massage system provide a healing impression through the design of Karesansui, a Japanese dry garden related to Zen.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1199" style="margin-bottom: 0.3em;">
                <strong>Analysis of the association between metaverse-based engineering education and environment, social and governance platforms (ID:&nbsp;P1199)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jihyung Kim,</span>
                        
                    
                        
                            <i>Pohang University of Science and Technology(POSTECH;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sungeun Park,</span>
                        
                    
                        
                            <i>POSTECH;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ju Hong Park,</span>
                        
                    
                        
                            <i>Pohang University of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1199" class="wrap-collabsible"> <input id="collapsibleabstractP1199" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1199" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper proposed a metaverse environment-based engineering education method to utilize it as a platform that meets sustainable environmental, social, and governance according to social movements for reducing energy consumption and carbon footprint can be reduced by replacing them. Verifying the proposed method, we measured potential carbon footprint reduction as environmental factors and immersion as social and governance factors. Result of the experiment, the average value of all items was about 4.51, and the five items with the highest average value were separated from the real world due to the high sense of immersion and presence from the environment.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1200" style="margin-bottom: 0.3em;">
                <strong>Influence of Personality and Communication Behavior of a Conversational Agent on User Experience and Social Presence in Augmented Reality (ID:&nbsp;P1200)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Katerina Georgieva Koleva,</span>
                        
                    
                        
                            <i>Technische Universität Berlin;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maurizio Vergari,</span>
                        
                    
                        
                            <i>Technische Universität Berlin;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tanja Kojic,</span>
                        
                    
                        
                            <i>Technische Universität Berlin;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sebastian Möller,</span>
                        
                    
                        
                            <i>Technische Universität Berlin;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jan-Niklas Voigt-Antons,</span>
                        
                    
                        
                            <i>University of Applied Sciences Hamm-Lippstadt</i>
                        
                     
                
            </p>
            
                <div id="P1200" class="wrap-collabsible"> <input id="collapsibleabstractP1200" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1200" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>A virtual embodiment can benefit conversational agents, but it is unclear how their personalities and non-verbal behavior influence the User Experience and Social Presence in Augmented Reality (AR). We asked 30 users to converse with a virtual assistant who gives recommendations about city activities. The participants interacted with two different personalities: Sammy, a cheerful blue mouse, and Olive, a serious green human-like agent. Each was presented with two body languages - happy/friendly and annoyed/unfriendly. We conclude how agent representation and humor affect User Experience aspects, and that body language is significant in the evaluation and perception of the AR agent.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1201" style="margin-bottom: 0.3em;">
                <strong>Experimental Immersive 3D Camera Setup for Mobile Phones (ID:&nbsp;P1201)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Christian Aulbach,</span>
                        
                    
                        
                            <i>Julius-Maximilians-Universität Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Felix Scheuerpflug,</span>
                        
                    
                        
                            <i>Julius-Maximilians-Universität Würzburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel Pohl,</span>
                        
                    
                        
                            <i>immerVR GmbH;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sebastian von Mammen,</span>
                        
                    
                        
                            <i>University of Würzburg</i>
                        
                     
                
            </p>
            
                <div id="P1201" class="wrap-collabsible"> <input id="collapsibleabstractP1201" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1201" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Immersive media is becoming popular due to increased consumer access to virtual reality (VR) headsets. Mass adoption of 3D and immersive media may require devices similar to 2D phone cameras. Smartphones now feature wide-angle cameras with up to 150° field of view, approaching the 180° field of view in VR180 stereo photos. In this work, we use two smartphones with 123° wide-angle cameras and create a new app to capture immersive media in an equirectangular format for VR consumption. Our work is a proof of concept deploying current mobile devices to approach VR180 stereo photos with the available smartphone today.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1202" style="margin-bottom: 0.3em;">
                <strong>AR Simulations in VR: The Case for Environmental Awareness (ID:&nbsp;P1202)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ryleigh Byrne,</span>
                        
                    
                        
                            <i>Duke University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhehan Qu,</span>
                        
                    
                        
                            <i>Duke University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Christian Fronk,</span>
                        
                    
                        
                            <i>Duke University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sangjun Eom,</span>
                        
                    
                        
                            <i>Duke University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tim Scargill,</span>
                        
                    
                        
                            <i>Duke University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maria Gorlatova,</span>
                        
                    
                        
                            <i>Duke University</i>
                        
                     
                
            </p>
            
                <div id="P1202" class="wrap-collabsible"> <input id="collapsibleabstractP1202" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1202" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Augmented reality (AR) simulations in virtual reality (VR) offer controlled conditions, fewer hardware limitations, and access to diverse settings. However, simulations in VR must replicate the effects of environmental context in AR. Here, we examine perceived virtual content transparency under varying environment illuminance, and conduct a user study identifying discrepancies between AR and a standard simulation in VR. Results show perceived transparency remains high across all illuminance levels tested in VR, but is reduced at low illuminance in AR. This illustrates the impact of environment properties on the efficacy of AR simulations in VR, and motivates development of context-aware simulations.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1203" style="margin-bottom: 0.3em;">
                <strong>VR-Engine: Training Post-Earthquake Damages in Building Structure (ID:&nbsp;P1203)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Shah Rukh Humayoun,</span>
                        
                    
                        
                            <i>San Francisco State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jenna Wong,</span>
                        
                    
                        
                            <i>San Francisco State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Khanh Nguyen,</span>
                        
                    
                        
                            <i>San Francisco State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Purva Zinjarde,</span>
                        
                    
                        
                            <i>San Francisco State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Prathiba Ramesh,</span>
                        
                    
                        
                            <i>San Francisco State University</i>
                        
                     
                
            </p>
            
                <div id="P1203" class="wrap-collabsible"> <input id="collapsibleabstractP1203" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1203" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The use of latest advancements in technology, such as virtual reality (VR), can increase student engagement in classrooms and improve their learning skills in Science, Technology, Engineering, and Mathematics (STEM) fields. Therefore, it is now a time to explore deeply how we can use VR more effectively targeting different real-life applications in STEM fields. Targeting this, we developed a VR environment, called VR-Engine, to train civil engineering students about post-earthquake damages to the building structure. VR-Engine enables students to go around a 3D building structure to learn and diagnoses different kinds of damages in a post-earthquake scenario.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1204" style="margin-bottom: 0.3em;">
                <strong>Exploring User Preferences of VR Relaxation Experiences: A Comparative Mixed-Methods Study (ID:&nbsp;P1204)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Magdalena Igras-Cybulska,</span>
                        
                    
                        
                            <i>AGH UST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Radoslaw Sterna,</span>
                        
                    
                        
                            <i>Jagiellonian University in Kraków;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Grzegorz Łukawski,</span>
                        
                    
                        
                            <i>Kielce University of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gabriela Wrońska,</span>
                        
                    
                        
                            <i>AGH University of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1204" class="wrap-collabsible"> <input id="collapsibleabstractP1204" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1204" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This comparative study presents the results of a user experience investigation conducted during the use of four relaxing VR applications, distinguished by style and interactivity (Fujii, Real VR Fishing, TRIPP, Nature Treks VR), tested by 12 participants in a balanced order, within a within-subjects scenario. Both immersion and anxiety were measured. Semi-structured individual interviews were conducted to better understand user preferences. Among the most frequently suggested features, personalization and voluntary microinteractions appeared to be the most preferred.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1206" style="margin-bottom: 0.3em;">
                <strong>EndovasculAR: Utility of Mixed Reality to Segment Large Displays in Surgical Settings (ID:&nbsp;P1206)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Griffin J Hurt,</span>
                        
                    
                        
                            <i>University of Pittsburgh;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Talha Khan,</span>
                        
                    
                        
                            <i>University of Pittsburgh;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michael Ryan Kann,</span>
                        
                    
                        
                            <i>University of Pittsburgh School of Medicine;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Edward Andrews,</span>
                        
                    
                        
                            <i>University of Pittsburgh Medical Center;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jacob Biehl,</span>
                        
                    
                        
                            <i>University of Pittsburgh</i>
                        
                     
                
            </p>
            
                <div id="P1206" class="wrap-collabsible"> <input id="collapsibleabstractP1206" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1206" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Mixed reality (MR) holds potential for transforming endovascular surgery by enhancing information delivery. This advancement could significantly alter surgical interfaces, leading to improved patient outcomes. Our research utilizes MR technology to transform physical monitor displays inside the operating room (OR) into holographic windows. We aim to reduce cognitive load on surgeons by counteracting the split attention effect and enabling ergonomic display layouts. Our research is tackling key design challenges, including hands-free interaction, and occlusion management in densely crowded ORs. We are conducting studies to understand user behavior changes when people consult information on holographic windows compared to conventional displays.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1208" style="margin-bottom: 0.3em;">
                <strong>Exploring the Chameleon Effect in Metaverse Environments for Enabling Creative Communication (ID:&nbsp;P1208)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ai Nakada,</span>
                        
                    
                        
                            <i>Toyota Central R&amp;D Labs., Inc.;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Takayoshi Yoshimura,</span>
                        
                    
                        
                            <i>Toyota Central R&amp;D Labs., Inc.;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hiroyuki Sakai,</span>
                        
                    
                        
                            <i>Toyota Central R&amp;D Labs., Inc.;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Satoshi Nakagawa,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tomohiro Tanikawa,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yasuo Kuniyoshi,</span>
                        
                    
                        
                            <i>The University of Tokyo</i>
                        
                     
                
            </p>
            
                <div id="P1208" class="wrap-collabsible"> <input id="collapsibleabstractP1208" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1208" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this study, we explore the `Chameleon Effect' within metaverse environments and its impact on creative communication. This experiment investigates how avatar mimicry in online interactions influences creativity. Participants were divided into Mimic and Original conditions, participating in brainstorming tasks, notably the creative tasks such as the Alternative Uses Task. The study assesses the quality of ideas generated—focusing on Fluency, Flexibility, Elaboration, and Originality—and participant impressions of their avatars. Our findings suggest significant implications for enhancing creativity in virtual settings, providing insights into the future of collaborative innovation in the metaverse.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1209" style="margin-bottom: 0.3em;">
                <strong>Visual Attention and Virtual Human Facial Animations in Virtual Reality (VR): An Eye-Tracking Study (ID:&nbsp;P1209)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Shu Wei,</span>
                        
                    
                        
                            <i>University of Oxford;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel Freeman,</span>
                        
                    
                        
                            <i>University of Oxford;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Aitor Rovira,</span>
                        
                    
                        
                            <i>University of Oxford</i>
                        
                     
                
            </p>
            
                <div id="P1209" class="wrap-collabsible"> <input id="collapsibleabstractP1209" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1209" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We set out to understand the visual attention and perceptions of virtual humans in individuals vulnerable to paranoia in a VR eye-tracking study. In a factorial between-groups design, 122 individuals with elevated paranoia experienced a virtual lift ride with virtual humans that varied in facial animation (static or animated) and expression (neutral or positive). Facial animation (p=0.053) led to a significant reduction in co-presence. Positive expressions (p-adj=0.046) significantly decreased the visual attention to virtual humans when their faces were static. Our results indicate that virtual human programming could influence user perception and visual behaviours for people with mistrust.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1210" style="margin-bottom: 0.3em;">
                <strong>Predictive Task Guidance with Artificial Intelligence in Augmented Reality (ID:&nbsp;P1210)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Benjamin Rheault,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Shivvrat Arya,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Akshay Vyas,</span>
                        
                    
                        
                            <i>University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rohith Peddi,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Brett Benda,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Vibhav Gogate,</span>
                        
                    
                        
                            <i>University of Texas in Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Nicholas Ruozzi,</span>
                        
                    
                        
                            <i>University of Texas in Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yu Xiang,</span>
                        
                    
                        
                            <i>University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Eric Ragan,</span>
                        
                    
                        
                            <i>University of Florida</i>
                        
                     
                
            </p>
            
                <div id="P1210" class="wrap-collabsible"> <input id="collapsibleabstractP1210" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1210" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We put forward an augmented reality (AR) system using artificial intelligence (AI) to guide users through real-world procedures. The system uses the cameras of the headset to recognize objects and their positions in real time. It constructs a model to predict: (i) the task being completed, (ii) the step of the task they are on, and (iii) whether they have made any errors. By updating this model in real-time as the user completes the task, the system automatically updates the instructions and cues provided to the user. This system represents a step towards ubiquitous task guidance via AR</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1213" style="margin-bottom: 0.3em;">
                <strong>Design of experiment for evaluation of anomalous environments for product visualization in virtual reality (ID:&nbsp;P1213)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Francesco Musolino,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dario Gentile,</span>
                        
                    
                        
                            <i>Politecnico di Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">fabio vangi,</span>
                        
                    
                        
                            <i>Polytechnic University of Bari;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michele Fiorentino,</span>
                        
                    
                        
                            <i>Polythecnic Institute of Bari</i>
                        
                     
                
            </p>
            
                <div id="P1213" class="wrap-collabsible"> <input id="collapsibleabstractP1213" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1213" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality metaverse is candidate to be the future platform for retail. Nevertheless, the knowledge in designing virtual environments and their influence on products is limited in literature. Common “brick and mortar” virtualization of shops as realistic digital twin is a reductive approach compared to more creative experiences. This work explores novel paradigms by evaluating “anomalous environments” deviated from the ordinary conception. Different anomalies are evaluated within 8 scenes promoting an existing line of futuristiclooking tracksuits. The experimentation on 42 subjects, shows that anomalous environments, especially when aligned with the product's storytelling, can increase products likeability, interest and perceived valorization.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1215" style="margin-bottom: 0.3em;">
                <strong>Repeat Body-Ownership Illusions in Commodity Virtual Reality (ID:&nbsp;P1215)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Pauline W Cha,</span>
                        
                    
                        
                            <i>Davidson College;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tabitha C. Peck,</span>
                        
                    
                        
                            <i>Davidson College</i>
                        
                     
                
            </p>
            
                <div id="P1215" class="wrap-collabsible"> <input id="collapsibleabstractP1215" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1215" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual self-avatars have been shown to produce the Proteus effect, however limited work investigates the subjective sense of embodiment using commodity virtual reality systems. In this work, we present results from a pilot experiment where participants are given a self-avatar in a simple virtual experience while wearing a cardboard head-mounted display. Participants then repeat the experience five days later. Overall, subjective embodiment scores are similar to those reported in experience using higher-fidelity systems. However, the subjective sense of embodiment significantly lowered from trial one to trial two.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1217" style="margin-bottom: 0.3em;">
                <strong>Optimization of a Standalone VR Experience through User Perception: A Journey to the Hidden Spaces of the Burgos Cathedral (ID:&nbsp;P1217)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">David Checa,</span>
                        
                    
                        
                            <i>Universidad de Burgos;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mario Alaguero Alaguero,</span>
                        
                    
                        
                            <i>Universidad de Burgos</i>
                        
                     
                
            </p>
            
                <div id="P1217" class="wrap-collabsible"> <input id="collapsibleabstractP1217" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1217" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This project offers a groundbreaking virtual reality (VR) experience that allows visitors to explore inaccessible areas of the Burgos Cathedral. Employing photogrammetry techniques, these spaces have been digitally recreated in high detail. The application, optimized for standalone VR devices, has undergone meticulous optimization. This was achieved by analyzing visitor attention patterns, enhancing resolution in frequently observed areas to ensure a rich and seamless experience. This initiative not only significantly improves the visitor experience but also serves as a model for the dissemination of cultural heritage through VR.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1218" style="margin-bottom: 0.3em;">
                <strong>A Calibration Interface for 3D Gaze Depth Disambiguation in Virtual Environments (ID:&nbsp;P1218)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Cameron Boyd,</span>
                        
                    
                        
                            <i>Augusta University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mia Thompson,</span>
                        
                    
                        
                            <i>Augusta University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maddie Smith,</span>
                        
                    
                        
                            <i>Augusta University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jason Orlosky,</span>
                        
                    
                        
                            <i>Augusta University</i>
                        
                     
                
            </p>
            
                <div id="P1218" class="wrap-collabsible"> <input id="collapsibleabstractP1218" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1218" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In Augmented and Virtual Reality, accurate eye tracking is a requirement for many applications. Though state-of-the art algorithms have enabled sub-degree accuracy for line-of-sight tracking, one remaining problem is that depth tracking, i.e. calculation of the gaze intersection at various depths, is still inaccurate. In this paper, we propose a 3D calibration method that accounts for gaze depth in addition to line-of-sight. By taking advantage of 3D calibration points and modeling the relationship between gaze inaccuracy and depth, we show that we can improve depth calculations and better determine the 3D position of gaze intersections in virtual environments.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1219" style="margin-bottom: 0.3em;">
                <strong>Insights from the Participatory Design of a Virtual Reality Spatial Navigation Application for Veterans with a History of TBI (ID:&nbsp;P1219)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Lal 'Lila' Bozgeyikli,</span>
                        
                    
                        
                            <i>University of Arizona;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Evren Bozgeyikli,</span>
                        
                    
                        
                            <i>University of Arizona;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ryu Kevin Funakoshiya,</span>
                        
                    
                        
                            <i>University of Arizona;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Quan Le,</span>
                        
                    
                        
                            <i>University of Arizona;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kyle Walker,</span>
                        
                    
                        
                            <i>University of Arizona;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">L. Matthew Law,</span>
                        
                    
                        
                            <i>Phoenix VA Health Care System;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel R. Griffiths,</span>
                        
                    
                        
                            <i>Phoenix VA Health Care System;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Arne Ekstrom,</span>
                        
                    
                        
                            <i>University of Arizona;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jonathan Lifshitz,</span>
                        
                    
                        
                            <i>Phoenix VA Health Care System</i>
                        
                     
                
            </p>
            
                <div id="P1219" class="wrap-collabsible"> <input id="collapsibleabstractP1219" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1219" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present a virtual reality (VR) application for potential application to cognitive rehabilitation of Veterans with a history of traumatic brain injury (TBI). Our application offers exploration of various virtual environments. We discuss the notable aspects of our design and implementation and insights from the participatory design process with lived-experience Veteran consultants.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1220" style="margin-bottom: 0.3em;">
                <strong>HMD VR Experience Motivates Exercise Intention in Patients with Type 2 Diabetes (ID:&nbsp;P1220)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Sun-Jeong Kim,</span>
                        
                    
                        
                            <i>Hallym University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dong-Soo Shin,</span>
                        
                    
                        
                            <i>Hallym University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ohk-Hyun Ryu,</span>
                        
                    
                        
                            <i>Chuncheon Sacred Heart Hospital;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Shin-Jeong Kim,</span>
                        
                    
                        
                            <i>Hallym University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yongsoon Park,</span>
                        
                    
                        
                            <i>Chuncheon Sacred Heart Hospital;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yong-Jun Choi,</span>
                        
                    
                        
                            <i>Hallym University</i>
                        
                     
                
            </p>
            
                <div id="P1220" class="wrap-collabsible"> <input id="collapsibleabstractP1220" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1220" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Patients are expected to manage their chronic conditions, but very few exercise regularly in addition to taking their medications. Therefore, we developed a 3-minute HMD VR program to educate patients about the importance of exercise. The control group (n=30) received usual care, while the experimental group (n=30) experienced virtual reality and assessed SSQ and exercise intention. No patients experienced VR sickness and the experimental group had a statistically significant higher intention to exercise for diabetes management. Older adults found the VR experience exciting and novel. For diabetic foot experience, color change is enough to motivate them.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1221" style="margin-bottom: 0.3em;">
                <strong>The Influence of Extended Reality and Virtual Characters' Embodiment Levels on User Experience in Well-Being Activities (ID:&nbsp;P1221)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Tanja Kojic,</span>
                        
                    
                        
                            <i>Technische Universität Berlin;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maurizio Vergari,</span>
                        
                    
                        
                            <i>Technische Universität Berlin;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marco Podratz,</span>
                        
                    
                        
                            <i>Technische University Berlin;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sebastian Möller,</span>
                        
                    
                        
                            <i>Technische Universität Berlin;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jan-Niklas Voigt-Antons,</span>
                        
                    
                        
                            <i>University of Applied Sciences Hamm-Lippstadt</i>
                        
                     
                
            </p>
            
                <div id="P1221" class="wrap-collabsible"> <input id="collapsibleabstractP1221" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1221" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Millions of people have seen their daily habits transform, reducing physical activity and leading to mental health issues. This study explores how virtual characters impact motivation for well-being. Three prototypes with cartoon, robotic, and human-like avatars were tested by 22 participants. Results show that animated virtual avatars, especially with extended reality, boost motivation, enhance comprehension of activities, and heighten presence. Multiple output modalities, like audio and text, with character animations, improve the user experience. Notably, the cartoon-like character evoked positive responses. This research highlights virtual characters' potential to engage individuals in daily well-being activities.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1223" style="margin-bottom: 0.3em;">
                <strong>Multimodal Exploration of Terrain with Immersive Interactions (ID:&nbsp;P1223)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Shamima Yasmin,</span>
                        
                    
                        
                            <i>Eastern Washington University</i>
                        
                     
                
            </p>
            
                <div id="P1223" class="wrap-collabsible"> <input id="collapsibleabstractP1223" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1223" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality (VR) lets users explore objects in an immersive environment. Users freely interact with objects in a virtual environment (VE), where they can assign physical properties to objects, make them animate or inanimate, and have them respond according to the underlying characteristics. This research explores geological topology using VR. In addition, users studied a terrain model in a multimodal audio-visual VE. Two types of audio-enhanced modes were used: “Continuous” and “Sporadic.” Initial findings showed that users preferred multimodal exploration of terrain models compared to the traditional unimodal vision-only version.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1224" style="margin-bottom: 0.3em;">
                <strong>Career XRcade Framework: Student-driven Collaboration Processes to Develop Learning Environments for Immersive Career Exploration (ID:&nbsp;P1224)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Zahra Khaleghian,</span>
                        
                    
                        
                            <i>Arizona State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Robert LiKamWa,</span>
                        
                    
                        
                            <i>Arizona State University</i>
                        
                     
                
            </p>
            
                <div id="P1224" class="wrap-collabsible"> <input id="collapsibleabstractP1224" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1224" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The Career XRcade (CXR) Framework is our platform to allow college students creating educational immersive learning experiences that prepare high school students to explore diverse career paths. These virtual journeys through various career paths offer students a medium to understand real-world job environments and demands. We have demonstrated the use of our framework through the creation of two virtual worlds each showcasing 5 career paths in Cybersecurity and Esports industries. Our proposed user studies on our case studies will gather data and insights from our stakeholders, aiming to measure the effectiveness of such a framework in academic settings.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1225" style="margin-bottom: 0.3em;">
                <strong>Primitive-based Model Reduction for Huge 3D Meshes of Industrial Plants (ID:&nbsp;P1225)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Haruki Hattori,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yutaka Ohtake,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tatsuya Yatagawa,</span>
                        
                    
                        
                            <i>Hitotsubashi University</i>
                        
                     
                
            </p>
            
                <div id="P1225" class="wrap-collabsible"> <input id="collapsibleabstractP1225" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1225" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study addresses the model reduction for huge 3D meshes of industrial plants, which typically consist of millions of triangles, where a plant mesh is converted into a set of primitive shapes, including spheres, cuboids, cylinders, cones, torus, and more complicated extruded and quadratic surfaces. To achieve this, we partition the input mesh using geometric features, and each part is converted to a primitive to minimize the surface deviation. Storing each primitive into a leaf node of the bounding volume hierarchy (BVH), our system achieves more than 90% of model reduction and interactive rendering performance for several plant models.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1227" style="margin-bottom: 0.3em;">
                <strong>PPVR: A Privacy-preserving Approach for User Behaviors in VR (ID:&nbsp;P1227)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ruoxi Sun,</span>
                        
                    
                        
                            <i>CSIRO's Data61;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hanwen Wang,</span>
                        
                    
                        
                            <i>The University of Adelaide;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Minhui Xue Dr,</span>
                        
                    
                        
                            <i>CSIRO's Data61;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hsiang-Ting Chen,</span>
                        
                    
                        
                            <i>University of Adelaide</i>
                        
                     
                
            </p>
            
                <div id="P1227" class="wrap-collabsible"> <input id="collapsibleabstractP1227" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1227" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The increasing popularity of immersive environments creates a pressing challenge of reconciling service providers' desire to collect user behavior data with users' privacy concerns. In this study, we investigate the use of differential privacy (DP) algorithms in VR applications to enable statistical analysis of 3D spatial motion data while protecting against re-identification attacks. We assessed the efficacy of DP on both the original 3D spatial data and its cumulative heat map representation. This study underscores the utility of the DP algorithm in the context of 3D body motion data and highlights its potential for widespread adoption in diverse VR applications.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1228" style="margin-bottom: 0.3em;">
                <strong>Developing a VR Meditation Program for College Students to Use on Campus with a Message Chair (ID:&nbsp;P1228)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Fang Wang,</span>
                        
                    
                        
                            <i>University of Missouri;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Scottie D Murrell Mr,</span>
                        
                    
                        
                            <i>University of Missouri;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Albert Zhou,</span>
                        
                    
                        
                            <i>University of Missouri;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kanishka Peryala,</span>
                        
                    
                        
                            <i>University of Missouri, Columbia</i>
                        
                     
                
            </p>
            
                <div id="P1228" class="wrap-collabsible"> <input id="collapsibleabstractP1228" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1228" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>College students often experience high level of stress due to academic commitment and associated factors. To explore the potential benefits of VR in stress management for students while they are on campus, we developed a VR meditation application to use with a message chair. The application utilizes hand tracking technology and is designed to be used comfortably in conjunction with a massage chair to provide a convenient and accessible way to help manage stress for students while on campus. We plan to further develop the application and conduct further studies on the effects of this approach on students' mental health.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1230" style="margin-bottom: 0.3em;">
                <strong>TeleAutoHINTS: A Virtual or Augmented Reality (VR/AR) System for Automated Tele-Neurologic Evaluation of Acute Vertigo (ID:&nbsp;P1230)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Haochen Wei,</span>
                        
                    
                        
                            <i>Johns Hopkins University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Justin Bolsey,</span>
                        
                    
                        
                            <i>Johns Hopkins Hospital;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Edward Kuwera,</span>
                        
                    
                        
                            <i>Johns Hopkins Hospital;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Peter Kazanzides,</span>
                        
                    
                        
                            <i>Johns Hopkins University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kemar E Green,</span>
                        
                    
                        
                            <i>Johns Hopkins University School of Medicine</i>
                        
                     
                
            </p>
            
                <div id="P1230" class="wrap-collabsible"> <input id="collapsibleabstractP1230" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1230" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Annually, millions of patients in the United States visit the emergency room (ER) due to symptoms of vertigo or dizziness. Rapidly distinguishing between benign causes and more severe conditions requires performing and interpreting a three-step bedside head and eye movement assessment known as HINTS (head impulse, nystagmus, and test of skew). A shortage of experts trained in conducting and interpreting this test exists. Consequently, we developed TeleAutoHINTS, comprising (1) a Hololens2-based head and eye tracking platform for automated tele-sensing and (2) an interconnected interface for visualization and analysis. We tested and evaluated TeleAutoHINTS on three subjects.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1286" style="margin-bottom: 0.3em;">
                <strong>Hitting a Brick Wall: Passive Haptic Feedback for Control Elements in Virtual Reality  (ID:&nbsp;P1286)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Markus Tatzgern,</span>
                        
                    
                        
                            <i>Salzburg University of Applied Sciences;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">David Fasching,</span>
                        
                    
                        
                            <i>Salzburg University of Applied Sciences;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Vivian Gómez,</span>
                        
                    
                        
                            <i>Universidad de los Andes;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sebastian Calvache,</span>
                        
                    
                        
                            <i>Universidad de los Andes;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Pablo Figueroa,</span>
                        
                    
                        
                            <i>Universidad de los Andes</i>
                        
                     
                
            </p>
            
                <div id="P1286" class="wrap-collabsible"> <input id="collapsibleabstractP1286" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1286" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The experience and performance of Virtual Reality (VR) simulations and training applications can be improved by utilizing haptic feedback when interacting with typical control elements of machines such as buttons and slider. To experience haptic feedback with a consumer-grade head-mounted display (HMD), in-built hand tracking solutions can be used. However, these solutions may suffer from tracking issues leading to imprecise interactions. Hence, we designed a controller-based hand input method that allows users to perceive haptic feedback with typical handheld controllers when interacting with virtual control elements while at the same time benefitting from precise controller-based tracking.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1356" style="margin-bottom: 0.3em;">
                <strong>Precueing Compound Tasks in Virtual and Augmented Reality (ID:&nbsp;P1356)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ahmed Rageeb Ahsan,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Andrew W. Tompkins,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Eric Ragan,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jaime Ruiz,</span>
                        
                    
                        
                            <i>University of Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ryan P. McMahan,</span>
                        
                    
                        
                            <i>University of Central Florida</i>
                        
                     
                
            </p>
            
                <div id="P1356" class="wrap-collabsible"> <input id="collapsibleabstractP1356" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1356" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper addresses the challenge of determining the quantity and composition of visual task cues for effective precueing.AR and VR can seamlessly overlay additional visual information to help people improve work efficiently and reduce errors, but the effectiveness of such task assistance depends on the amount of information given. Prior research has found benefits of supplemental visual cues for giving future hints for upcoming steps, though many experiments have predominantly focused on simplified tasks. We present an experiment assessing different visual cues in VR to test a user's ability to harness distinct precueing information.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1330" style="margin-bottom: 0.3em;">
                <strong>A Scoping Review on Immersive Technologies in the Oil and Gas Industry (ID:&nbsp;P1330)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Muskan Sarvesh,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mehdi Marzban,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">minseok ryan kang,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Simon S. Park,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ron Hugo,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kangsoo Kim,</span>
                        
                    
                        
                            <i>University of Calgary</i>
                        
                     
                
            </p>
            
                <div id="P1330" class="wrap-collabsible"> <input id="collapsibleabstractP1330" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1330" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper explores the role of immersive technologies, such as Augmented Reality, Virtual Reality, Mixed Reality, Digital Twins, and Building Information Modeling in the oil and gas industry. Through a comprehensive scoping review, we address key research questions related to the types of immersive technologies used in the context of the oil and gas pipeline industry, and the application scenarios like training, testing, maintenance, and monitoring. It highlights the potential of immersive technologies, identifies research gaps, and suggests future directions.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1259" style="margin-bottom: 0.3em;">
                <strong>SIGMA: An Open-Source Interactive System for Mixed-Reality Task Assistance Research - Extended Abstract (ID:&nbsp;P1259)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Dan Bohus,</span>
                        
                    
                        
                            <i>Microsoft Research;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sean Andrist,</span>
                        
                    
                        
                            <i>Microsoft Research;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Nick C. W. Saw,</span>
                        
                    
                        
                            <i>Microsoft Research;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ann Paradiso,</span>
                        
                    
                        
                            <i>Microsoft Research;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ishani Chakraborty,</span>
                        
                    
                        
                            <i>Microsoft;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mahdi Rad,</span>
                        
                    
                        
                            <i>Microsoft</i>
                        
                     
                
            </p>
            
                <div id="P1259" class="wrap-collabsible"> <input id="collapsibleabstractP1259" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1259" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We introduce an open-source system called SIGMA (short for "Situated Interactive Guidance, Monitoring, and Assistance") as a platform for conducting research on task-assistive agents in mixed-reality scenarios. The system leverages the sensing and rendering affordances of a head-mounted mixed reality device in conjunction with large language and vision models to guide users step by step through procedural tasks. By open-sourcing the system, we aim to lower the barrier to entry, accelerate research in this space, and chart a path towards community-driven end-to-end evaluation of large language, vision, and multimodal models in the context of real-world interactive applications.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1324" style="margin-bottom: 0.3em;">
                <strong>[EXTENDED] Interactive Data Fusion of Neural Radiance Fields for Facility Inspection in Virtual Reality (ID:&nbsp;P1324)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ke Li,</span>
                        
                    
                        
                            <i>Deutsches Elektronen-Synchrotron (DESY);</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Susanne Schmidt,</span>
                        
                    
                        
                            <i>Universität Hamburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tim Rolff,</span>
                        
                    
                        
                            <i>Universität Hamburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Reinhard Bacher,</span>
                        
                    
                        
                            <i>Deutsches Elektronen Synchrotron DESY;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wim Leemans,</span>
                        
                    
                        
                            <i>Deutsches Elektronen Synchrotron;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Frank Steinicke,</span>
                        
                    
                        
                            <i>Universität Hamburg</i>
                        
                     
                
            </p>
            
                <div id="P1324" class="wrap-collabsible"> <input id="collapsibleabstractP1324" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1324" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Real-world industrial facilities often present complex equipment that requires extensive inspection and maintenance for their operations. We present a virtual reality framework that supports virtual facility inspection and maintenance tasks by using neural radiance field models to replicate, store, and visualize the appearance of complex industrial facilities. To overcome the performance bottleneck of VR NeRF rendering, we present two interactive data fusion techniques that can merge a NeRF model with its' corresponding CAD model through contextualized visualizations. Technical benchmarking results and preliminary expert feedback are presented for the initial evaluation of our framework.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1300" style="margin-bottom: 0.3em;">
                <strong>Virtual Day: a VR Game for the Evaluation of Prospective Memory in Older Adults (ID:&nbsp;P1300)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Maryam Alimardani,</span>
                        
                    
                        
                            <i>Tilburg University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Guido Morera,</span>
                        
                    
                        
                            <i>Tilburg University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alexandra Hering,</span>
                        
                    
                        
                            <i>Tilburg University</i>
                        
                     
                
            </p>
            
                <div id="P1300" class="wrap-collabsible"> <input id="collapsibleabstractP1300" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1300" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Prospective memory (PM), the ability to remember to perform tasks in the future, is vital for maintaining functional independence in older adults. This paper introduces “Virtual Day”, a novel Virtual Reality (VR) game designed to assess PM in an immersive and realistic environment. We report a feasibility study where Virtual Day was compared to its clinical counterpart, a digital board game. Results indicate a preference for Virtual Day, noting it as more engaging and enjoyable, despite being more challenging. Follow-up development will build on these findings to use VR as a potential tool for cognitive interventions to promote healthy aging.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1281" style="margin-bottom: 0.3em;">
                <strong>Framework for Social XR: Navigating Challenges, Ethics, and Opportunities for Authentic Community Engagement (ID:&nbsp;P1281)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Stella Doukianou,</span>
                        
                    
                        
                            <i>University of the Arts;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Vali Lalioti,</span>
                        
                    
                        
                            <i>University of the Arts London</i>
                        
                     
                
            </p>
            
                <div id="P1281" class="wrap-collabsible"> <input id="collapsibleabstractP1281" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1281" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR) redefine community engagement, posing challenges in user experience, privacy, security, inclusivity, and authentic interactions. This work proposes strategies for authentic community interactions in virtual realms, presenting a framework for ethical engagement through AR/VR/MR. Assessing industry trends, we categorize XR potential and challenges into Users, Technology, and XR experiences, emphasizing research gaps and opportunities for addressing social goals. We review and propose strategies to navigate the ethical considerations, inclusive design practices, and user satisfaction metrics to foster genuine connections across boundaries.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1093" style="margin-bottom: 0.3em;">
                <strong>Development of a Touchable VR Planetarium for Both Sighted and Visually Impaired People (ID:&nbsp;P1093)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Kota Suzuki,</span>
                        
                    
                        
                            <i>Kogakuin University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Keita Ushida,</span>
                        
                    
                        
                            <i>Kogakuin University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Qiu Chen,</span>
                        
                    
                        
                            <i>Kogakuin University</i>
                        
                     
                
            </p>
            
                <div id="P1093" class="wrap-collabsible"> <input id="collapsibleabstractP1093" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1093" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>The authors proposed and developed a “touchable” VR planetarium. The user wears a VR headset and “touches” the stars with the controllers. Because we can't touch the stars in reality, this application provides the users with additional value and experience of the planetarium. As this feature is valuable for visually impaired people to experience the starry sky, the authors also implemented the functions that help it. In the trial use by visually impaired people, they experienced the starry sky with the support functions and evaluated the VR planetarium as a valuable application.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1107" style="margin-bottom: 0.3em;">
                <strong>Aging Naturally: Virtual reality nature vs real-world nature's effects on executive functioning and stress recovery in older adults (ID:&nbsp;P1107)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Sara LoTemplio,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sharde Johnson,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michaela Rice,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rachel Masters,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sara-Ashley Collins,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Joshua Hofecker,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jordan Rivera,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dylan Schreiber,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Victoria Interrante,</span>
                        
                    
                        
                            <i>University of Minnesota;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Francisco Raul Ortega,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Deana Davalos,</span>
                        
                    
                        
                            <i>Colorado State University</i>
                        
                     
                
            </p>
            
                <div id="P1107" class="wrap-collabsible"> <input id="collapsibleabstractP1107" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1107" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Development of Alzheimer's Dementia &amp; other related dementias (ADRD) is characterized by decline in executive functioning (EF), and onset risk of ADRD is increased by stress. Previous work has shown that spending time in nature or virtual reality nature can improve EF and improve stress recovery in younger adults. Yet, little work has examined whether these benefits can extend to older adults. We examine how spending time in either nature or an equivalently designed virtual reality natural environment can affect EF and stress in older adults compared to a lab control condition.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1340" style="margin-bottom: 0.3em;">
                <strong>Multiple Level of Details Neural Implicit Surface Representation for Unconstrained Viewpoint Rendering (ID:&nbsp;P1340)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Zhihua Chen,</span>
                        
                    
                        
                            <i>East China University of Science &amp; Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuhang Li,</span>
                        
                    
                        
                            <i>East China University of Science &amp; Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Lei Dai,</span>
                        
                    
                        
                            <i>East China University of Science &amp; Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ping Li,</span>
                        
                    
                        
                            <i>The Hong Kong Polytechnic University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Lei Zhu,</span>
                        
                    
                        
                            <i>The Hong Kong University of Science and Technology (Guangzhou);</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Bin Sheng,</span>
                        
                    
                        
                            <i>Shanghai Jiao Tong University</i>
                        
                     
                
            </p>
            
                <div id="P1340" class="wrap-collabsible"> <input id="collapsibleabstractP1340" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1340" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>To mitigate artifacts and performance degradation in neural implicit representation visualization for unconstrained viewpoint rendering, we propose a feature voxel grid-based neural representation architecture. This approach flexibly encodes the implicit surface with multiple levels of detail, facilitating high-quality rendering with dynamic switching between detail levels. Additionally, we implement a range-limited strategy to concentrate on sampling valid areas while excluding undefined areas. Our results in unconstrained viewpoint scenarios demonstrate the effectiveness of our method. This work extends the capabilities of neural implicit representations, broadening their potential applications beyond previously defined limitations.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1357" style="margin-bottom: 0.3em;">
                <strong>Study of Cross-Reality Interfaces with Avatar Redirection to Improve Desktop Presentations To Headset-Immersed Audiences (ID:&nbsp;P1357)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jason Wolfgang Woodworth,</span>
                        
                    
                        
                            <i>University of Louisiana at Lafayette;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Christoph W Borst,</span>
                        
                    
                        
                            <i>University of Louisiana at Lafayette</i>
                        
                     
                
            </p>
            
                <div id="P1357" class="wrap-collabsible"> <input id="collapsibleabstractP1357" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1357" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We study cross-reality desktop interfaces that allow a desktop user, with a 3D avatar, to present lecture-style content to a headset-immersed audience. Recent work on VR meeting spaces for presentations suggests that some presenters prefer to use desktop interfaces for comfort or other factors. We designed and compared interfaces to understand design tradeoffs. A key result is that the SDesk interface, offering an audience perspective and eye-tracked gaze gestures, is preferred over a standard VR-simulating desktop interface.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
</div>
<div>
    <h2 id="P3" class="pink" style="padding-top:25px;">Wednesday Posters</h2>
    <p class="small">Talk with the authors: 9:45&#8209;10:15, 13:00&#8209;13:30, 15:00&#8209;15:30, 17:00&#8209;17:30, Room: Sorcerer's Apprentice Ballroom</p>  
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1231" style="margin-bottom: 0.3em;">
                <strong>Virtual Streamer with Conversational and Tactile Interaction (ID:&nbsp;P1231)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Vaishnavi Josyula,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sowresh MecheriSenthil,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Abbas Khawaja,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jose M Garcia,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ayush Bhardwaj,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ashish Pratap,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jin Ryong Kim,</span>
                        
                    
                        
                            <i>The University of Texas at Dallas</i>
                        
                     
                
            </p>
            
                <div id="P1231" class="wrap-collabsible"> <input id="collapsibleabstractP1231" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1231" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper introduces an approach to an interactive virtual streamer that provides conversational and tactile interactions, offering insights into levels of immersion and personalization for the user. User interaction is categorized into various types, such as the spatial nature of the virtual streamer's stream, along with conversational and tactile interactions with the streamer. We deploy the virtual streamer in a VR environment and conclude the efficiency of user interactions with the virtual streamer via a scenario-based assessment.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1233" style="margin-bottom: 0.3em;">
                <strong>Affordance Maintenance-based 3D Scene Synthesis for Immersive Mixed Reality (ID:&nbsp;P1233)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Haiyan Jiang,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dongdong Weng,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xiaonuo Dongye,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1233" class="wrap-collabsible"> <input id="collapsibleabstractP1233" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1233" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>With the explosion of head-mounted displays into the commercial market, immersive scenes are being used in many applications. As these applications are usually used indoors, the user would suffer from safety and immersion issues when immersed in a virtual scene. Therefore, we propose a 3D synthesis method that synthesizes the 3D scene according to the affordance of physical objects and the intention of the user. The results show that virtual objects in synthetic scenes have the same affordances as physical objects in the real world and can provide interaction medium and feedback to the situated user and avoid collisions.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1236" style="margin-bottom: 0.3em;">
                <strong>An AR-Based Multi-User Learning Environment for Anatomy Seminars (ID:&nbsp;P1236)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Danny Schott,</span>
                        
                    
                        
                            <i>Otto-von-Guericke University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jonas Mandel,</span>
                        
                    
                        
                            <i>Otto-von-Guericke-Universität Magdeburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Florian Heinrich,</span>
                        
                    
                        
                            <i>Otto-von-Guericke-Universität Magdeburg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Lovis Schwenderling,</span>
                        
                    
                        
                            <i>Otto-von-Guericke-University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Matthias Kunz,</span>
                        
                    
                        
                            <i>Clinic for Cardiology and Angiology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rüdiger Braun-Dullaeus,</span>
                        
                    
                        
                            <i>Clinic for Cardiology and Angiology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Christian Hansen,</span>
                        
                    
                        
                            <i>Faculty of Computer Science</i>
                        
                     
                
            </p>
            
                <div id="P1236" class="wrap-collabsible"> <input id="collapsibleabstractP1236" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1236" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Understanding the intricate and rapid changes in shape during embryonic formation is vital for medical students. Using the example of embryonic human heart development, we introduce an AR-based multi-user approach to enhance understanding and foster a participatory learning environment. Through a user-centered approach, we created a prototype accommodating two player roles and enabling multi-modal inputs to encourage dynamic group discussions. We invited four anatomy experts to evaluate three system configurations in an interdisciplinary workshop to assess integration feasibility into anatomy seminars. The gathered data and qualitative feedback indicate the potential of our collaborative concept for integration into the medical curriculum.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1237" style="margin-bottom: 0.3em;">
                <strong>Evaluating User Perception Toward Pose-Changed Avatar in Remote Dissimilar Spaces (ID:&nbsp;P1237)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Taehei Kim,</span>
                        
                    
                        
                            <i>KAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hyeshim Kim,</span>
                        
                    
                        
                            <i>KAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jeongmi Lee,</span>
                        
                    
                        
                            <i>KAIST;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sung-Hee Lee,</span>
                        
                    
                        
                            <i>Korea Advanced Institute of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1237" class="wrap-collabsible"> <input id="collapsibleabstractP1237" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1237" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In dissimilar telepresence environments, where spaces differ in size and furniture layouts, researchers have proposed methods to adjust avatar movement to deviate from the user's while still adapting to the physical environment. From this perspective, this poster aims to investigate users' perceptions and preferences toward their adjusted avatars. Especially, we propose an experiment focusing on pose standing and sitting which is the most common pose when talking to others. We describe the system setup and initial results that show how users perceive their avatars when their pose category is changed to avoid physics conflict.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1241" style="margin-bottom: 0.3em;">
                <strong>Working with XR in Public: Effects on Users and Bystanders (ID:&nbsp;P1241)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Verena Biener,</span>
                        
                    
                        
                            <i>Coburg University of applied sciences and arts;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Snehanjali Kalamkar,</span>
                        
                    
                        
                            <i>Coburg University of Applied Sciences and Arts;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">John J Dudley,</span>
                        
                    
                        
                            <i>University of Cambridge;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jinghui Hu,</span>
                        
                    
                        
                            <i>University of Cambridge;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Per Ola Kristensson,</span>
                        
                    
                        
                            <i>University of Cambridge;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jörg Müller,</span>
                        
                    
                        
                            <i>University of Bayreuth;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jens Grubert,</span>
                        
                    
                        
                            <i>Coburg University</i>
                        
                     
                
            </p>
            
                <div id="P1241" class="wrap-collabsible"> <input id="collapsibleabstractP1241" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1241" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Recent commercial virtual and augmented reality (VR/AR) devices have been promoted as tools for knowledge work. Their possibility to display virtual screens can be especially helpful in mobile contexts, in which users might not have access to multiple screens, for example, when working on a train. As using such devices in public is still uncommon, this motivates our study to better understand the implications of using AR and VR for work in public on users and bystanders.Therefore, we present initial results of a study in a university cafeteria comparing three different systems: a laptop with a single screen; a laptop combined with an optical see-through AR headset; a laptop combined with an immersive VR headset.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1242" style="margin-bottom: 0.3em;">
                <strong>360° Storytelling for Immersive Teaching Online and in the Classroom for Secondary and Tertiary Education (ID:&nbsp;P1242)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Lloyd Spencer Davis,</span>
                        
                    
                        
                            <i>University of Otago;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wiebke Finkler,</span>
                        
                    
                        
                            <i>University of Otago;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wei Hong Lo,</span>
                        
                    
                        
                            <i>University of Otago;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mary Rabbidge,</span>
                        
                    
                        
                            <i>Otago Boys Highschool;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Lei Zhu,</span>
                        
                    
                        
                            <i>University of Otago;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Stefanie Zollmann,</span>
                        
                    
                        
                            <i>University of Otago</i>
                        
                     
                
            </p>
            
                <div id="P1242" class="wrap-collabsible"> <input id="collapsibleabstractP1242" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1242" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this work, we present the findings of a study designed to investigate the impact of 360° videos on student engagement and learning outcomes in both secondary and tertiary educational contexts. The research focused on two distinct scenarios: Teaching Science to Secondary School Students and Hybrid University Courses integrating On-campus and Distant Students. The study employed a multifaceted approach, combining video production, test protocols, and evaluations to assess the efficacy of 360° videos.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1243" style="margin-bottom: 0.3em;">
                <strong>Side-By-Side Might Win: Occlusion Negatively Affects The Performance Of Augmented Reality Task Instructions (ID:&nbsp;P1243)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Robin Wiethüchter,</span>
                        
                    
                        
                            <i>ETH Zürich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Saikishore Kalloori,</span>
                        
                    
                        
                            <i>ETH Zürich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">David Lindlbauer,</span>
                        
                    
                        
                            <i>Carnegie Mellon University</i>
                        
                     
                
            </p>
            
                <div id="P1243" class="wrap-collabsible"> <input id="collapsibleabstractP1243" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1243" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We compare different representations for Augmented Reality instructions for a combined pick &amp; place and drawing task. Participants completed the task with instructions presented as 3D AR overlays, as 3D AR offset (side-by-side), or as 2D panels. Results indicate that participants preferred the 3D offset instructions, especially compared to 3D overlays. Our results stand in contrast to prior work where 3D overlays were preferred. Our work points at the need for the community to better define benchmarks and standardized tests to create guidelines for when to use what type of AR representation.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1244" style="margin-bottom: 0.3em;">
                <strong>Gaze Pattern Genius: Gaze-Driven VR Interaction using Unsupervised Domain Adaption (ID:&nbsp;P1244)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Kexin Wang,</span>
                        
                    
                        
                            <i>Beihang university;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yang Gao,</span>
                        
                    
                        
                            <i>Beihang university;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">wenfeng song,</span>
                        
                    
                        
                            <i>Beijing information science and technology university;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuecheng Li,</span>
                        
                    
                        
                            <i>Beihang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Aimin Hao,</span>
                        
                    
                        
                            <i>Beihang University</i>
                        
                     
                
            </p>
            
                <div id="P1244" class="wrap-collabsible"> <input id="collapsibleabstractP1244" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1244" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This poster advocates shifting VR interaction to gaze-driven interaction, a more intuitive alternative to traditional controls like VR controllers or gestures. Our focus is on enhancing neural network recognition accuracy, especially with limited user-specific gaze data. We introduce a novel framework for capturing gaze gesture patterns and propose a template dataset concept to boost neural training. Our domain adaptation model, blending template depth and sparse user data authenticity, consistently excels in recognizing gaze patterns across diverse users. Empirical user studies confirm: gaze-driven interactions not only elevate VR experiences but also redefine immersive VR control dynamics.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1246" style="margin-bottom: 0.3em;">
                <strong>AR-based Dynamic Sandbox for Hydraulic Earth-rock Dam Break Simulation (ID:&nbsp;P1246)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Huiwen Liu,</span>
                        
                    
                        
                            <i>China Institute of Water Resources and Hydropower Research;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuan Wang,</span>
                        
                    
                        
                            <i>Beihang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dawei Zhang,</span>
                        
                    
                        
                            <i>China Institute of Water Resources and Hydropower Research</i>
                        
                     
                
            </p>
            
                <div id="P1246" class="wrap-collabsible"> <input id="collapsibleabstractP1246" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1246" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>VR/AR-based hydraulic scenes holds significant value for geological disaster display. However, the river dynamics models prevalent in computational fluid dynamics (CFD) and hydraulics rely largely on pure numerical simulations, lacking intuitive visual representation. In contrast, current AR-based physical simulation methods often fail to incorporate hydraulic parameters adequately, falling short of the accuracy needed for real hydraulic models. This paper proposes integrating a precise numerical river dynamics model into a visual fluid-solid interaction simulation, which achieves a balance between a realistic visual simulation of dam breaks and the numerical accuracy aligned with river dynamics models, while maintaining high computational efficiency.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1249" style="margin-bottom: 0.3em;">
                <strong>Asymmetric VR Chores: A Social Presence Preliminary Study (ID:&nbsp;P1249)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Stephen Saunders,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alvaro Quevedo,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Winnie Sun,</span>
                        
                    
                        
                            <i>Ontario Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sheri Anne Horsburgh,</span>
                        
                    
                        
                            <i>Ontario Tech University</i>
                        
                     
                
            </p>
            
                <div id="P1249" class="wrap-collabsible"> <input id="collapsibleabstractP1249" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1249" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>VR applications focused on daily activities for health care focusing either physical or cognitive interventions continue gaining momentum as a result of consumer-level devices. However, interactions mainly focused on single user experiences with little research conducted on VR and asymmetric gameplay, which refers to the fact that players play the game using different media. This article presents a preliminary study focused on how the addition of asymmetry to a house chores VR game positively influences social presence when playing the game. Our motivation is driven by research that indicates that asymmetry in games increases communication and social connection.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1250" style="margin-bottom: 0.3em;">
                <strong>Enhancing the Immersive Experience of the Yijing in Claborate-Style Painting through Virtual Reality (ID:&nbsp;P1250)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yuting Cheng,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mengjie Huang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jiashu Yang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wenxin Sun,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University</i>
                        
                     
                
            </p>
            
                <div id="P1250" class="wrap-collabsible"> <input id="collapsibleabstractP1250" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1250" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Chinese claborate-style painting is a traditional painting that depicts objects with delicate brushstrokes of color. Virtual reality (VR) can create an immersive virtual environment, a representation that brings a new direction to the exhibition of paintings. In order to convey the yijing (artistic conception) of the claborate-style painting in VR, this paper investigates how to enhance the user's feeling of yijing in VR and set up two paths in VR to provide the user with a more immersive experience and provide a new reference for future VR design practices of Chinese paintings.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1251" style="margin-bottom: 0.3em;">
                <strong>A  Bio-Inspired Computational Model for the   3D Motion Estimation  (ID:&nbsp;P1251)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Di Zhang,</span>
                        
                    
                        
                            <i>Communication University of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ping Lu,</span>
                        
                    
                        
                            <i>Beijing University of Posts and Telecommunications;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Qi Wu,</span>
                        
                    
                        
                            <i>Communication University of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Long Ye,</span>
                        
                    
                        
                            <i>Communication University of China</i>
                        
                     
                
            </p>
            
                <div id="P1251" class="wrap-collabsible"> <input id="collapsibleabstractP1251" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1251" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Stereoscopic motion perception is pivotal for human-world interaction and has sparked advancements in artificial vision technologies. We introduce the Stereo Motion Perception Model (SMPM), a bio-inspired computational model designed to emulate the process of stereoscopic motion perception. The SMPM extracts temporal, spatial, and shape features from stereoscopic video stimuli, enabling accurate motion perception. The performance of SMPM is tested and compared with human perception. The results indicates that the SMPM effectively estimates target motion on both simplistic and complex scene, high consistency is shown with human stereo motion perception. Future implications of the bio-inspired model is discussed.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1252" style="margin-bottom: 0.3em;">
                <strong>Controlling Experience for Interaction Techniques in Virtual Reality Exergames (ID:&nbsp;P1252)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Wenxin Sun,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mengjie Huang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Chenxin Wu,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wendi Wang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rui Yang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jiajia Shi,</span>
                        
                    
                        
                            <i>Kunshan Rehabilitation Hospital</i>
                        
                     
                
            </p>
            
                <div id="P1252" class="wrap-collabsible"> <input id="collapsibleabstractP1252" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1252" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality (VR) has become an essential platform for upper limb rehabilitation by introducing VR interaction techniques (e.g., hand gestures, controllers, and tangible tools) into traditional rehabilitation apparatus. Controlling experience plays a significant role in upper limb exergames, highly relevant to rehabilitation outcomes. This study focused on users' controlling experience when integrating VR interaction techniques and traditional rehabilitation tables (commonly employed in clinics and homes). This study revealed that tangible tools or controllers contributed to the higher controlling experience, and they were recommended as the suitable options for upper limb exergames when building VR rehabilitations.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1253" style="margin-bottom: 0.3em;">
                <strong>Visual Complexity in VR: Implications for Cognitive Load (ID:&nbsp;P1253)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Maximilian Rettinger,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xuefei Jiang,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jiaxin Yang,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gerhard Rigoll,</span>
                        
                    
                        
                            <i>Technical University of Munich</i>
                        
                     
                
            </p>
            
                <div id="P1253" class="wrap-collabsible"> <input id="collapsibleabstractP1253" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1253" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Given the potential of Virtual Reality for education and training, it is essential to implement it as effectively as possible. For this reason, we investigate the impact of virtual environments with different complexity levels on the cognitive load. We compare the methods 1) No-Room, 2) Empty-Room, 3) Filled-Room, and 4) Animated-Room. In a within-subjects study (n=40), participants completed a letter recall and a mental rotation task to assess the cognitive load. The results show significant differences in the letter recall task, indicating that the cognitive load is also lower in the rooms with less complexity.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1256" style="margin-bottom: 0.3em;">
                <strong>Cholesteric Liquid Crystal Based Reconfigurable Optical Combiner for Head-Mounted Display Application (ID:&nbsp;P1256)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yuanjie Xia,</span>
                        
                    
                        
                            <i>University of Glasgow;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dr Haobo Li,</span>
                        
                    
                        
                            <i>University of Glasgow;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Marija Vaskeviciute,</span>
                        
                    
                        
                            <i>University of Glasgow;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniele Faccio,</span>
                        
                    
                        
                            <i>University of Glasgow;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Affar Karimullah,</span>
                        
                    
                        
                            <i>University of Glasgow;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hadi Heidari,</span>
                        
                    
                        
                            <i>University of Glasgow;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rami Ghannam,</span>
                        
                    
                        
                            <i>University of Glasgow</i>
                        
                     
                
            </p>
            
                <div id="P1256" class="wrap-collabsible"> <input id="collapsibleabstractP1256" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1256" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Recent advancements in Head-Mounted Displays (HMDs) showcase their potential to replace traditional displays. The upcoming generation of HMDs necessitates a seamless transition between Augmented Reality (AR) and Virtual Reality (VR) modes. We introduce an innovative Cholesteric Liquid Crystal (CLC) based optical combiner for HMDs. This approach enables the display device to switch between AR, VR and transparent modes via temperature modulation. We demonstrated the reconfigurability of the tunable optical combiners using 532 nm laser sources. Our findings demonstrate that CLC-based optical combiners can switch between the three modes at corresponding temperatures, paving the way for versatile applications in future HMDs.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1257" style="margin-bottom: 0.3em;">
                <strong>Multi-3D-Models Registration-Based Augmented Reality Instructions for Assembly (ID:&nbsp;P1257)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Seda Tuzun Canadinc,</span>
                        
                    
                        
                            <i>Webster University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wei Yan Ph.D.,</span>
                        
                    
                        
                            <i>Texas A&amp;M University</i>
                        
                     
                
            </p>
            
                <div id="P1257" class="wrap-collabsible"> <input id="collapsibleabstractP1257" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1257" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>BRICKxAR (Multi 3D Models/M3D) prototype offers markerless, in-situ, and step-by-step, highly accurate Augmented Reality (AR) assembly instructions for large or small part assembly. The prototype employs multiple assembly phases of deep learning-trained 3D model-based AR registration coupled with a step count. This ensures object recognition and tracking persist while the model updates at each step, even if a part's location is not visible to the AR camera. The use of phases simplifies the complex assembly instructions. The testing and heuristic evaluation findings indicate that BRICKxAR (M3D) provides robust instructions for assembly, promising potential applicability at different scales and scenarios.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1258" style="margin-bottom: 0.3em;">
                <strong>Semantic Web-Enabled Intelligent VR Tutoring System for Engineering Education: A Theoretical Framework (ID:&nbsp;P1258)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Victor Häfner,</span>
                        
                    
                        
                            <i>Karlsruhe Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tengyu Li,</span>
                        
                    
                        
                            <i>Karlsruhe Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Felix Longge Michels,</span>
                        
                    
                        
                            <i>Karlsruhe Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Polina Häfner,</span>
                        
                    
                        
                            <i>Karlsruhe Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Haoran Yu,</span>
                        
                    
                        
                            <i>Karlsruhe Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jivka Ovtcharova,</span>
                        
                    
                        
                            <i>Karlsruhe Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1258" class="wrap-collabsible"> <input id="collapsibleabstractP1258" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1258" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>An unmet demand exists for personalized virtual reality (VR) training that adapts to users' abilities and performance. The primary issue lies in the high cost of setting up and maintaining virtual environments, resulting in limited flexibility for training content updates. To address this, we incorporate Semantic Web technology into VR training and explore the integration of the intelligent tutoring system. Finally, we propose a framework for a Semantic Web-Enabled Intelligent VR Tutoring System, especially for engineering education.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1260" style="margin-bottom: 0.3em;">
                <strong>Virtual Reality (VR)-Based Training Tool for Positive Reinforcement and Communication in Autistic Children (ID:&nbsp;P1260)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ashayla Williams,</span>
                        
                    
                        
                            <i>Purdue Northwest University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Magesh Chandramouli,</span>
                        
                    
                        
                            <i>Purdue Northwest University</i>
                        
                     
                
            </p>
            
                <div id="P1260" class="wrap-collabsible"> <input id="collapsibleabstractP1260" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1260" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper integrates a novel virtual reality (VR) framework with eye-tracking technology to enhance social communication skills in children diagnosed with Autism Spectrum Disorder (ASD). ASD often leads to a 'nonsocial bias,' hindering the perception of social cues. Leveraging social motivation theory, the framework employs positive reinforcement, integrating restricted interest objects (RIs) to enhance social attention and interaction. This approach signifies a significant advancement in using VR to address ASD-related social communication challenges. We believe the results from this research have the potential to enhance the overall quality of life of autistic children by helping them acquire essential social skills.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1261" style="margin-bottom: 0.3em;">
                <strong>Therapet: Designing an Interactive XR Therapeutic Pet (ID:&nbsp;P1261)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jordan Glendinning,</span>
                        
                    
                        
                            <i>Abertay University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Haocheng Yang,</span>
                        
                    
                        
                            <i>Abertay University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Andrew Hogue,</span>
                        
                    
                        
                            <i>OntarioTech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Patrick C. K. Hung,</span>
                        
                    
                        
                            <i>Ontaruo Tech University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ruth Falconer,</span>
                        
                    
                        
                            <i>Abertay University</i>
                        
                     
                
            </p>
            
                <div id="P1261" class="wrap-collabsible"> <input id="collapsibleabstractP1261" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1261" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>As Extended Reality (XR) technologies continue to mature, they offer ever more diverse, complex, and impactful experiences. We present ``Therapet'', an augmented reality (AR) application designed for the Hololens 2. Therapet presents an interactive therapeutic pet avatar---using Volumetric Video recordings and traditionally animated models---with which users can interact with by feeding, playing fetch, and petting. We conducted a pilot study to investigate usability and user preferences, revealing positive user experiences and insights into the use of Volumetric Video avatars. Furthermore, we discuss lessons learned to create a successful AR therapeutic pet application.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1262" style="margin-bottom: 0.3em;">
                <strong> A Methodology for Predicting User Curvature Perception of 3D Objects (ID:&nbsp;P1262)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Nayan N Chawla,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Joseph LaViola,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ryan P. McMahan,</span>
                        
                    
                        
                            <i>University of Central Florida</i>
                        
                     
                
            </p>
            
                <div id="P1262" class="wrap-collabsible"> <input id="collapsibleabstractP1262" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1262" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>One's perception of an object's curvature affects its perceived appeal, realism, and even distance. However, studies indicate curvature perception often differs from the object's mathematically defined curvature, and no alternative for predicting curvature perception exists. We present two pairwise-comparison studies where participants selected objects perceived to have more curvature. The results indicate some objects are perceived to have significantly more curvature than others, yielding distinct perceptual clusters. We then demonstrate that traditional curvature measures poorly predict curvature perception, and present a novel methodology with results proving it a capable indicator of how a 3D object's curvature will be perceived.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1263" style="margin-bottom: 0.3em;">
                <strong>The Impact of Virtual Agent Locomotion and Emotional Postures on Social Perception and Brain Activity in Mixed Reality (ID:&nbsp;P1263)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Zhuang Chang,</span>
                        
                    
                        
                            <i>The University of Auckland;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jiashuo Cao,</span>
                        
                    
                        
                            <i>The University of Auckland;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kunal Gupta,</span>
                        
                    
                        
                            <i>The University of Auckland;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Huidong Bai,</span>
                        
                    
                        
                            <i>The University of Auckland;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mark Billinghurst,</span>
                        
                    
                        
                            <i>Auckland Bioengineering Institute</i>
                        
                     
                
            </p>
            
                <div id="P1263" class="wrap-collabsible"> <input id="collapsibleabstractP1263" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1263" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>When people interact with virtual characters, the character's non-verbal cues can change the user's perception of and engagement with it. In this work, we investigate the impact of virtual agent locomotion and emotional posture on user engagement, as measured by subjective questionnaires and EEG signals. We found the EEG-based engagement index was significantly lower in the postural agent condition, which was opposite to the questionnaire-based engagement result. From these results, we differentiate social engagement from task engagement and discuss their influence on overall perceived engagement.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1264" style="margin-bottom: 0.3em;">
                <strong>[EXTENDED] Immersive Rover Control and Obstacle Detection based on Extended Reality and Artificial Intelligence (ID:&nbsp;P1264)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Sofia Coloma,</span>
                        
                    
                        
                            <i>University of Luxembourg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alexandre Frantz,</span>
                        
                    
                        
                            <i>University of Luxembourg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Dave van der Meer,</span>
                        
                    
                        
                            <i>University of Luxembourg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ernest Skrzypczyk,</span>
                        
                    
                        
                            <i>SnT;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Andrej Orsula,</span>
                        
                    
                        
                            <i>University of Luxembourg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Miguel Olivares-Mendez,</span>
                        
                    
                        
                            <i>SnT - University of Luxembourg</i>
                        
                     
                
            </p>
            
                <div id="P1264" class="wrap-collabsible"> <input id="collapsibleabstractP1264" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1264" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Lunar exploration has become a key focus, driving scientific and technological advances. Ongoing missions are deploying rovers to the Moon's surface, targeting the far side and south pole. However, these terrains pose challenges, emphasizing the need for precise obstacles and resource detection to avoid mission risks. This work proposes a novel system that integrates extended reality and artificial intelligence to teleoperate lunar rovers. It is capable of autonomously detecting rocks and recreating an immersive 3D virtual environment of the robot's location. This system has been validated in a lunar laboratory to observe its advantages over traditional 2D-based teleoperation approaches.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1265" style="margin-bottom: 0.3em;">
                <strong>Effects of Ankle Tendon Electrical Stimulation on Detection Threshold of Redirected Walking (ID:&nbsp;P1265)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Takashi Ota,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Keigo Matsumoto,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kazusa Aoyama,</span>
                        
                    
                        
                            <i>Gunma University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tomohiro Amemiya Ph.D.,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Takuji Narumi,</span>
                        
                    
                        
                            <i>the University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hideaki Kuzuoka,</span>
                        
                    
                        
                            <i>The University of Tokyo</i>
                        
                     
                
            </p>
            
                <div id="P1265" class="wrap-collabsible"> <input id="collapsibleabstractP1265" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1265" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Redirected walking (RDW) is a method for exploring virtual spaces larger than physical spaces while preserving a natural walking sensation. We proposed a locomotion method that applies ankle tendon electrical stimulation (TES), which is known to induce a body sway and tilt sensation, to RDW. We evaluated how TES affects the detection threshold (DT), which is the maximum gain at which visual manipulation is not noticed. The results indicated that the DT was expanded when TES was applied to induce the body tilt sensation in the same direction as the RDW's visual manipulation.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1266" style="margin-bottom: 0.3em;">
                <strong>Multimodal Interaction with Gaze and Pressure Ring in Mixed Reality (ID:&nbsp;P1266)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Zhimin Wang,</span>
                        
                    
                        
                            <i>Beihang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">JingYi Sun,</span>
                        
                    
                        
                            <i>Beihang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mingwei Hu,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maohang Rao,</span>
                        
                    
                        
                            <i>Beihang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yangshi Ge,</span>
                        
                    
                        
                            <i>School of Computer Science and Engineering;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Weitao Song,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Feng Lu,</span>
                        
                    
                        
                            <i>Beihang University</i>
                        
                     
                
            </p>
            
                <div id="P1266" class="wrap-collabsible"> <input id="collapsibleabstractP1266" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1266" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Controller-free augmented reality devices currently rely on gesture interaction, which can cause arm fatigue and limit hand mobility. This paper proposes a multimodal interaction technique that combines gaze and pressure ring worn on the index finger. The proposed technique eliminates the need for direct hand capturing using the camera, allowing for more flexible hand movements and reducing fatigue. The experiment conducted in this study demonstrates the effectiveness of the pressure ring in enhancing interaction efficiency.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1267" style="margin-bottom: 0.3em;">
                <strong>Audio-driven Talking Face Video Generation with Emotion (ID:&nbsp;P1267)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jiadong Liang,</span>
                        
                    
                        
                            <i>Beihang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Feng Lu,</span>
                        
                    
                        
                            <i>Beihang University</i>
                        
                     
                
            </p>
            
                <div id="P1267" class="wrap-collabsible"> <input id="collapsibleabstractP1267" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1267" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Vivid talking face generation has potential applications in virtual reality. Existing methods can generate talking faces that are synchronized with the audio, but typically ignore the accurate expression of emotions. In this paper, we propose an advanced two-step framework to synthesize talking face videos with vivid emotional appearances.Vivid talking face generation has potential applications in virtual reality. Existing methods can generate talking faces that are synchronized with the audio, but typically ignore the accurate expression of emotions. In this paper, we propose an advanced two-step framework to synthesize talking face videos with vivid emotional appearances. The first step is designed to generate emotional fine-grained landmarks, including the normalized landmarks, gaze, and head pose. In the second step, we map the facial landmarks to latent key points, which are then fed into the pre-trained model to generate high-quality face images. Extensive experiments demonstrate the effectiveness of our method.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1269" style="margin-bottom: 0.3em;">
                <strong>Neuromorphic-Enabled Implementation of Extremely Low-Power Gaze Estimation (ID:&nbsp;P1269)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Zhipeng Sui,</span>
                        
                    
                        
                            <i>Tsinghua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Weihua He,</span>
                        
                    
                        
                            <i>Tsinghua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Fei Liang,</span>
                        
                    
                        
                            <i>Huawei Technologies Co Ltd;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yongxiang Feng,</span>
                        
                    
                        
                            <i>Huawei Technologies Co Ltd;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xiaobao Wei,</span>
                        
                    
                        
                            <i>Chinese Academy of Sciences Institute of Automation;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Qiushuang Lian,</span>
                        
                    
                        
                            <i>Tsinghua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ziyang Zhang,</span>
                        
                    
                        
                            <i>Huawei Technologies Co Ltd.;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Guoqi Li,</span>
                        
                    
                        
                            <i>Chinese Academy of Sciences Institute of Automation;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wenhui Wang,</span>
                        
                    
                        
                            <i>Tsinghua University</i>
                        
                     
                
            </p>
            
                <div id="P1269" class="wrap-collabsible"> <input id="collapsibleabstractP1269" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1269" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Event camera has great potential in the field of eye tracking, while current event-based gaze estimation suffers from complex imaging settings and participation of RGB modality. We propose a novel architecture for completely event-based low-power spiking gaze estimation using only one eye signal. Our architecture employs a wake-up module to judge the state of inputs, and then enters one of the three modules including hibernation, lightweight SNN segmentation network, and image processing module to obtain the gaze estimation results. We prove that our method performs better in terms of accuracy and power consumption on Angelopoulos's dataset.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1270" style="margin-bottom: 0.3em;">
                <strong>Exploring Strategies for Crafting Emotional Facial Animations in Virtual Characters (ID:&nbsp;P1270)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Hyewon Song,</span>
                        
                    
                        
                            <i>Yonsei University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Seongmin Lee,</span>
                        
                    
                        
                            <i>Yonsei University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sanghoon Lee,</span>
                        
                    
                        
                            <i>Yonsei University</i>
                        
                     
                
            </p>
            
                <div id="P1270" class="wrap-collabsible"> <input id="collapsibleabstractP1270" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1270" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Emotional portrayal by virtual characters is a critical aspect of communication in virtual reality (VR). Our investigation focuses on understanding the factors that influence the depiction of emotions in these characters. Our findings suggest that emotional representation in virtual characters is highly influenced by factors such as facial expressions, head movements, and overall appearance. Surprisingly, despite being a central point in previous studies, lip-syncing seems to have less impact on conveying emotions. These insights from our study hold promising potential for the VR community, enabling the development of virtual characters capable of expressing a wide range of emotions.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1271" style="margin-bottom: 0.3em;">
                <strong>Perceived Realism in 360° Video-Based vs. 3D Modeled Virtual Reality Environment (ID:&nbsp;P1271)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Ibrahim Itani,</span>
                        
                    
                        
                            <i>Deakin University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kaja Antlej,</span>
                        
                    
                        
                            <i>Deakin University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michael Mortimer,</span>
                        
                    
                        
                            <i>Deakin University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ben Horan,</span>
                        
                    
                        
                            <i>Deakin University</i>
                        
                     
                
            </p>
            
                <div id="P1271" class="wrap-collabsible"> <input id="collapsibleabstractP1271" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1271" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality impacts users differently depending on the visualization environments. This study compares the impact of two visualization styles in virtual environments on perceived realism and perception. An experiment was conducted using a high-fidelity 360° video-based environment and a modeled three-dimensional (3D) environment. The results show the 360° video-based environment produced a higher perceived realism rating and a greater sense of reality. However, no impact was observed on users' perception of the depicted environments. Users' perceptions were positive, indicating an engaging experience with a great effect on opinions regarding the stigma of Geelong's post-industrial manufacturing status.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1272" style="margin-bottom: 0.3em;">
                <strong>Physical OOP: Spatial Program Physical Objects Interactions (ID:&nbsp;P1272)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Xiaoyan Wei,</span>
                        
                    
                        
                            <i>Th University of Adelaide;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zijian Yue,</span>
                        
                    
                        
                            <i>The University of Adelaide;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hsiang-Ting Chen,</span>
                        
                    
                        
                            <i>University of Adelaide</i>
                        
                     
                
            </p>
            
                <div id="P1272" class="wrap-collabsible"> <input id="collapsibleabstractP1272" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1272" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Spatial computing has the potential to revolutionise the way we interact with the world around us. However, the current generation of development tools has not yet fully adapted to this shift.  This creates a large perceptual distance between the abstract variable and the physical object.We introduce the Physical Object-Oriented Programming Environment (PhyOOP). PhyOOP empowers users to capture various states of physical objects through live video streams from cameras and insert these visual representations into their code. Users can augment physical objects by attaching executable code snippets onto objects, enabling opportunistic execution triggered by camera observations.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1273" style="margin-bottom: 0.3em;">
                <strong>Intrinsic Omnidirectional Video Decomposition (ID:&nbsp;P1273)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Rong-Kai Xu,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Fang-Lue Zhang,</span>
                        
                    
                        
                            <i>Victoria University of Wellingtong;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Lei Zhang,</span>
                        
                    
                        
                            <i>Beijing Institute of Technology</i>
                        
                     
                
            </p>
            
                <div id="P1273" class="wrap-collabsible"> <input id="collapsibleabstractP1273" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1273" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Intrinsic decomposition of omnidirectional video is a challenging task. We propose a method that can provide temporally consistent decomposition results. Leveraging the 360-degree scene representation, we maintain the global point cloud to propagate and reuse the similar inter-frame content and establish temporal constraints which elevate the quality of frame-wise decomposition while maintaining inter-frame coherence. By optimizing the proposed objective function, our method achieves a precise separation of reflectance and shading components. Comprehensive experiments demonstrate that our approach outperforms existing intrinsic decomposition methods. Our method also hold promise for various video manipulation applications.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1274" style="margin-bottom: 0.3em;">
                <strong>3D printing guide plate model construction method for Orthopedic surgery based on Virtual Reality (ID:&nbsp;P1274)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Wenjun Tan,</span>
                        
                    
                        
                            <i>Northeastern University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Pinan Lu,</span>
                        
                    
                        
                            <i>Northeastern University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Qingya Li,</span>
                        
                    
                        
                            <i>Northeastern University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jinzhu Yang,</span>
                        
                    
                        
                            <i>Northeastern University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">TianLong Ji,</span>
                        
                    
                        
                            <i>The First Hospital of China Medical University</i>
                        
                     
                
            </p>
            
                <div id="P1274" class="wrap-collabsible"> <input id="collapsibleabstractP1274" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1274" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This work studies the key technologies of surgical simulation and surgical guide plate design on the basis of VR and constructs a VR prototype system of surgical guide plate design. Firstly, this thesis delves into the study of mesh simplification algorithms. Secondly, a design methodology for surgical guide plate based on triangular mesh filling is proposed. Building upon this foundation, a closed surgical guide plate mesh model is constructed by adding triangular surfaces to the mesh. All key algorithms are integrated and optimized in this thesis to develop a prototype system for VR surgical guide plate design.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1275" style="margin-bottom: 0.3em;">
                <strong>Social Simon Effect between Two Adjacent Avatars in VR (ID:&nbsp;P1275)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Xiaotong Li,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuji Hatada,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Takuji Narumi,</span>
                        
                    
                        
                            <i>The University of Tokyo</i>
                        
                     
                
            </p>
            
                <div id="P1275" class="wrap-collabsible"> <input id="collapsibleabstractP1275" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1275" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study examined the Social Simon Effect (SSE), which indicates that each individual in joint actions automatically activating representations of the other's behavior in the motor system, between two adjacent avatars in virtual reality and investigated the impact of the visual representation of the avatar on SSE. The results showed that SSE was induced when the co-actor's avatar was displayed in full-body or was entirely transparent, but it was absent when only the two hands were visible.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1276" style="margin-bottom: 0.3em;">
                <strong>Throwing in Virtual Reality: Performance and Preferences Across Input Device Configurations (ID:&nbsp;P1276)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Amirpouya Ghasemaghaei,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yahya Hmaiti,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mykola Maslych,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Esteban Segarra Martinez,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Joseph LaViola,</span>
                        
                    
                        
                            <i>University of Central Florida</i>
                        
                     
                
            </p>
            
                <div id="P1276" class="wrap-collabsible"> <input id="collapsibleabstractP1276" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1276" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>An underexplored interaction metaphor in virtual reality (VR) is throwing, with a considerable challenge in achieving accurate and natural results. We conducted a preliminary investigation of participants' VR throwing performance, measuring their accuracy and preferences across various input device configurations and throwable object types. Participants were tasked with throwing different throwable objects toward random targets using one throwing input configuration out of the five we developed. Our work is relevant to researchers and developers aiming to improve throwing interactions in VR. We demonstrate that on-body tracking leads to the highest throwing accuracy, whereas most participants preferred a controller-derived configuration.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1277" style="margin-bottom: 0.3em;">
                <strong>I look, but I don't see it: Inattentional Blindness as an Evaluation Metric for Augmented-Reality  (ID:&nbsp;P1277)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Nayara Faria,</span>
                        
                    
                        
                            <i>Virginia Tech;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Brian Williams,</span>
                        
                    
                        
                            <i>Virginia Tech Transportation Institute;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Joseph L Gabbard,</span>
                        
                    
                        
                            <i>Virginia Tech</i>
                        
                     
                
            </p>
            
                <div id="P1277" class="wrap-collabsible"> <input id="collapsibleabstractP1277" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1277" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>As vehicles increasingly incorporate augmented reality (AR) into head-up displays, assessing their safety in driving becomes vital. AR, blending real and synthetic scenes, can cause inattentional blindness (IB), where crucial information is missed despite users directly looking at them. Traditional HCI methods, centered on task time and accuracy, fall short in evaluating AR's impact on human performance in safety-critical contexts. Our real-road user study with AR-enabled vehicles focuses on inattentional blindness as a key metric for assessment. The results underline the importance of including IB in AR evaluations, extending to other safety-critical sectors like manufacturing, and healthcare.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1278" style="margin-bottom: 0.3em;">
                <strong>Exploring the Impacts of Interactive Tasks on Redirected Walking in Virtual Reality (ID:&nbsp;P1278)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jieun Lee,</span>
                        
                    
                        
                            <i>Gwangju Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Aya Ataya,</span>
                        
                    
                        
                            <i>Gwangju Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">SeungJun Kim,</span>
                        
                    
                        
                            <i>Gwangju Institute of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1278" class="wrap-collabsible"> <input id="collapsibleabstractP1278" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1278" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Reorientation in virtual reality (VR) is manipulating user orientation within a small physical space to support travel in a large virtual space. Reorientation can be applied to alter the user's orientation in VR content as the user rotates to engage with the content. This study integrated different types of interactions with reorientation to reduce the discrepancy between virtual and real rotation without causing user discomfort. We exceeded the rotation gain threshold to examine the enhancement of reorientation performance in 48 participants. The results indicate that engagement in interactive tasks effectively prevents users from discerning reorientation manipulation without inducing VR sickness.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1280" style="margin-bottom: 0.3em;">
                <strong>A Comparative Analysis of Text Readability on Display Layouts in Virtual Reality (ID:&nbsp;P1280)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Samiha Sultana,</span>
                        
                    
                        
                            <i>BRAC University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Md. Ashraful Alam,</span>
                        
                    
                        
                            <i>BRAC University</i>
                        
                     
                
            </p>
            
                <div id="P1280" class="wrap-collabsible"> <input id="collapsibleabstractP1280" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1280" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Curved displays, featuring layouts such as semi-circular and fully circular designs, offer superior visual experiences compared to flat screens due to their alignment with human vision. This study assesses user experience in VR with different display layouts, comparing curved designs (quarter circle, semicircle, three-fourths circle, full circle) and flat screens. It focuses on reading comprehension across 8 display types. The study finds that shorter display layouts are preferred for their faster reading speeds compared to longer curved and flat displays. The results offer key insights for designing and optimizing curved displays in virtual reality applications.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1283" style="margin-bottom: 0.3em;">
                <strong>Integrating Spatial Design with Reality: An AR Game Scene Generation Approach (ID:&nbsp;P1283)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jia Liu,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Renjie Zhang,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Taishi Sawabe,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuichiro Fujimoto,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Masayuki Kanbara,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hirokazu Kato,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1283" class="wrap-collabsible"> <input id="collapsibleabstractP1283" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1283" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>AR games can offer a distinct and immersive experience set apart from traditional games. When creating AR games, one of the most formidable challenges faced by designers lies in the unpredictability of real-world environments, making it difficult to seamlessly integrate virtual world with the player's surroundings. Our approach addresses this challenge by converting the designer's design and the player's real environment into scene graphs, and then integrating the two graphs to determine the placement of virtual objects. In addition, we provide a user-friendly interface for designers to quickly visualize inspirations, and an integration module for naturally arranging virtual objects.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1284" style="margin-bottom: 0.3em;">
                <strong>OnStage: Design and Development of a Performance and Practice Simulation System (ID:&nbsp;P1284)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Chong Cao,</span>
                        
                    
                        
                            <i>Beihang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yulu Lu,</span>
                        
                    
                        
                            <i>School of New Media Art and Disign</i>
                        
                     
                
            </p>
            
                <div id="P1284" class="wrap-collabsible"> <input id="collapsibleabstractP1284" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1284" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Traditional dance practice processes use mirrors or videos to observe movements and facial expressions, have limitations due to their reliance on a single viewpoint. Meanwhile, the transition from dance training rooms to stage performances involves significant environmental changes, including limited rehearsal times and limited opportunities for costume adjustments. In this paper, we present a dance performance and practice simulation system that allows dancers to observe dance movements from multiple perspectives, providing them with an advance preview of their performance in aformal setting.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1285" style="margin-bottom: 0.3em;">
                <strong>CovisitVM: Cross-Reality Virtual Museum Visiting (ID:&nbsp;P1285)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Xuansheng Xia,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yue Li,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hai-Ning Liang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University</i>
                        
                     
                
            </p>
            
                <div id="P1285" class="wrap-collabsible"> <input id="collapsibleabstractP1285" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1285" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual Reality Head-Mounted Displays (VR HMDs) are the main ways for users to immerse in a virtual environment and interact with its virtual objects. The experiences of those around the VR HMD users and their effects on HMD users' experiences have not been well studied. In this work, we invite participants to engage in a cross-reality virtual museum visit. With low, medium, and high degrees of non-HMD user involvement, they could incrementally observe, navigate within, and interact with the virtual museum. Our study provides insights into the design of engaging multiuser VR experiences and cross-reality collaborations.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1287" style="margin-bottom: 0.3em;">
                <strong>MemoryVR: Collecting and Sharing Memories in Personal Virtual Museums (ID:&nbsp;P1287)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jiachen Liang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yue Li,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xueqi Wang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ziyue Zhao,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hai-Ning Liang,</span>
                        
                    
                        
                            <i>Xi'an Jiaotong-Liverpool University</i>
                        
                     
                
            </p>
            
                <div id="P1287" class="wrap-collabsible"> <input id="collapsibleabstractP1287" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1287" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present MemoryVR, a virtual museum system designed to preserve and share personal memories. This system enables users to create customized virtual museums within a spatial enclosure, providing an immersive and enriched way to experience personal memories. We invited participants to use MemoryVR to create their own personal virtual museums and visit those created by others. Results from evaluation studies showed a positive impact of MemoryVR on their experience of memories. Participants reported that their experiences within the personal virtual museums were fulfilling, invoking a sense of ritual, ownership, curiosity, and engagement.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1288" style="margin-bottom: 0.3em;">
                <strong>AgileFingers: Authoring AR Character Animation Through Hierarchical and Embodied Hand Gestures (ID:&nbsp;P1288)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yue Lin,</span>
                        
                    
                        
                            <i>The Hong Kong University of Science and Technology (Guangzhou);</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yudong Huang,</span>
                        
                    
                        
                            <i>The Hong Kong University of Science and Technology (Guangzhou);</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">David Yip,</span>
                        
                    
                        
                            <i>The Hong Kong University of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zeyu Wang,</span>
                        
                    
                        
                            <i>The Hong Kong University of Science and Technology (Guangzhou)</i>
                        
                     
                
            </p>
            
                <div id="P1288" class="wrap-collabsible"> <input id="collapsibleabstractP1288" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1288" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present AgileFingers, a hand gesture-based solution for authoring AR character animation based on a mobile device. Our work initially categorizes four major types of animals under Vertebrata. We conducted a formative study on how users construct hierarchical relationships in full-body skeletal animation and potential hand structure mapping rules. Informed by the study, we developed a hierarchical segmented control system, which enables novice users to manipulate full-body 3D characters with unimanual gestures sequentially. Our user study reveals the ease of use, intuitiveness, and high adaptability of the AgileFingers system across various characters. </p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1289" style="margin-bottom: 0.3em;">
                <strong>Exploring Interactive Gestures with Voice Assistant on HMDs in Social Situations (ID:&nbsp;P1289)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Xin Yi,</span>
                        
                    
                        
                            <i>Tsinghua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yan Kong,</span>
                        
                    
                        
                            <i>CS;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xueze Kang,</span>
                        
                    
                        
                            <i>CS;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Shuning Zhang,</span>
                        
                    
                        
                            <i>Tsinghua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Xueyang Wang,</span>
                        
                    
                        
                            <i>Tsinghua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuntao Wang,</span>
                        
                    
                        
                            <i>Tsinghua University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yu Tian,</span>
                        
                    
                        
                            <i>China Astronaut Research and Training Center;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hewu Li,</span>
                        
                    
                        
                            <i>Tsinghua University</i>
                        
                     
                
            </p>
            
                <div id="P1289" class="wrap-collabsible"> <input id="collapsibleabstractP1289" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1289" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Voice assistants (VAs) are becoming increasingly popular in VR and MR. However, speaking to the VA in social (e.g., during conversation with others) may lead to misunderstanding and embarrassment. In this paper, we proposed CollarMic, a technique that allowed the users to use speak-to-shoulder gesture to indicate whether to talk to the VA. Through a brainstorming with 10 experts, we first collected a total of 62 conversation switching gestures in social that leveraged different body parts (e.g., head and hand). We further eliminated different gestures through voting and interviews. Our findings highlighted the speak-to-shoulder gesture that best suits our technique.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1290" style="margin-bottom: 0.3em;">
                <strong>Augmented Reality Guidance for Numerical Control Program Setups (ID:&nbsp;P1290)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Constantin Kleinbeck,</span>
                        
                    
                        
                            <i>Friedrich-Alexander Universität Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tobias Hassel,</span>
                        
                    
                        
                            <i>Friedrich-Alexander Universität Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Julian Kreimeier,</span>
                        
                    
                        
                            <i>Friedrich-Alexander University Erlangen-Nürnberg;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel Roth,</span>
                        
                    
                        
                            <i>Technical University of Munich</i>
                        
                     
                
            </p>
            
                <div id="P1290" class="wrap-collabsible"> <input id="collapsibleabstractP1290" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1290" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Setting up a new numerical control (NC) program requires a time-consuming and error-prone validation process. Complications arise as diagnostic information like trajectories are commonly presented as abstract numerical value. We devised, implemented, and evaluated an Augmented Reality (AR) NC-program setup assistance system. Interviews with machine operators identified common issues during program setup and we designed and evaluated visual guidance methods. Our findings indicate that the AR path preview system can decrease error detection time, increase detection rate, and enhance user confidence. Users quickly noticed errors in paths, while errors related to milling depth were slower to detect.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1291" style="margin-bottom: 0.3em;">
                <strong>A Case Study on User Centered Design for VR Based Training in Health (ID:&nbsp;P1291)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Pablo Figueroa,</span>
                        
                    
                        
                            <i>Universidad de los Andes;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Carlos Rivera,</span>
                        
                    
                        
                            <i>Hospital Militar Central;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Patricia Casas,</span>
                        
                    
                        
                            <i>Hospital Militar Central;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jorge E López,</span>
                        
                    
                        
                            <i>Hospital Militar Central;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Daniel Mora,</span>
                        
                    
                        
                            <i>Hospital Militar Central;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ivan Gómez,</span>
                        
                    
                        
                            <i>Hospital Militar Central;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kelly Katherine Peñaranda,</span>
                        
                    
                        
                            <i>Universidad de Los Andes</i>
                        
                     
                
            </p>
            
                <div id="P1291" class="wrap-collabsible"> <input id="collapsibleabstractP1291" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1291" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We present GoMi-VR, a Virtual Reality (VR) environment for training physicians in their duties during the first minute of a baby delivery. Our development process allowed us to discuss requirements remotely, create rapid prototypes, test them with experts, identify improvements, and repeat the process in a promptly manner.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1298" style="margin-bottom: 0.3em;">
                <strong>Evaluation of Monocular and Binocular Contrast Sensitivity on Virtual Reality Head-Mounted Displays (ID:&nbsp;P1298)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Khushi Bhansali,</span>
                        
                    
                        
                            <i>Cornell University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Miguel Lago,</span>
                        
                    
                        
                            <i>U.S. FDA;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ryan Beams,</span>
                        
                    
                        
                            <i>Food and Drug Administration;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Chumin Zhao,</span>
                        
                    
                        
                            <i>CDRH, United States Food and Drug Administration</i>
                        
                     
                
            </p>
            
                <div id="P1298" class="wrap-collabsible"> <input id="collapsibleabstractP1298" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1298" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality (VR) creates an immersive experience by rendering a pair of graphical views on a head-mounted display (HMD). However, image quality assessment on VR HMDs has been primarily limited to monocular optical bench measurements on a single eyepiece. We begin to bridge the gap between monocular and binocular image quality evaluation by developing a WebXR test platform to perform human observer experiments. Specifically, monocular and binocular contrast sensitivity functions (CSFs) are obtained using varied interpupillary distance (IPD) conditions. A combination of optical image quality characteristics and binocular summation can potentially predict the binocular contrast sensitivity on VR HMDs.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1299" style="margin-bottom: 0.3em;">
                <strong>Visuo-vestibular Congruency Impacts on Player Experiences in Virtual Reality (ID:&nbsp;P1299)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Benjamin Williams,</span>
                        
                    
                        
                            <i>Staffordshire University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Christopher Headleand,</span>
                        
                    
                        
                            <i>Staffordshire University</i>
                        
                     
                
            </p>
            
                <div id="P1299" class="wrap-collabsible"> <input id="collapsibleabstractP1299" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1299" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Exposure to Virtual Reality is typically multisensory, and incorporates information from several sensory modalities. Within this context, the congruency between cross-modal feedback plays a crucial role in enhancing user experience and minimising potential discomfort. Whilst some studies have investigated the visuo-vestibular relationship in VR exposure, the exploration of congruency has often been limited in scope. In this paper, vestibular congruency in VR is investigated in detail, with a focus on its significance in recreational activities. Participants were tasked with a virtual driving activity in motion-actuated environments, with three conditions altering the degree of correlation between the visual and vestibular senses.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1301" style="margin-bottom: 0.3em;">
                <strong>Redirected Walking vs. Omni-Directional Treadmills: An Evaluation of Presence (ID:&nbsp;P1301)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Raiffa Syamil,</span>
                        
                    
                        
                            <i>University of Central Florida;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mahdi Azmandian,</span>
                        
                    
                        
                            <i>Sony Interactive Entertainment;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sergio Casas-Yrurzum,</span>
                        
                    
                        
                            <i>University of Valencia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Pedro Morillo,</span>
                        
                    
                        
                            <i>University of Valencia;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Carolina Cruz-Neira,</span>
                        
                    
                        
                            <i>University of Central Florida</i>
                        
                     
                
            </p>
            
                <div id="P1301" class="wrap-collabsible"> <input id="collapsibleabstractP1301" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1301" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Omni-Directional Treadmills (ODT) and Redirected Walking (RDW) seem suitable for eliciting presence through a full-body walking experience, however both present unique mechanisms that can affect users' presence, comfort, and overall preference. To measure this effect, we conducted a counterbalanced within-subjects user study with 20 participants. Participants wore a wireless VR headset and experienced a tour of a virtual art museum, once using RDW and another time using a passive, slip-based ODT. Both solutions elicit similar amounts of presence, however RDW is perceived as more natural and is the preferred choice of the participants.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1303" style="margin-bottom: 0.3em;">
                <strong>Can a Novel Virtual Reality Simulator, Developed for Standalone HMDs, Effectively Prepare Patients for an MRI Examination? (ID:&nbsp;P1303)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yue Yang,</span>
                        
                    
                        
                            <i>Johns Hopkins University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Emmanuel A Corona,</span>
                        
                    
                        
                            <i>Stanford University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Bruce Daniel,</span>
                        
                    
                        
                            <i>Stanford University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Christoph Leuze,</span>
                        
                    
                        
                            <i>Stanford University</i>
                        
                     
                
            </p>
            
                <div id="P1303" class="wrap-collabsible"> <input id="collapsibleabstractP1303" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1303" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Magnetic Resonance Imaging (MRI) examinations can be scary for pediatric or claustrophobic patients. Virtual reality (VR) simulation has shown the potential to prepare patients for MRI and alleviate anxiety. However, existing VR simulations either use low-quality mobile headsets or require a complex setup, limiting immersion level and practicality. No existing simulation incorporates interactive elements to train patients to hold still. We have designed a novel VR-MRI simulator for standalone HMDs that offers high-quality and interactive MRI simulation. After comparing it with standard preparatory material, we conclude that our VR-MRI could be more engaging, satisfying, and effective for MRI preparation.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1305" style="margin-bottom: 0.3em;">
                <strong>The Influence of Metaverse Environment Design on Learning Experiences in Virtual Reality Classes: A Comparative Study (ID:&nbsp;P1305)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Valentina Uribe,</span>
                        
                    
                        
                            <i>Universidad de los Andes;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Vivian Gómez,</span>
                        
                    
                        
                            <i>Universidad de los Andes;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Pablo Figueroa,</span>
                        
                    
                        
                            <i>Universidad de los Andes</i>
                        
                     
                
            </p>
            
                <div id="P1305" class="wrap-collabsible"> <input id="collapsibleabstractP1305" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1305" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this study, we investigate learning and the quality of the classroom experience by conducting classes in four metaverse environments: Workrooms, Spatial, Mozilla Hubs, and Arthur. Using questionnaires, we analyze factors like avatars, spatial layout, mobility, and additional functions' influence on concentration, usability, presence, and learning. Despite minimal differences in learning outcomes, significant variations in classroom experience emerged. Particularly, metaverses with restricted movement and functions showed heightened immersion, concentration, and presence. Additionally, our findings underscore the beneficial influence of avatars featuring lifelike facial expressions in improving the overall learning encounter.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1306" style="margin-bottom: 0.3em;">
                <strong>Towards a centered-user interaction in immersive visualization for preoperative surgical planning in complex malformations. A mental model elicitation approach: Work in progress (ID:&nbsp;P1306)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Carlos J Latorre-Rojas,</span>
                        
                    
                        
                            <i>Universidad Militar Nueva Granada;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Alexander Rozo-Torres,</span>
                        
                    
                        
                            <i>Universidad Militar Nueva Granada;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Javier A. Luzon,</span>
                        
                    
                        
                            <i>Akershus University Hospital;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wilson J. Sarmiento,</span>
                        
                    
                        
                            <i>Universidad Militar Nueva Granada</i>
                        
                     
                
            </p>
            
                <div id="P1306" class="wrap-collabsible"> <input id="collapsibleabstractP1306" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1306" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Extended Reality (XR) technologies have opened new possibilities for image visualization in healthcare, but eliciting user requirements for XR development is challenging. Human-computer interaction can help obtain user mental models, mostly unexplored in XR for health. An ongoing project aims to address this gap by studying the skills and backgrounds of radiologists and surgeons using the same XR tool in preoperative surgical planning for complex malformations through elucidating mental model, performing technical review and proposing potential interaction models or interaction concepts. The work aims to contribute insights for designing more intuitive, immersive medical visualization tools.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1308" style="margin-bottom: 0.3em;">
                <strong>AnyTracker: Tracking Pose for Any Object in Videos (ID:&nbsp;P1308)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Wenbo Li,</span>
                        
                    
                        
                            <i>Zhejiang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhaoyang Huang,</span>
                        
                    
                        
                            <i>The Chinese University of Hong Kong;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yijin Li,</span>
                        
                    
                        
                            <i>Zhejiang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yichen Shen,</span>
                        
                    
                        
                            <i>Zhejiang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Shuo Chen,</span>
                        
                    
                        
                            <i>Zhejiang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhaopeng Cui,</span>
                        
                    
                        
                            <i>Zhejiang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hongsheng Li,</span>
                        
                    
                        
                            <i>The Chinese University of Hong Kong;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Guofeng Zhang,</span>
                        
                    
                        
                            <i>Computer Science College</i>
                        
                     
                
            </p>
            
                <div id="P1308" class="wrap-collabsible"> <input id="collapsibleabstractP1308" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1308" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Object pose estimation is crucial for AR and video editing, but current methods are limited to RGB-D videos or require prior scanning, making them unsuitable for internet videos. We propose AnyTracker, tracking 6-DoF poses of casual objects in RGB videos, which utilizes GroupGOTR to estimate correspondence between reference and query images for a group of query points. It incorporates the TransGRU module and a geometry module for iterative refinement of correspondences. During inference, AnyTracker employs Active Bundle Adjustment (ABA) to establish feature tracks based on correspondences and estimate per-frame object pose. AnyTracker achieves superior accuracy in correspondence and pose estimation.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1309" style="margin-bottom: 0.3em;">
                <strong>3D Pano Inpainting: Building a VR Environment from a Single Input Panorama (ID:&nbsp;P1309)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Shivam Asija,</span>
                        
                    
                        
                            <i>California Polytechnic State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Edward Du,</span>
                        
                    
                        
                            <i>California Polytechnic State University San Luis Obispo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Nam Nguyen,</span>
                        
                    
                        
                            <i>California Polytechnic State University, San Luis Obispo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Stefanie Zollmann,</span>
                        
                    
                        
                            <i>University of Otago;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jonathan Ventura,</span>
                        
                    
                        
                            <i>California Polytechnic State University</i>
                        
                     
                
            </p>
            
                <div id="P1309" class="wrap-collabsible"> <input id="collapsibleabstractP1309" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1309" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Creating 360-degree 3D content is challenging because it requires either a multi-camera rig or a collection of many images taken from different perspectives. Our approach aims to generate a 360? VR scene from a single panoramic image using a learning-based inpainting method adapted for panoramic content. We introduce a pipeline capable of transforming an equirectangular panoramic RGB image into a complete 360? 3D virtual reality scene represented as a textured mesh, which is easily rendered on a VR headset using standard graphics rendering pipelines. We qualitatively evaluate our results on a synthetic dataset consisting of 360 panoramas in indoor scenes.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1311" style="margin-bottom: 0.3em;">
                <strong>Effects of Transcranial Direct-Current Stimulation on Hand Redirection (ID:&nbsp;P1311)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Shogo Tachi,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Keigo Matsumoto,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maki Ogawa,</span>
                        
                    
                        
                            <i>the University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ayumu Yamashita,</span>
                        
                    
                        
                            <i>The University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Takuji Narumi,</span>
                        
                    
                        
                            <i>the University of Tokyo;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kaoru Amano,</span>
                        
                    
                        
                            <i>The University of TOkyo</i>
                        
                     
                
            </p>
            
                <div id="P1311" class="wrap-collabsible"> <input id="collapsibleabstractP1311" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1311" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Hand redirection (HR) is a technique to manipulate the position and shape of an object by shifting the virtual hand position and posture from the physical hand position and posture. This study assessed the detection thresholds (DTs) for horizontal HR during left DLPFC intervention using transcranial direct current stimulation (tDCS). Based on a user study, our findings revealed that DTs for leftward shifts significantly increased in the sham condition, likely due to habituation. Interestingly, anodal tDCS stimulation effectively mitigated this increase in DTs. These results suggest that tDCS may reset the effects of HR habituation or enhance attention for HR.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1314" style="margin-bottom: 0.3em;">
                <strong>Developing a Multimodal Clinical Nursing Simulation with a Virtual Preceptor in AR (ID:&nbsp;P1314)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Hyeongil Nam,</span>
                        
                    
                        
                            <i>Hanyang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ji-Young Yeo,</span>
                        
                    
                        
                            <i>Hanyang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kisub Lee,</span>
                        
                    
                        
                            <i>Hanyang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kangsoo Kim,</span>
                        
                    
                        
                            <i>University of Calgary;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jong-Il Park,</span>
                        
                    
                        
                            <i>Hanyang University</i>
                        
                     
                
            </p>
            
                <div id="P1314" class="wrap-collabsible"> <input id="collapsibleabstractP1314" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1314" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Nursing education plays a crucial role in preparing future nurses to provide high-quality patient care. To ensure its effectiveness, it is imperative to instruct nursing students in discerning patient signs/symptoms using a range of sensory modalities. Moreover, pairing nursing students with expert preceptors offers a highly effective means of providing valuable feedback and mentorship. In this paper, we propose a novel Augmented Reality (AR) simulation framework for effective clinical nursing education with multimodal signs/symptoms and a preceptor agent to provide guidance/instruction. Using the proposed framework, we developed an AR nurse training system that resembles practical clinical nursing environments.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1315" style="margin-bottom: 0.3em;">
                <strong>Design of Time-Continuous Rating Interfaces for Collecting Empathic Responses in VR, and Initial Evaluation with VR180 Video Viewing (ID:&nbsp;P1315)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Md Istiak Jaman Ami,</span>
                        
                    
                        
                            <i>University of Louisiana at Lafayette;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jason Wolfgang Woodworth,</span>
                        
                    
                        
                            <i>University of Louisiana at Lafayette;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Christoph W Borst,</span>
                        
                    
                        
                            <i>University of Louisiana at Lafayette</i>
                        
                     
                
            </p>
            
                <div id="P1315" class="wrap-collabsible"> <input id="collapsibleabstractP1315" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1315" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We design and assess interfaces for time-continuous self-reporting of empathic responses in VR, using dimensions of empathic concern and personal distress. Visual styles included Circular Rating Indicators (CRI), Color Dials (CD), and Interactive Curving Faces (Frowny). Input types included desktop knobs and VR controller touchpads. We considered the intuitiveness, effectiveness, and intrusiveness of designs with viewers of a stereo 180° video. Initial results highlight the CRI and Frowny as promising choices, calling for future examination of their effectiveness.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1317" style="margin-bottom: 0.3em;">
                <strong>Understanding How Interaction Experiences Factor into Security Perception of Payment Authentication in Virtual Reality (ID:&nbsp;P1317)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jingjie Li,</span>
                        
                    
                        
                            <i>University of Edinburgh;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sunpreet Singh Arora,</span>
                        
                    
                        
                            <i>Visa Research;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kassem Fawaz,</span>
                        
                    
                        
                            <i>University of Wisconsin-Madison;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Younghyun Kim,</span>
                        
                    
                        
                            <i>University of Wisconsin-Madison;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Can Liu,</span>
                        
                    
                        
                            <i>Visa Research;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sebastian Meiser,</span>
                        
                    
                        
                            <i>University of Lübeck;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mohsen Minaei,</span>
                        
                    
                        
                            <i>Visa Research;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Maliheh Shirvanian,</span>
                        
                    
                        
                            <i>Netflix;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kim Wagner,</span>
                        
                    
                        
                            <i>Visa Research</i>
                        
                     
                
            </p>
            
                <div id="P1317" class="wrap-collabsible"> <input id="collapsibleabstractP1317" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1317" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Users embrace the rapid development of virtual reality (VR) technology for everyday settings. These settings include payment, which makes user authentication necessary. Despite this need, there is a limited understanding of how users' unique experiences in VR contribute to their security perception. To understand this question, we designed probes of payment authentication, which are embedded in the routine payment of a VR game, to provoke participants' reactions from multiple angles.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1318" style="margin-bottom: 0.3em;">
                <strong>The Effects of Colored Environmental Surroundings in Virtual Reality (ID:&nbsp;P1318)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Deyrel Diaz,</span>
                        
                    
                        
                            <i>Clemson University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Andrew Robb,</span>
                        
                    
                        
                            <i>Clemson University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sabarish V. Babu,</span>
                        
                    
                        
                            <i>Clemson University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Christopher Pagano,</span>
                        
                    
                        
                            <i>Clemson University</i>
                        
                     
                
            </p>
            
                <div id="P1318" class="wrap-collabsible"> <input id="collapsibleabstractP1318" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1318" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Research has shown that environmental cues affect long-term memory and spatial cognition, but there is still a lack of understanding of the exact characteristics that produce these effects. We conducted a virtual reality within-subjects repeated measures study on 51 participants to test color congruency. Participants saw and studied 20 objects, then completed object recall and placement tasks in a recall room with a congruent or incongruent color. The objective and subjective data we gathered suggest that congruent color conditions influenced long-term memory and speed for recalled objects. Object size was also shown to influence spatial cognition and long-term memory.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1319" style="margin-bottom: 0.3em;">
                <strong>Initial investigations into information retention and perception on Virtual Human Race (ID:&nbsp;P1319)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Deyrel Diaz,</span>
                        
                    
                        
                            <i>Clemson University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Samaneh Zamanifard,</span>
                        
                    
                        
                            <i>Clemson University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Matias Volonte,</span>
                        
                    
                        
                            <i>Clemson University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Andrew Duchowski,</span>
                        
                    
                        
                            <i>Clemson University</i>
                        
                     
                
            </p>
            
                <div id="P1319" class="wrap-collabsible"> <input id="collapsibleabstractP1319" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1319" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual humans have long been studied in the field of embodied conversational agents. Most studies have focused on understanding the verbal and non-verbal cues required of a virtual agent for relationship building, trust, and credibility. Some studies have even gone as far as looking into characteristics like clothes, accessories, and race to see what effects they may have on the interlocutor. We seek feedback on an investigative study where we look to better understand how avatar race may affect not only previously investigated affect, but also information retention and eye gaze behavior. We discuss the technical design and research methodology.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1320" style="margin-bottom: 0.3em;">
                <strong>The Role of Haptic Feedback in Enhancing Technical Skills Acquisition and Transfer in an Immersive Simulator: A User Study (ID:&nbsp;P1320)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Intissar Cherif,</span>
                        
                    
                        
                            <i>Univ Evry,Université Paris Saclay;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Amine Chellali,</span>
                        
                    
                        
                            <i>Univ Evry,Université Paris Saclay;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mohamed Chaouki Babahenini,</span>
                        
                    
                        
                            <i>University of Biskra;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Samir Otmane,</span>
                        
                    
                        
                            <i>Université d'Evry , Université Paris Saclay</i>
                        
                     
                
            </p>
            
                <div id="P1320" class="wrap-collabsible"> <input id="collapsibleabstractP1320" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1320" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We study the impact of haptic feedback on basic technical skills transfer from VR to the real world. Twenty-four volunteers were divided into two training groups (haptic and no-haptic groups) and a control group. The training groups learned to perform a "Ring Transfer" task in a VR simulator and all participants performed pre-, post, and retention tests on a similar physical setup. Results show that skill transfer is observed for both training groups and not for the control group. The haptic group participants also improved their performance compared to the no-haptic group, but the difference was not significant.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1322" style="margin-bottom: 0.3em;">
                <strong>[EXTENDED] The Restorative Influence of Virtual Reality Environment Design (ID:&nbsp;P1322)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jalynn Blu Nicoly,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Rachel Masters,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Vidya D Gaddy,</span>
                        
                    
                        
                            <i>Colorado State University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Victoria Interrante,</span>
                        
                    
                        
                            <i>University of Minnesota;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Francisco Raul Ortega,</span>
                        
                    
                        
                            <i>Colorado State University</i>
                        
                     
                
            </p>
            
                <div id="P1322" class="wrap-collabsible"> <input id="collapsibleabstractP1322" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1322" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>We aim to explore whether beauty in moving and still virtual environments (VEs) contributes to restorativeness. We hypothesized that the moving forest environment would be the most restorative and the abstract art would be the least restorative. The Perceived Restorativeness Scale (PRS) and the Zuckerman Inventory of Personal Reactions (ZIPERS) positive affect showed a significant increase in restorativeness in the moving forest condition than in the control condition. Additionally, PRS indicated more significance in the moving forest condition than in the abstract art condition.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1333" style="margin-bottom: 0.3em;">
                <strong>Propagation as Data (PaD): Neural Phase Hologram Generation with Variable Distance Support (ID:&nbsp;P1333)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Jun Yeong Cha,</span>
                        
                    
                        
                            <i>Kyung Hee University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hyunmin Ban,</span>
                        
                    
                        
                            <i>Kyung Hee University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Seungmi Choi,</span>
                        
                    
                        
                            <i>Kyung Hee University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hui Yong Kim,</span>
                        
                    
                        
                            <i>Kyung Hee Univ.</i>
                        
                     
                
            </p>
            
                <div id="P1333" class="wrap-collabsible"> <input id="collapsibleabstractP1333" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1333" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Most of the neural network models for generating phase holograms developed so far are trained and validated only for a single distance. Consequently, if a distance is altered, the performance of models tends to decline dramatically. To address this, we introduce a novel approach called `Propagation as Data (PaD)'. Unlike conventional methods, our proposed model does not include the propagation process in a neural network. We pre-calculate propagation kernels and use them as conditioning data. Experimental results demonstrate that our model can consistently generate high-quality phase holograms across a range of distances with a single model.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1153" style="margin-bottom: 0.3em;">
                <strong>Generating Look-Alike Avatars: Perception of Head Shape, Texture Fidelity and Head Orientation of Other People (ID:&nbsp;P1153)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Kwame Agyemang Baffour,</span>
                        
                    
                        
                            <i>Graduate Center;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Oyewole Oyekoya,</span>
                        
                    
                        
                            <i>City University of New York - Hunter College</i>
                        
                     
                
            </p>
            
                <div id="P1153" class="wrap-collabsible"> <input id="collapsibleabstractP1153" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1153" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This research seeks to determine the influence of head shape, texture fidelity and head orientation of a look-alike avatar on perception of likeability and visual realism, especially when judged by other people. Two textured look-alike avatars were generated using: (i) three-dimensional (3D) stereophotogrammetry; and (ii) 3D face reconstruction from a single full-face image. Participants compared three different head orientations (0 degree, 45 degree, 90 degree) of the look-alike avatars' textured heads to their corresponding head silhouettes. Results suggest that participants prefer geometrically accurate photorealistic avatars of other people due to the accuracy of the head shape and texture fidelity.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1339" style="margin-bottom: 0.3em;">
                <strong>You're Hired! Effect of Virtual Agents' Social Status and Social Attitudes on Stress Induction in Virtual Job Interviews (ID:&nbsp;P1339)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Celia Kessassi,</span>
                        
                    
                        
                            <i>IMT Atlantique;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Cédric Dumas,</span>
                        
                    
                        
                            <i>IMT Atlantique;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Caroline G. L. Cao,</span>
                        
                    
                        
                            <i>University of Illinois;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mathieu Chollet,</span>
                        
                    
                        
                            <i>University of Glasgow</i>
                        
                     
                
            </p>
            
                <div id="P1339" class="wrap-collabsible"> <input id="collapsibleabstractP1339" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1339" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality offers new possibilities for social skills training. In fact, with virtual reality, it is possible to get immersed and train to various social situations, including job interviews. In this paper, we investigate the effect of virtual recruiters' social status and social attitudes on participants' stress during a job interview. Results show that negative recruiter attitudes led to higher subjective stress compared to neutral attitudes, and that participants with high social anxiety react differently to positive feedback compared to participants with low social anxiety. The mechanisms of social stress induction in virtual reality are complex and deserve further study.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1323" style="margin-bottom: 0.3em;">
                <strong>[EXTENDED] Eye Tracking Performance in Mobile Mixed Reality (ID:&nbsp;P1323)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Satyam Awasthi,</span>
                        
                    
                        
                            <i>University of California, Santa Barbara;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Vivian Ross,</span>
                        
                    
                        
                            <i>University of California, Santa Barbara;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Sydney Lim,</span>
                        
                    
                        
                            <i>University of California, Santa Barbara;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michael Beyeler,</span>
                        
                    
                        
                            <i>University of California, Santa Barbara;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Tobias Höllerer,</span>
                        
                    
                        
                            <i>University of California, Santa Barbara</i>
                        
                     
                
            </p>
            
                <div id="P1323" class="wrap-collabsible"> <input id="collapsibleabstractP1323" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1323" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Implementing and evaluating eye tracking across multiple platforms and use cases can be challenging due to the lack of standardized metrics and measurements. Additionally, existing calibration methods and accuracy measurements often do not account for the common scenarios of walking and scanning in mobile AR settings. We conducted user studies evaluating eye tracking on the Magic Leap One, the HoloLens 2, and the Meta Quest Pro. Our results reveal that the degree to which locomotion influenced eye tracking performance depended on the headset, with the HoloLens 2, which features a retractable visor, displaying the greatest decrease in accuracy during locomotion.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1337" style="margin-bottom: 0.3em;">
                <strong>The Influence of Perspective on Training Effects in Virtual Reality Public Speaking Training (ID:&nbsp;P1337)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Fumitaka Ueda,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yuichiro Fujimoto,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Taishi Sawabe,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Masayuki Kanbara,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hirokazu Kato,</span>
                        
                    
                        
                            <i>Nara Institute of Science and Technology</i>
                        
                     
                
            </p>
            
                <div id="P1337" class="wrap-collabsible"> <input id="collapsibleabstractP1337" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1337" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>A third-person perspective in virtual reality (VR) based public speaking training enables trainees to observe themselves through self-avatars, potentially enhancing their public speaking skills. This study investigates the influence of perspective on the training effects, i.e., changes in audience evaluations before and after training. In the experiment, VR job interview training was conducted for five days under three perspective conditions. Mock interviews were also performed before and after training and were assessed by external raters. The results indicate that the training effects were significantly higher in the Front condition regarding verbal communication skills and the overall impression of the interview.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1347" style="margin-bottom: 0.3em;">
                <strong>Enhancing Immersion in Virtual Reality: Cost-Efficient Spatial Audio Generation for Panoramic Videos (ID:&nbsp;P1347)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Di Zhang,</span>
                        
                    
                        
                            <i>Communication University of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">JIAXIN SHI,</span>
                        
                    
                        
                            <i>Communication University of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Long Ye,</span>
                        
                    
                        
                            <i>Communication University of China</i>
                        
                     
                
            </p>
            
                <div id="P1347" class="wrap-collabsible"> <input id="collapsibleabstractP1347" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1347" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper presents a novel system for generating 5.1.4 format spatial audio for home theater scenes using panoramic video content. In virtual reality, spatial audio technology is an integral part, however, current virtual reality videos lack systematic spatial audio production methods, while traditional spatial audio production methods are expensive and complex. To address this problem, this system provides an efficient and cost-effective solution for audio producers to match audio with panoramic visuals. Finally, this study provides a user interface?considering the human auditory properties, the system is optimized to generate 5.1.4 spatial surround audio to enhance the immersion of the listener.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1336" style="margin-bottom: 0.3em;">
                <strong>Toward Optimized AR-based Human-Robot Interaction Ergonomics: Modeling and Predicting Interaction Comfort (ID:&nbsp;P1336)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Yunqiang Pei,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Bowen Jiang,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kaiyue Zhang,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ziyang Lu,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Mingfeng Zha,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Guoqing Wang,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Zhitao Liu,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Ning Xie,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yang Yang,</span>
                        
                    
                        
                            <i>University of Electronic Science and Technology of China;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Heng Tao Shen,</span>
                        
                    
                        
                            <i>Tongji University</i>
                        
                     
                
            </p>
            
                <div id="P1336" class="wrap-collabsible"> <input id="collapsibleabstractP1336" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1336" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Augmented Reality in Human-Robot Interaction (AR-HRI) boosts user experience. The key challenge is refining interaction methods to minimize discomfort and enhance quality. This AR-HRI study uses Galvanic Skin Respons (GSR) to predict and improve user comfort. User studies tested interaction strategies in an AR environment. A machine learning model, developed from GSR data, predicted comfort levels and informed strategy changes. Comfort metrics were visualized every second using Hololens 2, creating an AR-HRI comfort system. The method improved user comfort, provided a new AR-HRI metric, and highlighted future research opportunities.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1091" style="margin-bottom: 0.3em;">
                <strong>Visual Perception in VR Training: Impact of Information Transfer Methods (ID:&nbsp;P1091)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Maximilian Rettinger,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Michael Hug,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hassan Kamel,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yashita Saxena,</span>
                        
                    
                        
                            <i>Technical University of Munich;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Gerhard Rigoll,</span>
                        
                    
                        
                            <i>Technical University of Munich</i>
                        
                     
                
            </p>
            
                <div id="P1091" class="wrap-collabsible"> <input id="collapsibleabstractP1091" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1091" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual Reality (VR) has great potential for education and training, but it is not fully understood how to make VR training as effective as possible. An investigation indicated that a combination of auditory and textual information is the best way to present training instructions and explanations to the user. However, is it also suitable to perceive the training content visualized in the virtual environment? We conduct a within-subjects study to investigate this by comparing users' gaze attention in four information transfer methods. The results are consistent with previous findings that the auditory-visual-combination is the most appropriate of the methods compared.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1335" style="margin-bottom: 0.3em;">
                <strong>Distribution-Shifting: Improved Phase Hologram Processing with Novel Phase Distortion Metric (ID:&nbsp;P1335)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Seungmi Choi,</span>
                        
                    
                        
                            <i>Kyung Hee University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Jun Yeong Cha,</span>
                        
                    
                        
                            <i>Kyung Hee University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hyunmin Ban,</span>
                        
                    
                        
                            <i>Kyung Hee University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Kwan-Jung Oh,</span>
                        
                    
                        
                            <i>ETRI;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hyunsuk Ko,</span>
                        
                    
                        
                            <i>Hanyang University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Hui Yong Kim,</span>
                        
                    
                        
                            <i>Kyung Hee Univ.</i>
                        
                     
                
            </p>
            
                <div id="P1335" class="wrap-collabsible"> <input id="collapsibleabstractP1335" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1335" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Due to the lack of suitable phase distortion metrics, the optimization and evaluation of phase-holograms have relied on numerical reconstruction (NR) domain metrics. However, the necessity of NR for hologram processing leads to more intricate design and a higher computational burden. In this paper, we introduce a distribution-shifting (DS) algorithm to enable optimizing or measuring distortions directly in phase-domain, considering 2π-periodicity and shift-invariance. Experimental results with various noise types demonstrate a strong correlation between phase-domain metric with DS and their NR-domain counterparts. We believe that our DS-metric could facilitate direct optimization approaches in various phase-hologram processing techniques.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
        <div style="margin-left: 25px;">
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
                        
            <p class="medLarge" id="P1338" style="margin-bottom: 0.3em;">
                <strong>Using Virtual Reality to Promote Pro-environmental Consumption (ID:&nbsp;P1338)</strong>
            </p>
            <p class="font_70">
                
                
                    
                    
                                                    
                            <span class="bold">Dr. Ou Li,</span>
                        
                    
                        
                            <i>Hangzhou Normal University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Wenchao Su,</span>
                        
                    
                        
                            <i>Hangzhou Normal University;</i>
                        
                     
                
                    
                    
                                                    
                            <span class="bold">Yan Shi,</span>
                        
                    
                        
                            <i>Hangzhou Normal University</i>
                        
                     
                
            </p>
            
                <div id="P1338" class="wrap-collabsible"> <input id="collapsibleabstractP1338" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstractP1338" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality (VR) has emerged as an effective method for encouraging eco-friendly behaviors. The present study aimed to investigate the effectiveness and underlying mechanisms of VR in promoting pro-environmental consumption. The results indicated that, when compared to traditional mediums such as 2D video and printed material, VR can improve individuals' pro-environmental consumption. This favorable effect of VR was found to be mediated by the inclusion of nature in self (INS). Additionally, this study also demonstrated that individuals' green values act as the boundary condition.</p>
                        </div>
                    </div>
                </div>   
            
        </div>
    
</div>

            </div>

            <!--  footer -->
            <div class="ieeevrfooter">
                <hr>
                <div>
                    <a href="https://www.ieee.org" target="_new" style="margin-left: 2rem;">
                        <img src=/assets/images/sponsors/ieee-logo-white.svg alt="IEEE" style="height: 35px;">
                    </a>
                    <a  href="https://www.computer.org" target="_new" style="margin-left: 2rem;">
                        <img src=/assets/images/sponsors/ieee-cs-logo-white.svg alt="IEEE Computer Society" style="height: 35px;">
                    </a>
                    <a href="http://vgtc.org" target="_new" style="margin-left: 2rem;">
                        <img src=/assets/images/sponsors/ieee-vgtc-logo-white.svg alt="IEEE Visualization and Graphics Technical Community" style="height: 35px;">
                    </a>
                </div>
                <p style="text-align:center ! important;">©IEEE VR Conference 2025, Sponsored by the IEEE Computer Society and the Visualization and Graphics Technical Committee</p>
            </div>
        </section>

    </article>
</div>

    </div>
  </body>
</html>
