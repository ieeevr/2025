<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">



<!-- begin _includes/seo.html --><title>Doctoral Consortium | IEEE VR 2024</title>
<meta name="description" content="The 31st IEEE Conference on Virtual Reality and 3D User Interfaces ">


  <meta name="author" content="IEEE VR 2024">


<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="IEEE VR 2024">
<meta property="og:title" content="Doctoral Consortium">
<meta property="og:url" content="http://localhost:4000/program/doctoral-consortium/">


  <meta property="og:description" content="The 31st IEEE Conference on Virtual Reality and 3D User Interfaces ">



  <meta property="og:image" content="http://localhost:4000/assets/images/IEEEVR_2024_OGImage.png">



  <meta name="twitter:site" content="@IEEEVR">
  <meta name="twitter:title" content="Doctoral Consortium">
  <meta name="twitter:description" content="The 31st IEEE Conference on Virtual Reality and 3D User Interfaces ">
  <meta name="twitter:url" content="http://localhost:4000/program/doctoral-consortium/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="http://localhost:4000/assets/images/IEEEVR_2024_OGImageSquare.png">
    
  

  







  

  


<link rel="canonical" href="http://localhost:4000/program/doctoral-consortium/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "IEEE VR 2024",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="IEEE VR 2024 Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>



      <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="shortcut icon" type="image/png" href="https://www.ieeevr.org/2024/favicon.png"/> 

<!-- end custom head snippets -->

      
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">      
      <script src="https://code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  </head>

  <body class="layout--ieeevr-default">
    <div class="initial-content">
     <!-- For all browsers -->
<script src="https://kit.fontawesome.com/4eee35f757.js"></script>
<link rel="stylesheet" href="//code.jquery.com/ui/1.13.2/themes/base/jquery-ui.css">
<script src="https://code.jquery.com/ui/1.13.2/jquery-ui.js"></script>
<link rel="stylesheet" href="/assets/css/main.css?version=20240318">
<link rel="stylesheet" href="/assets/css/tableStyles.css?version=20240318">
<link rel="stylesheet" href="/assets/css/styles.css?version=20240318">

<button onclick="topFunction()" id="myBtnTop" title="Go to top">Top</button>

<script>
    var mybutton = document.getElementById("myBtnTop");

    window.onscroll = function() {
        scrollFunction()
    };

    function scrollFunction() {
        if (document.body.scrollTop > 100 || document.documentElement.scrollTop > 100) {
            mybutton.style.display = "block";
        } else {
            mybutton.style.display = "none";
        }
    }

    function topFunction() {
        document.body.scrollTop = 0;
        document.documentElement.scrollTop = 0;
    }

    function myFunction() {
        var x = document.getElementById("myTopnav");
        if (x.className === "topnav") {
            x.className += " responsive";
        } else {
            x.className = "topnav";
        }
    }
</script>

<div id="main" role="main">
    <article class="splash" style="margin:0px;" itemscope itemtype="https://schema.org/CreativeWork">
         <!-- banner -->
         <a href=/><img class="ieeevrbanner" src=/assets/images/IEEEVR_2025_LogoBanner.jpg alt="The official banner for the IEEE Conference on Virtual Reality + User Interfaces, comprised of a Kiwi wearing a VR headset overlaid on an image of Mount Cook and a braided river."></a>
       
        <!-- content -->
        <section class="page__content" itemprop="text">
            <div class="row">
                <!-- Main Content -->
                <h1>Doctoral Consortium</h1>
<div>
    <p>
        The Doctoral Consortium will be held on 16 March 2024 (Saturday) in the Sorcerer's Apprentice Ballroom. All times below are given in local time of Orlando, Florida USA EDT (UTC-4). 
    </p>
    <p>
        Here are the key information for the presenters and mentors:
        <ul>
            <li>Each presentation includes a 10-minute talk + 5-minute Q&amp;A.</li>
            <li>All presenters and mentors need to be present at the scheduled session and are encouraged to attend as much of the doctoral consortium as possible. </li>
            <li>After each session, the mentors and students can use the Break/Lunch time for further breakout discussions and mentoring. </li>
            <li>Some students may have a pair of mentors. Because some mentors cannot attend the conference in person, we recommend that the students take the initiative to reach out to allocated mentors and arrange a separate online meeting for mentoring. </li>
            <li>The venue for the DC track is: Sorcerer's Apprentice, 2. </li>
            <li>The final mentor allocation will be released soon. </li>
            <li>The presentation and mentoring at the DC mark the start of collaborations and we strongly recommend that the presenters and mentors hold periodical meetings to deepen the collaborations. </li>
        </ul>
    </p>
</div>
<div>
    <table class="styled-table font_80">
        <tr>
            <th colspan="2">Schedule - 16 March 2024, Saturday</th>
        </tr>
        <tr>
            <td>08:20 - 08:30 am</td>
            <td>
                Welcome
            </td>
        </tr>
        <tr>
            <td>08:30 - 10:00 am</td>
            <td>
                <strong>Presentations 1-6 (10-min talk + 5-min Q&amp;A for each presentation)</strong><br />
                Jinwook Kim - Jan Springer<br />
                Hyeongil Nam - Jens Grubert<br />
                Siamak Ahmadzadeh Bazzaz - Jan Springer<br />
                Muhammad Twaha Ibrahim - Jan Springer<br />
                Xueqi Wang - Frank Guan<br />
                Jiachen Liang - Frank Guan
            </td>
        </tr>
        <tr>
            <td>10:00 - 10:30 am</td>
            <td>
                Break (breakout with mentors)
            </td>
        </tr>
        <tr>
            <td>10:30 - 12:00 am</td>
            <td>
                <strong>Presentations 7-12 (10-min talk + 5-min Q&amp;A for each presentation)</strong><br />
                Seoyoung Kang - Bruce Thomas<br />
                Assem Kroma - Bruce Thomas<br />
                Kristen Grinyer - Bruce Thomas<br />
                Rachel Masters - Jens Grubert<br />
                Sunday Ubur - Shohei Mori<br />
                Dahlia Musa - Shohei Mori
            </td>
        </tr>
        <tr>
            <td>12:00 - 1:30 pm</td>
            <td>
               Lunch
            </td>
        </tr>
        <tr>
            <td>13:30 - 15:30 pm</td>            
            <td>
                <strong>Presentations 13-20 (10-min talk + 5-min Q&amp;A for each presentation)</strong><br />
                Dong Woo Yoo - Guillaume Moreau<br />
                Brett Benda - Steve Feiner<br />
                Seonji Kim - Dieter Schmalstieg<br />
                Hail Song - Jason Orlosky<br />
                Dongyun Han - Steve Feiner<br />
                Jennifer Cremer - Dieter Schmalstieg<br />
                Elham Mohammadrezaei - Guillaume Moreau<br />
                Nikitha Donekal Chandrashekar - Guillaume Moreau
            </td>
        </tr>
        <tr>
            <td>15:30 - 16:00 pm</td>
            <td>
                Break (breakout with mentors)
            </td>
        </tr>
        <tr>
            <td>16:00 - 17:00 pm</td>
            <td>
                <strong>Presentations 21-24 (10-min talk + 5-min Q&amp;A for each presentation)</strong><br />
                Danah Omary - Isaac Cho<br />
                Jingyi Zhang - Frank Guan<br />
                Tomáš Nováček - Isaac Cho<br />
                Ryan Canales - Isaac Cho<br />
            </td>
        </tr>
        <tr>
            <td>17:00 - 17:30 pm</td>
            <td>
                Breakout with mentors
            </td>
        </tr>
    </table>    
</div>
<div>
    <h2 id="P3" class="pink" style="padding-top:25px;">Accepted Students</h2>
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1109" style="margin-bottom: 0.3em;">
                <strong>The application of Digital Twins in Sustainable Urban Planning: from data acquisition to 3D virtualization (ID:&nbsp;1109)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Siamak Ahmadzadeh Bazzaz</span>, <i>IUSS Pavia University, Italy</i><br />
                <span class="bold">Mentor:</span> <span class="">Jan Springer</span>
            </p>
            
                <div id="1109" class="wrap-collabsible"> <input id="collapsibleabstract1109" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1109" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This research explores the role of Digital Twins (DT) in enhancing Sustainable Urban Planning, especially in climate-resilient regions. It integrates DT for real-time data acquisition, processing, and 3D virtualization, focusing on climate change adaptation and environmental risk mitigation. The study combines a literature review, a conceptual framework, case studies, and data collection, showcasing a comprehensive approach to applying DT in urban planning. At the core of this research is the innovative use of DT for dynamic urban management, enabling informed decision-making and improved resilience. Challenges such as integrating DT into existing urban infrastructures and ensuring user accessibility are identified, highlighting areas for further innovation. Looking forward, the research anticipates DT's growing role in urban planning, suggesting potential for broader adoption and further technological integration. This work positions DT as a transformative tool for sustainable and resilient urban development in the face of climate challenges.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1095" style="margin-bottom: 0.3em;">
                <strong>Remapped Interaction Detection by Naive Users in Virtual Reality (ID:&nbsp;1095)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Brett Benda</span>, <i>University of Florida, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Steve Feiner</span>
            </p>
            
                <div id="1095" class="wrap-collabsible"> <input id="collapsibleabstract1095" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1095" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Remapped interactions have been often evaluated under the lens of detection; if we use these techniques at undetectable levels, we will not risk distracting the user, breaking immersion, or increasing cybersickness. However, research has relied heavily on methods that induce behavior and grant knowledge to participants that conflict with actual end-users. The goal of this dissertation is to evaluate the detection of these techniques by naive users, who do not know they are being used and are focused on other tasks to guide the usage of these techniques outside the laboratory.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1112" style="margin-bottom: 0.3em;">
                <strong>Animating Interactive Virtual Humans (ID:&nbsp;1112)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Ryan Canales</span>, <i>Clemson University, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Isaac Cho</span>
            </p>
            
                <div id="1112" class="wrap-collabsible"> <input id="collapsibleabstract1112" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1112" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Immersive virtual worlds, such as in video games, have captivated the general public for years. Virtual reality (VR) takes immersion further, placing users inside virtual environments and allowing them to interact with virtual objects and even socialize with others in a shared virtual space. The effectiveness of VR is heavily influenced by the quality of virtual interactions and characters. This research proposal aims to address some of the challenges of creating engaging virtual experiences by focusing on the motion of virtual humans.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1113" style="margin-bottom: 0.3em;">
                <strong>Scan2Twin: Virtual Reality for Enhanced Anatomical Investigation (ID:&nbsp;1113)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Jennifer Cieliesz Cremer</span>, <i>University of Florida, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Dieter Schmalstieg</span>
            </p>
            
                <div id="1113" class="wrap-collabsible"> <input id="collapsibleabstract1113" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1113" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Interpreting 3D information from 2D slices of data points is a notoriously difficult process, especially in the medical field with its use of scan imaging. This dissertation aims to apply and assess the advantages of virtual reality (VR) for exploring volumetric visualization of medical images and enable efficient generation of explicit, unambiguous surface models. I will evaluate the effects of stereoscopic viewing on different methods of volumetric data visualizations, experiment with tool development to increase 3D modeling accessibility, and an expert review of the overall system design we refer to as Scan2Twin.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1120" style="margin-bottom: 0.3em;">
                <strong>Understanding the impact of the Fidelity of Multimodal Interactions in XR based Training Simulators on Cognitive Load (ID:&nbsp;1120)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Nikitha Donekal Chandrashekar</span>, <i>Virginia Tech, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Guillaume Moreau, Steve Feiner</span>
            </p>
            
                <div id="1120" class="wrap-collabsible"> <input id="collapsibleabstract1120" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1120" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>As eXtended Reality (XR) technologies evolve, the integration of multimodal interfaces becomes a defining factor in immersive experiences. Existing literature highlights a lack of consensus regarding the impact of these interfaces in XR environments. Due to the Uncanny Valley Effect and its amplification in multimodal stimuli, the uncanny valley effect poses a potential challenge of increased cognitive load due to dissonance between user expectations and reality. My research pivots on the observation that current studies often overlook a crucial factor—the fidelity of stimuli presented to users. The main goal of my research is to answer the question about how multimodal interactions in XR based applications impact the cognitive load experienced by the user. To address this gap, I employ a comprehensive human-computer interaction (HCI) research approach involving frameworks, theories, user studies and guidelines. The goal is to systematically investigate the interplay of stimulus fidelity and cognitive load in XR, aiming to offer insights for the design of Audio-Visual-Haptic (AVH) interfaces.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1114" style="margin-bottom: 0.3em;">
                <strong>Supporting Complex Interactions in Mobile Virtual Reality (ID:&nbsp;1114)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Kristen Grinyer</span>, <i>Carleton University, Canada</i><br />
                <span class="bold">Mentor:</span> <span class="">Bruce Thomas, Jens Grubert</span>
            </p>
            
                <div id="1114" class="wrap-collabsible"> <input id="collapsibleabstract1114" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1114" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual reality (VR) is poised to become a ubiquitous technology in the coming years. However, its high-cost barrier to adoption prevents user groups of lower socioeconomic statuses from accessing the technology, leaving them out of what may become a predominant computing paradigm in the next decade. Low-cost mobile VR has potential to democratize VR access but offers limited interaction capabilities due to low-fidelity hardware and few input options. Mobile VR cannot currently support most VR applications or offer effective VR experiences. This doctoral consortium proposal outlines my past, current, and future work aiming to investigate and develop low-cost interaction solutions for mobile VR to support complex interactions and outlines topics for future discussion.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1102" style="margin-bottom: 0.3em;">
                <strong>Perception of Visual Variables on Various Tiled Wall-Sized Display Settings in Immersive Environment (ID:&nbsp;1102)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Dongyun Han</span>, <i>Utah State University, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Steve Feiner, Dieter Schmalstieg</span>
            </p>
            
                <div id="1102" class="wrap-collabsible"> <input id="collapsibleabstract1102" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1102" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Immersive analytics indicates carrying out visual analytics in interactive virtual environments (IVEs). As immersive technologies have rapidly matured with commercial success, researchers have sought to leverage their potential to aid human data understanding and sensemaking processes. Unlike the limited information space available on 2D display settings, users in IVEs can use 3D immersive space as information space by creating as many visualizations as they want. Despite growing knowledge of its benefits, many challenges still remain for immersive analytics to be effectively deployed for various applications. Effective use of data visualizations in IVEs requires a better understanding of how people perceive, interpret, and process visualized information. This paper discusses my research questions and presents research progress.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1119" style="margin-bottom: 0.3em;">
                <strong>Dynamic Spatially Augmented Reality on Deformable Surfaces (ID:&nbsp;1119)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Muhammad Twaha Ibrahim</span>, <i>UC Irvine, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Jan Springer</span>
            </p>
            
                <div id="1119" class="wrap-collabsible"> <input id="collapsibleabstract1119" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1119" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Prior work on spatially augmented reality using multiple projectors have focused primarily on rigid objects. However, works on projection based displays on deformable dynamic objects have focused only on small scale single projector systems. Tracking a deformable surface and updating projections in real time to project on it is a significantly challenging task, even for single projector systems.  For my dissertation, I am working on developing the first end-to-end solution for achieving a real-time, seamless display on deformable surfaces using multiple projectors without requiring any prior knowledge of the surface shape or device calibration parameters. Challenges in achieving such a system include accurate calibration of the devices despite a continuously deforming surface, real-time tracking of the changing surface shape using multiple cameras, and real-time warp and blend of the projected imagery to create the seamless display on it. This work has tremendous applications on mobile and expeditionary systems where environmentals (e.g. wind, vibrations, suction) cannot be avoided. One can create large displays on tent walls in remote, austere military or emergency operations in minutes to support large scale command and control, mission rehearsal, large scale visualization or training operations. It can be used to create displays on mobile and inflatable objects for tradeshows/events and touring edutainment applications.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1104" style="margin-bottom: 0.3em;">
                <strong>Investigating Avatar Facial Expressions and Collaboration Dynamics for Social Presence in Avatar-Mediated XR Remote Communication (ID:&nbsp;1104)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Seoyoung Kang</span>, <i>KAIST, Republic of Korea,</i><br />
                <span class="bold">Mentor:</span> <span class="">Bruce Thomas, Yue Li (Remote)</span>
            </p>
            
                <div id="1104" class="wrap-collabsible"> <input id="collapsibleabstract1104" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1104" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In avatar-mediated remote communication, reflecting one's facial expressions in avatars remains challenging but essential in facilitating effective communication and fostering social presence. From our prior studies, we underscored the significance of emotion-based facial expressions in avatar-mediated remote communications within eXtended Reality (XR). Our findings indicated that during emotional conversations as well as informative speeches, activating the emotion-based blendshapes resulted in a level of social presence and communication quality comparable to activating all facial blendshapes. As we move forward, our research aims to investigate avatar facial expression considering XR contexts, with an emphasis on the challenges stemming from XR device constraints and the dynamic contexts of remote collaboration. We aspire to devise adaptive guidelines for avatar facial expression, leveraging emotional cues using XR devices. Additionally, we plan to systematically analyze the implications of remote collaboration's varying scenarios, formulating guidelines for adaptive avatar representation that account for different collaborative environments and scenarios. To enhance social presence and communication experiences among users, we are preparing for a series of user studies, aiming to validate the effectiveness of our approaches and prototype systems.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1022" style="margin-bottom: 0.3em;">
                <strong>Exploring and Designing VR Locomotion Method based on Bio-signal for Hands-free Context and its Improvement (ID:&nbsp;1022)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Jinwook Kim</span>, <i>KAIST, Republic of Korea,</i><br />
                <span class="bold">Mentor:</span> <span class="">Jan Springer, Qi Sun (Remote)</span>
            </p>
            
                <div id="1022" class="wrap-collabsible"> <input id="collapsibleabstract1022" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1022" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Along with the advanced hand-tracking algorithms, various interactions based on hand motion are actively developing. However, if too much function are concentrated on the hand, it might be a burden on the hand and interrupt immersion. Therefore, we focused on designing a locomotion method, which is essential for exploring the broad virtual world efficiently and comfortably. We utilized bio-signal (i.e., eye tracking, EEG) to reduce the load on the hand. First, we explored to compare the usability and efficiency of our method to hand-tracking-based locomotion methods. The result showed that our methods are suitable for hands-free VR contexts. Also, we found that EEG based method proposes enhanced experience when it is used with eye-tracking. In order to improve this effect, we are currently working on research that develops user-friendly Steady-State Visual Evoked Potential (SSVEP) stimuli that suit the VR HMD format. From our research, we aim to propose design guidelines for presenting appropriate locomotion methods depending on the various contexts in the virtual environment for an enhanced hands-free VR experience.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1096" style="margin-bottom: 0.3em;">
                <strong>Experience Graph using Spatio-Temporal Scene Data for Replaying Mixed Reality Interaction (ID:&nbsp;1096)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Seonji Kim</span>, <i>KAIST, Republic of Korea,</i><br />
                <span class="bold">Mentor:</span> <span class="">Dieter Schmalstieg, Qi Sun (Remote)</span>
            </p>
            
                <div id="1096" class="wrap-collabsible"> <input id="collapsibleabstract1096" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1096" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This study proposes an experience graph, a Mixed Reality (MR) experience replaying method for recording and playing spatially adaptive interaction in a mixed reality environment. Many MR immersive interactions are experienced in a space different from the original recorded space, so it is important to adaptively experience the interaction in the local space. As a solution to the collision and mismatch during the interactions in local space, there is research on scene generation or adaptive object placement, but the change of interactions dependent on space and the resulting experience transformation were not considered. This study structures spatial information and dependent interactions into an experience graph and proposes a spatial adaptive replay method that minimizes interaction deformation. Recorded interactions are evaluated quantitatively through loss rate and transformation rate, played interactions are evaluated qualitatively through user experiments, and finally, indicators important to the user experience are reflected in the experience graph. Utilizing the results of this study will contribute to experiencing immersive spatial MR where users actively record and share spatial experiences beyond the limitations of time and space.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1110" style="margin-bottom: 0.3em;">
                <strong>The Essentials of XR Prototyping for Non-Tech Professionals: Taxonomy and Low-Fidelity Tools to Aid Prototyping Decisions (ID:&nbsp;1110)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Assem Kroma</span>, <i>Carleton University, Canada</i><br />
                <span class="bold">Mentor:</span> <span class="">Bruce Thomas, Mark Billinghurst (Remote)</span>
            </p>
            
                <div id="1110" class="wrap-collabsible"> <input id="collapsibleabstract1110" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1110" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>This paper delves into the complexities of Extended Reality (XR), exploring the challenges in XR design, directing user attention, managing cybersickness, and the overall integrative nature of these issues in the context of prototyping. We offer an overview of the current state of XR and identify key areas where novice and non-technical professionals face challenges. The paper concludes with a proposed research agenda focused on three main themes and potential research directions. This work aims to enhance the design process and facilitate rapid prototyping, enabling non-technical professionals to create more effective and engaging XR experiences.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1123" style="margin-bottom: 0.3em;">
                <strong>Playful and participatory cultural heritage experience using extended reality technologies (ID:&nbsp;1123)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Jiachen Liang</span>, <i>Xi’an Jiaotong-Liverpool University, China</i><br />
                <span class="bold">Mentor:</span> <span class="">Frank Guan</span>
            </p>
            
                <div id="1123" class="wrap-collabsible"> <input id="collapsibleabstract1123" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1123" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Creative design and applications based on Cultural Heritage (CH) content have gradually become a new way of communicating and interpreting cultural values. The proposed research aims to improve the CH experience using extended reality (XR) technologies by investigating the digital affordances of XR technologies that can support the design of playful CH experience, and exploring appropriate ways to enable user-generated content for participatory CH experience. This is an interdisciplinary project that sits at the intersection of human-computer interaction (HCI), XR, and CH. A practice-based approach will be adopted to conduct research in the wild, designing and developing novel XR interfaces and interaction techniques for CH experiences. The combined design thinking and computational approach will allow an in-depth understanding of playful and participatory CH experience using XR technologies, providing empirical, artifact, and theoretical contributions to the fields of HCI, XR, and CH.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1116" style="margin-bottom: 0.3em;">
                <strong>Virtual Reality Nature Environment Designs for Mental Health (ID:&nbsp;1116)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Rachel Masters</span>, <i>Colorado State University, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Jens Grubert</span>
            </p>
            
                <div id="1116" class="wrap-collabsible"> <input id="collapsibleabstract1116" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1116" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Shinrin-yoku, or forest bathing, is a therapeutic nature immersion practice shown to reduce stress and restore mental resources. In a world where the negative effects of sustained stress are an increasingly prevalent issue, connection to nature is important. However, many populations who encounter the most stress are largely isolated from nature, like people working long hours in cities, nursing home residents, or hospital patients. For these populations, virtual reality (VR) nature immersion is a promising supplement for when nature is not accessible. In order to design optimal VR nature environments (VNEs), we must first understand why and how VNEs can be restorative. This is a current area of research, and one critical gap that the proposed research seeks to understand is how the different components of VNEs and their interactions can be optimally designed for short and long term stress reduction and attention restoration. The proposed research does a deeper exploration into the designs of plants and water for optimally restorative VR green and blue spaces.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1115" style="margin-bottom: 0.3em;">
                <strong>Reinforcement Learning for Context-aware and Adaptive Lighting Design using Extended Reality: Impacts on Human Emotions and Behaviors (ID:&nbsp;1115)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Elham Mohammadrezaei</span>, <i>Virginia Tech, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Guillaume Moreau</span>
            </p>
            
                <div id="1115" class="wrap-collabsible"> <input id="collapsibleabstract1115" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1115" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In the interconnected world of smart built environments, Extended Reality (XR) emerges as a transformative technology that can enhance user experiences through personalized lighting systems. The integration of XR with deep reinforcement learning for adaptive lighting design (ALD) has potential to optimize visual experiences while addressing real-time data analysis, XR system complexities, and integration challenges with building automation systems. The research centers on developing an XR-based ALD system that dynamically responds to user preferences, thereby positively impacting human emotions, behaviors, and overall well-being.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1121" style="margin-bottom: 0.3em;">
                <strong>3D Measurement System for Wound Care (ID:&nbsp;1121)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Dahlia Musa</span>, <i>New Jersey Institute of Technology, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Shohei Mori</span>
            </p>
            
                <div id="1121" class="wrap-collabsible"> <input id="collapsibleabstract1121" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1121" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Wounds can be challenging to treat and often require periodic measurements to determine progression. Healthcare providers typically measure wounds (e.g., pressure injuries, venous ulcers, and diabetic foot ulcers) using rulers or images; however, these measurements tend to be subjective and inaccurate. We are developing a system that measures and tracks wound progression from 3-dimensional (3D) scans. In this research, we are evaluating our 3D software in comparison to wound measurement methods commonly used by providers. Our 3D software may facilitate the measurement, documentation, and visualization of wounds to support clinicians’ treatment decisions and improve patient outcomes.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1106" style="margin-bottom: 0.3em;">
                <strong>Multimodal VR/MR Simulation with Empathic Agents for Nursing Education (ID:&nbsp;1106)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Hyeongil Nam</span>, <i>Hanyang University, Republic of Korea,</i><br />
                <span class="bold">Mentor:</span> <span class="">Jens Grubert</span>
            </p>
            
                <div id="1106" class="wrap-collabsible"> <input id="collapsibleabstract1106" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1106" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In the field of healthcare education, including medical and nursing education, realistic and interactive simulations of various clinical scenes are required to mimic real-life situations, necessitating more than just a procedural experience. There has been a recent surge in demand for virtual/mixed reality (VR/MR) due to its advantages in providing diverse simulation environments. However, several challenges still remain to be addressed to advance to the next level of VR/MR-based clinical simulation. In this paper, I introduce the research that I have conducted on creating immersive clinical nursing scenes in VR/MR-based learning and training simulation systems, focusing on key elements such as multi-modal interaction, virtual agents, and learner performance assessment. Furthermore, I will discuss future research aimed at fostering collaboration in various healthcare fields, including nursing, and developing VR/MR simulation systems that enable learners to experience and learn from personalized clinical situations. These topics and questions will be further discussed at the IEEE VR 2024 Doctoral Consortium.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1111" style="margin-bottom: 0.3em;">
                <strong>Precise Hand Tracking using Multiple Optical Sensors (ID:&nbsp;1111)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Tomáš Nováček</span>, <i>Faculty of Information Technology, CTU, Czech Republic</i><br />
                <span class="bold">Mentor:</span> <span class="">Isaac Cho</span>
            </p>
            
                <div id="1111" class="wrap-collabsible"> <input id="collapsibleabstract1111" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1111" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Virtual and extended reality are some of the fastest-growing fields today. The visual quality, the headset size, the latency – it all improves with every new device. However, the user interface is still falling behind.  After several decades, when we mainly used a mouse and keyboard as controllers for personal computers, we finally can and should use other methods of input. We can even use parts of our body other than our hands to control the virtual worlds. For example, we can track the eyes for additional input or run around the virtual world with our own feet. This doctoral thesis deals with hand-tracking controllers for virtual reality without the user needing to hold or wear any device to interact with the virtual world. The goal is to create an intuitive controller using optical sensors that can provide a more natural user interface. We propose and implement a set of algorithms to fuse the tracking data from multiple optical hand-tracking sensors that provide greater precision and tracking possibilities than a single optical sensor.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1057" style="margin-bottom: 0.3em;">
                <strong>Customizable Multi-Modal Mixed Reality Framework (ID:&nbsp;1057)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Danah Omary</span>, <i>University of North Texas, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Isaac Cho</span>
            </p>
            
                <div id="1057" class="wrap-collabsible"> <input id="collapsibleabstract1057" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1057" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Mixed Reality (MR) has potential to be used not only in the entertainment industry and the work industry, but also for assistive technology. Mixed Reality is suited for assistive technology because of its utilization of other forms of physical feedback to the human senses besides vision while still making use of visual feedback. We propose a glove-based MR system framework that will use finger and hand movement tracking along with tactile feedback so that the blind and visually-impaired (BVI) can interact tactily with virtual objects. In addition to touch, our proposed framework will include robust interactions through other modalities such as a custom voice assistant and an audio interface as well as visual interfaces that tailor to the visual needs of BVI users. Through the various modalities of interaction in our proposed framework, BVI users will be able to obtain a more detailed sense of virtual objects of any 3D model and their experiences will not be limited by vision. The customizable features and modalities that will be available in our proposed system framework will allow for a more individual experience that can be tailored to the variety of the different needs of the BVI as well as general users.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1098" style="margin-bottom: 0.3em;">
                <strong>Toward Realistic 3D Avatar Generation with Dynamic 3D Gaussian Splatting for AR/VR Communication (ID:&nbsp;1098)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Hail Song</span>, <i>Korea Advanced Institute of Science and Technology, Republic of Korea,</i><br />
                <span class="bold">Mentor:</span> <span class="">Jason Orlosky</span>
            </p>
            
                <div id="1098" class="wrap-collabsible"> <input id="collapsibleabstract1098" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1098" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Realistic avatars are fundamental for immersive experiences in Augmented Reality (AR) and Virtual Reality (VR) environments. In this work, we introduce a novel approach for avatar generation, combining 3D Gaussian Splatting with the parametric body model, SMPL. This methodology overcomes the inefficiencies of traditional image/video-based avatar creation, which is often slow and requires high computing resources. The integration of 3D Gaussian Splatting for representing human avatar offers realistic and real-time rendering for AR/VR applications. We also conducted preliminary tests to verify the quality of avatar representation using 3D Gaussian Splatting. These tests, displayed alongside outcomes from existing methods, demonstrate the potential of this research to significantly contribute to the creation of realistic avatars in the future. Additionally, several key discussions are presented, essential for developing and evaluating the system and providing valuable insights for future research.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1117" style="margin-bottom: 0.3em;">
                <strong>Enhancing Accessibility and Emotional Expression in Educational Extended Reality for Deaf and Hard of Hearing: A User-centric Investigation (ID:&nbsp;1117)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Sunday David Ubur</span>, <i>Virginia Tech, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Shohei Mori</span>
            </p>
            
                <div id="1117" class="wrap-collabsible"> <input id="collapsibleabstract1117" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1117" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>My research focuses on addressing accessibility challenges for the Deaf and Hard of Hearing (DHH) population, specifically within Extended Reality (XR) applications for education. Emphasizing improved communication, the study aims to enhance accessibility and emotional expression in XR environments. Objectives include identifying best practices and guidelines for designing accessible XR, exploring prescriptive and descriptive aspects of accessibility design, and integrating emotional aspects for DHH users. The three-phase research plan includes a literature review, development of an accessible XR system, and user studies. Anticipated contributions involve enhanced support for DHH individuals in XR and a deeper understanding of human-computer interaction aspects.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1122" style="margin-bottom: 0.3em;">
                <strong>User exploratory learning in a Virtual Reality museum (ID:&nbsp;1122)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Xueqi Wang</span>, <i>Xi'an Jiaotong-Liverpool University, China</i><br />
                <span class="bold">Mentor:</span> <span class="">Frank Guan</span>
            </p>
            
                <div id="1122" class="wrap-collabsible"> <input id="collapsibleabstract1122" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1122" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>Museums are actively digitizing collections to store, distribute and share them with more audiences. The adoption of interactive technologies is promoting the spread of culture by developing creative narratives to support education and entertainment. Virtual Reality (VR) museums is an extension of physical museums that presents content in digital forms, such as photographs of museum archives. Current extended reality technologies afford the creation of interactive cultural heritage learning experience with 3D assets. This project aims to obtain an in-depth understanding of users’ experiential learning in VR museums. We will investigate appropriate ways to engage users in learning culture heritage and creating personalized experiences. This is an interdisciplinary project that sits at the intersection of human-computer interaction (HCI), Museums and education. The research will provide insights and guidelines to the design and development of VR museums for effective experiential learning.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1092" style="margin-bottom: 0.3em;">
                <strong>Data-Driven Expertise Assessment in XR: Analyzing Multimodal Sensor Data for Psychomotor and Cognitive Tasks (ID:&nbsp;1092)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Dong Woo Yoo</span>, <i>Northeastern University, United States</i><br />
                <span class="bold">Mentor:</span> <span class="">Guillaume Moreau, Jason Orlosky</span>
            </p>
            
                <div id="1092" class="wrap-collabsible"> <input id="collapsibleabstract1092" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1092" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>In this doctoral research, a novel approach is developed for the real-time assessment of user expertise, focusing on skill-based and cognitive tasks within Extended Reality (XR) environments. This approach combines advanced user modeling with state-of-the-art sensors and includes an in-depth analysis of gaze behavior and physiological responses. A distinctive aspect of this research is the real-time analysis of multimodal sensor data, which provides deeper and more precise insights into user skills and cognitive abilities. The goal is to advance the field of expertise assessment by introducing a nuanced and dynamic perspective that is suitable for a variety of applications across different domains. This research aims to establish new standards in the assessment of user expertise, meeting the evolving needs of modern educational and professional settings.</p>
                        </div>
                    </div>
                </div>   
            
    
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
            
            <p class="medLarge" id="1107" style="margin-bottom: 0.3em;">
                <strong>Actor Takeover of Animated Characters (ID:&nbsp;1107)</strong>
            </p>
            <p class="clear font_75">
                <span class="bold">Author:</span> <span class="">Jingyi Zhang</span>, <i>University College London, United Kingdom</i><br />
                <span class="bold">Mentor:</span> <span class="">Frank Guan</span>
            </p>
            
                <div id="1107" class="wrap-collabsible"> <input id="collapsibleabstract1107" class="toggle" type="checkbox" /> 
                    <label for="collapsibleabstract1107" class="lbl-toggle">Abstract</label>
                    <div class="collapsible-content">
                        <div class="content-inner">
                            <p>With the rise in accessibility of consumer-grade virtual reality headsets, social virtual reality applications have now caught widespread attention. One's anticipation for the virtual world is for it to be lively with characters. However, current technology falls short of generating intelligent virtual agents, thus creating a densely-populated environment with which one could have a variety of interactions is out of reach. This position paper introduces a takeover system which enables a single actor to seamlessly take over control of multiple virtual characters in a social virtual reality environment. In our experiments, we expect our system to enhance the perceived social presence of the scene and to uncover the difference in perception regarding computer-driven agents and human controlled avatars when interacting with multiple characters in the same environment.</p>
                        </div>
                    </div>
                </div>   
            
    
</div>

            </div>

            <!--  footer -->
            <div class="ieeevrfooter">
                <hr>
                <div>
                    <a href="https://www.ieee.org" target="_new" style="margin-left: 2rem;">
                        <img src=/assets/images/sponsors/ieee-logo-white.svg alt="IEEE" style="height: 35px;">
                    </a>
                    <a  href="https://www.computer.org" target="_new" style="margin-left: 2rem;">
                        <img src=/assets/images/sponsors/ieee-cs-logo-white.svg alt="IEEE Computer Society" style="height: 35px;">
                    </a>
                    <a href="http://vgtc.org" target="_new" style="margin-left: 2rem;">
                        <img src=/assets/images/sponsors/ieee-vgtc-logo-white.svg alt="IEEE Visualization and Graphics Technical Community" style="height: 35px;">
                    </a>
                </div>
                <p style="text-align:center ! important;">©IEEE VR Conference 2025, Sponsored by the IEEE Computer Society and the Visualization and Graphics Technical Committee</p>
            </div>
        </section>

    </article>
</div>

    </div>
  </body>
</html>
